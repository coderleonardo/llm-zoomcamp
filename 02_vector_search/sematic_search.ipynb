{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412068d6",
   "metadata": {},
   "source": [
    "# Vector Search with Qdrant\n",
    "\n",
    "## Vector Search\n",
    "\n",
    "Vector search is the backbone of the modern internet, whether you notice it or not. It powers recommendation engines, chatbots, AI agents, and even major search engines.\n",
    "\n",
    "In simple terms, traditional keyword search works by matching exact words. This works well when you know the precise keywords present in the data. But what happens when there are no keywords? What if you're searching through images, audio, video or code, or even cross-modally?\n",
    "\n",
    "Moreover, even in text-heavy documents, keyword search struggles to capture context and meaning. The same idea can be phrased in countless ways, so it is completely unfeasible to compare/search for using keyword-based methods.\n",
    "\n",
    "Instead of relying on exact matches, vector search retrieves information based on semantic similarity measured numerically between vectorized data representations (embeddings). It recognizes patterns and relationships between concepts, enabling search systems to retrieve the most relevant content, even when the phrasing differs, terminology varies, or no explicit keywords exist.\n",
    "\n",
    "## Qdrant\n",
    "\n",
    "[Qdrant](https://qdrant.tech/) is an **open-source** vector search engine, a dedicated solution built in Rust for scalable vector search. \n",
    "If you're wondering why you might need a dedicated solution for vector search, we’ve addressed that in the article [\"Built for Vector Search\"](https://qdrant.tech/articles/dedicated-vector-search/).\n",
    "\n",
    "To TLDR:\n",
    "- To make production-level vector search at scale;\n",
    "- To stay in sync with the latest trends and best practices;\n",
    "- To fully use vector search capabilities (including those beyond simple similarity search).\n",
    "\n",
    "In this notebook, we’ll give you a small sneak peek into semantic (vector) search with Qdrant and encourage you to play around & see if it fits your needs!\n",
    "\n",
    "If you have any questions about vector search in Qdrant, feel free to reach out in our [Discord community](https://discord.gg/G7PQU6Cy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d2706",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "Qdrant is fully open-source, which means you can run it in multiple ways depending on your needs.  \n",
    "You can self-host it on your own infrastructure, deploy it on Kubernetes, or run it in managed Cloud.  \n",
    "\n",
    "We're going to run a Qdrant instance in a Docker container.\n",
    "\n",
    "### Docker\n",
    "\n",
    "All you need to do is pull the image and start the container using the following commands:\n",
    "\n",
    "```bash\n",
    "docker pull qdrant/qdrant\n",
    "\n",
    "docker run -p 6333:6333 -p 6334:6334 \\\n",
    "   -v \"$(pwd)/qdrant_storage:/qdrant/storage:z\" \\\n",
    "   qdrant/qdrant\n",
    "```\n",
    "\n",
    "The second line in the `docker run` command mounts local storage to keep your data persistent.\n",
    "So even if you restart or delete the container, your data will still be stored locally.\n",
    "\n",
    "- 6333 – REST API port\n",
    "- 6334 – gRPC API port\n",
    "\n",
    "To help you explore your data visually, Qdrant provides a built-in **Web UI**, available in both Qdrant Cloud and local instances.\n",
    "You can use it to inspect collections, check system health, and even run simple queries.\n",
    "\n",
    "When you're running Qdrant in Docker, the Web UI is available at http://localhost:6333/dashboard\n",
    "\n",
    "### Installing Required Libraries\n",
    "\n",
    "In the environment you created specifically for this course, we’ll install:\n",
    "\n",
    "- The `qdrant-client` package. We'll be using the Python client, but Qdrant also offers official clients for JavaScript/TypeScript, Go, and Rust, so you can choose the best fit for your own projects.\n",
    "\n",
    "- The `fastembed` package - an optimized embedding (data vectorization) solution designed specifically for Qdrant. Make sure you install version `>= 1.14.2` to use the **local inference** with Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c30d39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -q \"qdrant-client[fastembed]>=1.14.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52904ad3",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries & Connect to Qdrant\n",
    "\n",
    "Now let’s import the necessary modules from the `qdrant-client` package.\n",
    "\n",
    "The `QdrantClient` class allows us to establish a connection to the Qdrant service,  \n",
    "while the `models` module provides definitions for various configurations and parameters we’ll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2caceaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdrleo/Desktop/courses/alexey_llm_zoomcamp/llm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab9d9d",
   "metadata": {},
   "source": [
    "Initialize the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527eeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\"http://localhost:6333\") # connecting to local Qdrant instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaebe42",
   "metadata": {},
   "source": [
    "## Step 2: Study the Dataset\n",
    "\n",
    "To build a working vector search solution (and, more generally, to understand if/when/how it’s needed), it's good to study the dataset and figure out the nature and structure of the data we’re working with, for example:\n",
    "\n",
    "- modality — is it text, images, videos, a combination?  \n",
    "- specifics — if it’s text: language used, how big are the text pieces, are there any special characters, etc.  \n",
    "\n",
    "It will help us define:\n",
    "- the right data \"schema\" (what to vectorize, what to store as metadata, etc);  \n",
    "- the right embedding model (the best fit based on the domain, precision & resource requirements). \n",
    "\n",
    "We have a toy dataset provided for experimentation, let's check it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af03241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c3d0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec36204",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "188bb3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'course': 'data-engineering-zoomcamp',\n",
      "  'documents': [{'question': 'Course - When will the course start?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'The purpose of this document is to capture '\n",
      "                         'frequently asked technical questions\\n'\n",
      "                         'The exact day and hour of the course will be 15th '\n",
      "                         'Jan 2024 at 17h00. The course will start with the '\n",
      "                         \"first  “Office Hours'' live.1\\n\"\n",
      "                         'Subscribe to course public Google Calendar (it works '\n",
      "                         'from Desktop only).\\n'\n",
      "                         'Register before the course starts using this link.\\n'\n",
      "                         'Join the course Telegram channel with '\n",
      "                         'announcements.\\n'\n",
      "                         \"Don’t forget to register in DataTalks.Club's Slack \"\n",
      "                         'and join the channel.'},\n",
      "                {'question': 'Course - What are the prerequisites for this '\n",
      "                             'course?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'GitHub - DataTalksClub '\n",
      "                         'data-engineering-zoomcamp#prerequisites'},\n",
      "                {'question': 'Course - Can I still join the course after the '\n",
      "                             'start date?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': \"Yes, even if you don't register, you're still \"\n",
      "                         'eligible to submit the homeworks.\\n'\n",
      "                         'Be aware, however, that there will be deadlines for '\n",
      "                         \"turning in the final projects. So don't leave \"\n",
      "                         'everything for the last minute.'},\n",
      "                {'question': 'Course - I have registered for the Data '\n",
      "                             'Engineering Bootcamp. When can I expect to '\n",
      "                             'receive the confirmation email?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': \"You don't need it. You're accepted. You can also \"\n",
      "                         'just start learning and submitting homework without '\n",
      "                         'registering. It is not checked against any '\n",
      "                         'registered list. Registration is just to gauge '\n",
      "                         'interest before the start date.'},\n",
      "                {'question': 'Course - What can I do before the course starts?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'You can start by installing and setting up all the '\n",
      "                         'dependencies and requirements:\\n'\n",
      "                         'Google cloud account\\n'\n",
      "                         'Google Cloud SDK\\n'\n",
      "                         'Python 3 (installed with Anaconda)\\n'\n",
      "                         'Terraform\\n'\n",
      "                         'Git\\n'\n",
      "                         'Look over the prerequisites and syllabus to see if '\n",
      "                         'you are comfortable with these subjects.'},\n",
      "                {'question': 'Course - how many Zoomcamps in a year?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'There are 3 Zoom Camps in a year, as of 2024. '\n",
      "                         'However, they are for separate courses:\\n'\n",
      "                         'Data-Engineering (Jan - Apr)\\n'\n",
      "                         'MLOps (May - Aug)\\n'\n",
      "                         'Machine Learning (Sep - Jan)\\n'\n",
      "                         \"There's only one Data-Engineering Zoomcamp “live” \"\n",
      "                         'cohort per year, for the certification. Same as for '\n",
      "                         'the other Zoomcamps.\\n'\n",
      "                         'They follow pretty much the same schedule for each '\n",
      "                         'cohort per zoomcamp. For Data-Engineering it is '\n",
      "                         '(generally) from Jan-Apr of the year. If you’re not '\n",
      "                         'interested in the Certificate, you can take any zoom '\n",
      "                         'camps at any time, at your own pace, out of sync '\n",
      "                         'with any “live” cohort.'},\n",
      "                {'question': 'Course - Is the current cohort going to be '\n",
      "                             'different from the previous cohort?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes. For the 2024 edition we are using Mage AI '\n",
      "                         'instead of Prefect and re-recorded the terraform '\n",
      "                         'videos, For 2023, we used Prefect instead of '\n",
      "                         'Airflow..'},\n",
      "                {'question': 'Course - Can I follow the course after it '\n",
      "                             'finishes?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes, we will keep all the materials after the course '\n",
      "                         'finishes, so you can follow the course at your own '\n",
      "                         'pace after it finishes.\\n'\n",
      "                         'You can also continue looking at the homeworks and '\n",
      "                         'continue preparing for the next cohort. I guess you '\n",
      "                         'can also start working on your final capstone '\n",
      "                         'project.'},\n",
      "                {'question': 'Course - Can I get support if I take the course '\n",
      "                             'in the self-paced mode?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes, the slack channel remains open and you can ask '\n",
      "                         'questions there. But always sDocker containers exit '\n",
      "                         'code w search the channel first and second, check '\n",
      "                         'the FAQ (this document), most likely all your '\n",
      "                         'questions are already answered here.\\n'\n",
      "                         'You can also tag the bot @ZoomcampQABot to help you '\n",
      "                         'conduct the search, but don’t rely on its answers '\n",
      "                         '100%, it is pretty good though.'},\n",
      "                {'question': 'Course - Which playlist on YouTube should I '\n",
      "                             'refer to?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'All the main videos are stored in the Main “DATA '\n",
      "                         'ENGINEERING” playlist (no year specified). The '\n",
      "                         'Github repository has also been updated to show each '\n",
      "                         'video with a thumbnail, that would bring you '\n",
      "                         'directly to the same playlist below.\\n'\n",
      "                         'Below is the MAIN PLAYLIST’. And then you refer to '\n",
      "                         'the year specific playlist for additional videos for '\n",
      "                         'that year like for office hours videos etc. Also '\n",
      "                         'find this playlist pinned to the slack channel.\\n'\n",
      "                         'h\\n'\n",
      "                         'ttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-'},\n",
      "                {'question': 'Course - \\u200b\\u200bHow many hours per week am '\n",
      "                             'I expected to spend on this  course?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'It depends on your background and previous '\n",
      "                         'experience with modules. It is expected to require '\n",
      "                         'about 5 - 15 hours per week. [source1] [source2]\\n'\n",
      "                         'You can also calculate it yourself using this data '\n",
      "                         'and then update this answer.'},\n",
      "                {'question': 'Certificate - Can I follow the course in a '\n",
      "                             'self-paced mode and get a certificate?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'No, you can only get a certificate if you finish the '\n",
      "                         \"course with a “live” cohort. We don't award \"\n",
      "                         'certificates for the self-paced mode. The reason is '\n",
      "                         'you need to peer-review capstone(s) after submitting '\n",
      "                         'a project. You can only peer-review projects at the '\n",
      "                         'time the course is running.'},\n",
      "                {'question': 'Office Hours - What is the video/zoom link to '\n",
      "                             'the stream for the “Office Hour” or workshop '\n",
      "                             'sessions?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'The zoom link is only published to '\n",
      "                         'instructors/presenters/TAs.\\n'\n",
      "                         'Students participate via Youtube Live and submit '\n",
      "                         'questions to Slido (link would be pinned in the chat '\n",
      "                         'when Alexey goes Live). The video URL should be '\n",
      "                         'posted in the announcements channel on Telegram & '\n",
      "                         'Slack before it begins. Also, you will see it live '\n",
      "                         'on the DataTalksClub YouTube Channel.\\n'\n",
      "                         'Don’t post your questions in chat as it would be '\n",
      "                         'off-screen before the instructors/moderators have a '\n",
      "                         'chance to answer it if the room is very active.'},\n",
      "                {'question': 'Office Hours - I can’t attend the “Office hours” '\n",
      "                             '/ workshop, will it be recorded?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes! Every “Office Hours” will be recorded and '\n",
      "                         'available a few minutes after the live session is '\n",
      "                         'over; so you can view (or rewatch) whenever you '\n",
      "                         'want.'},\n",
      "                {'question': 'Homework - What are homework and project '\n",
      "                             'deadlines?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'You can find the latest and up-to-date deadlines '\n",
      "                         'here: '\n",
      "                         'https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\n'\n",
      "                         'Also, take note of Announcements from @Au-Tomator '\n",
      "                         'for any extensions or other news. Or, the form may '\n",
      "                         'also show the updated deadline, if Instructor(s) has '\n",
      "                         'updated it.'},\n",
      "                {'question': 'Homework - Are late submissions of homework '\n",
      "                             'allowed?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'No, late submissions are not allowed. But if the '\n",
      "                         'form is still not closed and it’s after the due '\n",
      "                         'date, you can still submit the homework. confirm '\n",
      "                         'your submission by the date-timestamp on the Course '\n",
      "                         'page.y\\n'\n",
      "                         'Older news:[source1] [source2]'},\n",
      "                {'question': 'Homework - What is the homework URL in the '\n",
      "                             'homework link?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Answer: In short, it’s your repository on github, '\n",
      "                         'gitlab, bitbucket, etc\\n'\n",
      "                         'In long, your repository or any other location you '\n",
      "                         'have your code where a reasonable person would look '\n",
      "                         'at it and think yes, you went through the week and '\n",
      "                         'exercises.'},\n",
      "                {'question': 'Homework and Leaderboard - what is the system '\n",
      "                             'for points in the course management platform?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'After you submit your homework it will be graded '\n",
      "                         'based on the amount of questions in a particular '\n",
      "                         'homework. You can see how many points you have right '\n",
      "                         'on the page of the homework up top. Additionally in '\n",
      "                         'the leaderboard you will find the sum of all points '\n",
      "                         'you’ve earned - points for Homeworks, FAQs and '\n",
      "                         'Learning in Public. If homework is clear, others '\n",
      "                         'work as follows: if you submit something to FAQ, you '\n",
      "                         'get one point, for each learning in a public link '\n",
      "                         'you get one point.\\n'\n",
      "                         '(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)'},\n",
      "                {'question': 'Leaderboard - I am not on the leaderboard / how '\n",
      "                             'do I know which one I am on the leaderboard?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'When you set up your account you are automatically '\n",
      "                         'assigned a random name such as “Lucid Elbakyan” for '\n",
      "                         'example. If you want to see what your Display name '\n",
      "                         'is.\\n'\n",
      "                         'Go to the Homework submission link →  '\n",
      "                         'https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 '\n",
      "                         '- Log in > Click on ‘Data Engineering Zoom Camp '\n",
      "                         '2024’ > click on ‘Edit Course Profile’ - your '\n",
      "                         'display name is here, you can also change it should '\n",
      "                         'you wish:'},\n",
      "                {'question': 'Environment - Is Python 3.9 still the '\n",
      "                             'recommended version to use in 2024?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes, for simplicity (of troubleshooting against the '\n",
      "                         'recorded videos) and stability. [source]\\n'\n",
      "                         'But Python 3.10 and 3.11 should work fine.'},\n",
      "                {'question': 'Environment - Should I use my local machine, '\n",
      "                             'GCP, or GitHub Codespaces for my environment?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'You can set it up on your laptop or PC if you prefer '\n",
      "                         'to work locally from your laptop or PC.\\n'\n",
      "                         'You might face some challenges, especially for '\n",
      "                         'Windows users. If you face cnd2\\n'\n",
      "                         'If you prefer to work on the local machine, you may '\n",
      "                         'start with the week 1 Introduction to Docker and '\n",
      "                         'follow through.\\n'\n",
      "                         'However, if you prefer to set up a virtual machine, '\n",
      "                         'you may start with these first:\\n'\n",
      "                         'Using GitHub Codespaces\\n'\n",
      "                         'Setting up the environment on a cloudV Mcodespace\\n'\n",
      "                         'I decided to work on a virtual machine because I '\n",
      "                         'have different laptops & PCs for my home & office, '\n",
      "                         'so I can work on this boot camp virtually anywhere.'},\n",
      "                {'question': 'Environment - Is GitHub codespaces an '\n",
      "                             'alternative to using cli/git bash to ingest the '\n",
      "                             'data and create a docker file?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'GitHub Codespaces offers you computing Linux '\n",
      "                         'resources with many pre-installed tools (Docker, '\n",
      "                         'Docker Compose, Python).\\n'\n",
      "                         'You can also open any GitHub repository in a GitHub '\n",
      "                         'Codespace.'},\n",
      "                {'question': 'Environment - Do we really have to use GitHub '\n",
      "                             'codespaces? I already have PostgreSQL & Docker '\n",
      "                             'installed.',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': \"It's up to you which platform and environment you \"\n",
      "                         'use for the course.\\n'\n",
      "                         'Github codespaces or GCP VM are just possible '\n",
      "                         'options, but you can do the entire course from your '\n",
      "                         'laptop.'},\n",
      "                {'question': 'Environment - Do I need both GitHub Codespaces '\n",
      "                             'and GCP?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Choose the approach that aligns the most with your '\n",
      "                         'idea for the end project\\n'\n",
      "                         'One of those should suffice. However, BigQuery, '\n",
      "                         'which is part of GCP, will be used, so learning that '\n",
      "                         'is probably a better option. Or you can set up a '\n",
      "                         'local environment for most of this course.'},\n",
      "                {'question': 'This happens when attempting to connect to a GCP '\n",
      "                             'VM using VSCode on a Windows machine. Changing '\n",
      "                             'registry value in registry editor',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': '1. To open Run command window, you can either:\\n'\n",
      "                         \"(1-1) Use the shortcut keys: 'Windows + R', or\\n\"\n",
      "                         '(1-2) Right Click \"Start\", and click \"Run\" to open.\\n'\n",
      "                         '2. Registry Values Located in Registry Editor, to '\n",
      "                         \"open it: Type 'regedit' in the Run command window, \"\n",
      "                         \"and then press Enter.' 3. Now you can change the \"\n",
      "                         'registry values \"Autorun\" in '\n",
      "                         '\"HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Command '\n",
      "                         'Processor\" from \"if exists\" to a blank.\\n'\n",
      "                         'Alternatively, You can simplify the solution by '\n",
      "                         'deleting the fingerprint saved within the '\n",
      "                         'known_hosts file. In Windows, this file is placed '\n",
      "                         'at  C:\\\\Users\\\\<your_user_name>\\\\.ssh\\\\known_host'},\n",
      "                {'question': 'Environment - Why are we using GCP and not other '\n",
      "                             'cloud providers?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'For uniformity at least, but you’re not restricted '\n",
      "                         'to GCP, you can use other cloud platforms like AWS '\n",
      "                         'if you’re comfortable with other cloud platforms, '\n",
      "                         'since you get every service that’s been provided by '\n",
      "                         'GCP in Azure and AWS or others..\\n'\n",
      "                         'Because everyone has a google account, GCP has a '\n",
      "                         'free trial period and gives $300 in credits  to new '\n",
      "                         'users. Also, we are working with BigQuery, which is '\n",
      "                         'a part of GCP.\\n'\n",
      "                         'Note that to sign up for a free GCP account, you '\n",
      "                         'must have a valid credit card.'},\n",
      "                {'question': 'Should I pay for cloud services?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'No, if you use GCP and take advantage of their free '\n",
      "                         'trial.'},\n",
      "                {'question': 'Environment - The GCP and other cloud providers '\n",
      "                             'are unavailable in some countries. Is it '\n",
      "                             'possible to provide a guide to installing a home '\n",
      "                             'lab?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'You can do most of the course without a cloud. '\n",
      "                         'Almost everything we use (excluding BigQuery) can be '\n",
      "                         'run locally. We won’t be able to provide guidelines '\n",
      "                         'for some things, but most of the materials are '\n",
      "                         'runnable without GCP.\\n'\n",
      "                         'For everything in the course, there’s a local '\n",
      "                         'alternative. You could even do the whole course '\n",
      "                         'locally.'},\n",
      "                {'question': 'Environment - I want to use AWS. May I do that?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes, you can. Just remember to adapt all the '\n",
      "                         'information on the videos to AWS. Besides, the final '\n",
      "                         'capstone will be evaluated based on the task: Create '\n",
      "                         'a data pipeline! Develop a visualisation!\\n'\n",
      "                         'The problem would be when you need help. You’d need '\n",
      "                         'to rely on  fellow coursemates who also use AWS (or '\n",
      "                         'have experience using it before), which might be in '\n",
      "                         'smaller numbers than those learning the course with '\n",
      "                         'GCP.\\n'\n",
      "                         'Also see Is it possible to use x tool instead of the '\n",
      "                         'one tool you use?'},\n",
      "                {'question': 'Besides the “Office Hour” which are the live '\n",
      "                             'zoom calls?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'We will probably have some calls during the Capstone '\n",
      "                         'period to clear some questions but it will be '\n",
      "                         'announced in advance if that happens.'},\n",
      "                {'question': 'Are we still using the NYC Trip data for January '\n",
      "                             '2021? Or are we using the 2022 data?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'We will use the same data, as the project will '\n",
      "                         'essentially remain the same as last year’s. The data '\n",
      "                         'is available here'},\n",
      "                {'question': 'Is the 2022 repo deleted?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'No, but we moved the 2022 stuff here'},\n",
      "                {'question': 'Can I use Airflow instead for my final project?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes, you can use any tool you want for your '\n",
      "                         'project.'},\n",
      "                {'question': 'Is it possible to use tool “X” instead of the '\n",
      "                             'one tool you use in the course?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes, this applies if you want to use Airflow or '\n",
      "                         'Prefect instead of Mage, AWS or Snowflake instead of '\n",
      "                         'GCP products or Tableau instead of Metabase or '\n",
      "                         'Google data studio.\\n'\n",
      "                         'The course covers 2 alternative data stacks, one '\n",
      "                         'using GCP and one using local installation of '\n",
      "                         'everything. You can use one of them or use your tool '\n",
      "                         'of choice.\\n'\n",
      "                         'Should you consider it instead of the one tool you '\n",
      "                         'use? That we can’t support you if you choose to use '\n",
      "                         'a different stack, also you would need to explain '\n",
      "                         'the different choices of tool for the peer review of '\n",
      "                         'your capstone project.'},\n",
      "                {'question': 'How can we contribute to the course?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Star the repo! Share it with friends if you find it '\n",
      "                         'useful ❣️\\n'\n",
      "                         'Create a PR if you see you can improve the text or '\n",
      "                         'the structure of the repository.'},\n",
      "                {'question': 'Environment - Is the course '\n",
      "                             '[Windows/mac/Linux/...] friendly?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes! Linux is ideal but technically it should not '\n",
      "                         'matter. Students last year used all 3 OSes '\n",
      "                         'successfully'},\n",
      "                {'question': 'Environment - Roadblock for Windows users in '\n",
      "                             'modules with *.sh (shell scripts).',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Have no idea how past cohorts got past this as I '\n",
      "                         \"haven't read old slack messages, and no FAQ entries \"\n",
      "                         'that I can find.\\n'\n",
      "                         'Later modules (module-05 & RisingWave workshop) use '\n",
      "                         'shell scripts in *.sh files and most Windows users '\n",
      "                         'not using WSL would hit a wall and cannot continue, '\n",
      "                         'even in git bash or MINGW64. This is why WSL '\n",
      "                         'environment setup is recommended from the start.'},\n",
      "                {'question': 'Any books or additional resources you recommend?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes to both! check out this document: '\n",
      "                         'https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md'},\n",
      "                {'question': 'Project - What is Project Attemp #1 and Project '\n",
      "                             'Attempt #2 exactly?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'You will have two attempts for a project. If the '\n",
      "                         'first project deadline is over and you’re late or '\n",
      "                         'you submit the project and fail the first attempt, '\n",
      "                         'you have another chance to submit the project with '\n",
      "                         'the second attempt.'},\n",
      "                {'question': 'How to troubleshoot issues',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'The first step is to try to solve the issue on your '\n",
      "                         'own. Get used to solving problems and reading '\n",
      "                         'documentation. This will be a real life skill you '\n",
      "                         'need when employed. [ctrl+f] is your friend, use it! '\n",
      "                         'It is a universal shortcut and works in all '\n",
      "                         'apps/browsers.\\n'\n",
      "                         'What does the error say? There will often be a '\n",
      "                         'description of the error or instructions on what is '\n",
      "                         'needed or even how to fix it. I have even seen a '\n",
      "                         'link to the solution. Does it reference a specific '\n",
      "                         'line of your code?\\n'\n",
      "                         'Restart app or server/pc.\\n'\n",
      "                         'Google it, use ChatGPT, Bing AI etc.\\n'\n",
      "                         'It is going to be rare that you are the first to '\n",
      "                         'have the problem, someone out there has posted the '\n",
      "                         'fly issue and likely the solution.\\n'\n",
      "                         'Search using: <technology> <problem statement>. '\n",
      "                         'Example: pgcli error column c.relhasoids does not '\n",
      "                         'exist.\\n'\n",
      "                         'There are often different solutions for the same '\n",
      "                         'problem due to variation in environments.\\n'\n",
      "                         'Check the tech’s documentation. Use its search if '\n",
      "                         'available or use the browsers search function.\\n'\n",
      "                         'Try uninstall (this may remove the bad actor) and '\n",
      "                         'reinstall of application or reimplementation of '\n",
      "                         'action. Remember to restart the server/pc for '\n",
      "                         'reinstalls.\\n'\n",
      "                         'Sometimes reinstalling fails to resolve the issue '\n",
      "                         'but works if you uninstall first.\\n'\n",
      "                         'Post your question to Stackoverflow. Read the '\n",
      "                         'Stackoverflow guide on posting good questions.\\n'\n",
      "                         'https://stackoverflow.com/help/how-to-ask\\n'\n",
      "                         'This will be your real life. Ask an expert in the '\n",
      "                         'future (in addition to coworkers).\\n'\n",
      "                         'Ask in Slack\\n'\n",
      "                         'Before asking a question,\\n'\n",
      "                         'Check Pins (where the shortcut to the repo and this '\n",
      "                         'FAQ is located)\\n'\n",
      "                         'Use the slack app’s search function\\n'\n",
      "                         'Use the bot @ZoomcampQABot to do the search for you\\n'\n",
      "                         'check the FAQ (this document), use search [ctrl+f]\\n'\n",
      "                         'When asking a question, include as much information '\n",
      "                         'as possible:\\n'\n",
      "                         'What are you coding on? What OS?\\n'\n",
      "                         'What command did you run, which video did you '\n",
      "                         'follow? Etc etc\\n'\n",
      "                         'What error did you get? Does it have a line number '\n",
      "                         'to the “offending” code and have you check it for '\n",
      "                         'typos?\\n'\n",
      "                         'What have you tried that did not work? This answer '\n",
      "                         'is crucial as without it, helpers would ask you to '\n",
      "                         'do the suggestions in the error log first. Or just '\n",
      "                         'read this FAQ document.\\n'\n",
      "                         'DO NOT use screenshots, especially don’t take '\n",
      "                         'pictures from a phone.\\n'\n",
      "                         'DO NOT tag instructors, it may discourage others '\n",
      "                         'from helping you. Copy and paste errors; if it’s '\n",
      "                         'long, just post it in a reply to your thread.\\n'\n",
      "                         'Use ``` for formatting your code.\\n'\n",
      "                         'Use the same thread for the conversation (that means '\n",
      "                         'reply to your own thread).\\n'\n",
      "                         'DO NOT create multiple posts to discuss the issue.\\n'\n",
      "                         'learYou may create a new post if the issue reemerges '\n",
      "                         'down the road. Describe what has changed in the '\n",
      "                         'environment.\\n'\n",
      "                         'Provide additional information in the same thread of '\n",
      "                         'the steps you have taken for resolution.\\n'\n",
      "                         'Take a break and come back later. You will be amazed '\n",
      "                         'at how often you figure out the solution after '\n",
      "                         'letting your brain rest. Get some fresh air, '\n",
      "                         'workout, play a video game, watch a tv show, '\n",
      "                         'whatever allows your brain to not think about it for '\n",
      "                         'a little while or even until the next day.\\n'\n",
      "                         'Remember technology issues in real life sometimes '\n",
      "                         'take days or even weeks to resolve.\\n'\n",
      "                         \"If somebody helped you with your problem and it's \"\n",
      "                         'not in the FAQ, please add it there. It will help '\n",
      "                         'other students.'},\n",
      "                {'question': 'How to ask questions',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'When the troubleshooting guide above does not help '\n",
      "                         'resolve it and you need another pair of eyeballs to '\n",
      "                         'spot mistakes. When asking a question, include as '\n",
      "                         'much information as possible:\\n'\n",
      "                         'What are you coding on? What OS?\\n'\n",
      "                         'What command did you run, which video did you '\n",
      "                         'follow? Etc etc\\n'\n",
      "                         'What error did you get? Does it have a line number '\n",
      "                         'to the “offending” code and have you check it for '\n",
      "                         'typos?\\n'\n",
      "                         'What have you tried that did not work? This answer '\n",
      "                         'is crucial as without it, helpers would ask you to '\n",
      "                         'do the suggestions in the error log first. Or just '\n",
      "                         'read this FAQ document.'},\n",
      "                {'question': 'How do I use Git / GitHub for this course?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'After you create a GitHub account, you should clone '\n",
      "                         'the course repo to your local machine using the '\n",
      "                         'process outlined in this video: Git for Everybody: '\n",
      "                         'How to Clone a Repository from GitHub\\n'\n",
      "                         'Having this local repository on your computer will '\n",
      "                         'make it easy for you to access the instructors’ code '\n",
      "                         'and make pull requests (if you want to add your own '\n",
      "                         'notes or make changes to the course content).\\n'\n",
      "                         'You will probably also create your own repositories '\n",
      "                         'that host your notes, versions of your file, to do '\n",
      "                         'this. Here is a great tutorial that shows you how to '\n",
      "                         'do this: '\n",
      "                         'https://www.atlassian.com/git/tutorials/setting-up-a-repository\\n'\n",
      "                         'Remember to ignore large database, .csv, and .gz '\n",
      "                         'files, and other files that should not be saved to a '\n",
      "                         'repository. Use .gitignore for this: '\n",
      "                         'https://www.atlassian.com/git/tutorials/saving-changes/gitignore '\n",
      "                         'NEVER store passwords or keys in a git repo (even if '\n",
      "                         'that repo is set to private).\\n'\n",
      "                         'This is also a great resource: '\n",
      "                         'https://dangitgit.com/'},\n",
      "                {'question': 'VS Code: Tab using spaces',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Error: Makefile:2: *** missing separator.  Stop.\\n'\n",
      "                         'Solution: Tabs in document should be converted to '\n",
      "                         'Tab instead of spaces. Follow this stack.'},\n",
      "                {'question': 'Opening an HTML file with a Windows browser from '\n",
      "                             'Linux running on WSL',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'If you’re running Linux on Windows Subsystem for '\n",
      "                         'Linux (WSL) 2, you can open HTML files from the '\n",
      "                         'guest (Linux) with whatever Internet Browser you '\n",
      "                         'have installed on the host (Windows). Just install '\n",
      "                         'wslu and open the page with wslview <file>, for '\n",
      "                         'example:\\n'\n",
      "                         'wslview index.html\\n'\n",
      "                         'You can customise which browser to use by setting '\n",
      "                         'the BROWSER environment variable first. For '\n",
      "                         'example:\\n'\n",
      "                         \"export BROWSER='/mnt/c/Program \"\n",
      "                         \"Files/Firefox/firefox.exe'\"},\n",
      "                {'question': 'Set up Chrome Remote Desktop for Linux on '\n",
      "                             'Compute Engine',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'This tutorial shows you how to set up the Chrome '\n",
      "                         'Remote Desktop service on a Debian Linux virtual '\n",
      "                         'machine (VM) instance on Compute Engine. Chrome '\n",
      "                         'Remote Desktop allows you to remotely access '\n",
      "                         'applications with a graphical user interface.\\n'\n",
      "                         'Taxi Data - Yellow Taxi Trip Records downloading '\n",
      "                         'error, Error no or XML error webpage\\n'\n",
      "                         'When you try to download the 2021 data from TLC '\n",
      "                         'website, you get this error:\\n'\n",
      "                         'If you click on the link, and ERROR 403: Forbidden '\n",
      "                         'on the terminal.\\n'\n",
      "                         'We have a backup, so use it instead: '\n",
      "                         'https://github.com/DataTalksClub/nyc-tlc-data\\n'\n",
      "                         'So the link should be '\n",
      "                         'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\\n'\n",
      "                         'Note: Make sure to unzip the “gz” file (no, the '\n",
      "                         '“unzip” command won’t work for this.)\\n'\n",
      "                         '“gzip -d file.gz”g'},\n",
      "                {'question': 'Taxi Data - How to handle taxi data files, now '\n",
      "                             'that the files are available as *.csv.gz?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'In this video, we store the data file as '\n",
      "                         '“output.csv”. The data file won’t store correctly if '\n",
      "                         'the file extension is csv.gz instead of csv. One '\n",
      "                         'alternative is to replace csv_name = “output.cs -v” '\n",
      "                         'with the file name given at the end of the URL. '\n",
      "                         'Notice that the URL for the yellow taxi data is: '\n",
      "                         'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz '\n",
      "                         'where the highlighted part is the name of the file. '\n",
      "                         'We can parse this file name from the URL and use it '\n",
      "                         'as csv_name. That is, we can replace csv_name = '\n",
      "                         '“output.csv” with\\n'\n",
      "                         'csv_name = url.split(“/”)[-1] . Then when we use '\n",
      "                         'csv_name to using pd.read_csv, there won’t be an '\n",
      "                         'issue even though the file name really has the '\n",
      "                         'extension csv.gz instead of csv since the pandas '\n",
      "                         'read_csv function can read csv.gz files directly.'},\n",
      "                {'question': 'Taxi Data - Data Dictionary for NY Taxi data?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Yellow Trips: '\n",
      "                         'https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\\n'\n",
      "                         'Green Trips: '\n",
      "                         'https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf'},\n",
      "                {'question': 'Taxi Data - Unzip Parquet file',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'You can unzip this downloaded parquet file, in the '\n",
      "                         'command line. The result is a csv file which can be '\n",
      "                         'imported with pandas using the pd.read_csv() shown '\n",
      "                         'in the videos.\\n'\n",
      "                         '‘’’gunzip green_tripdata_2019-09.csv.gz’’’\\n'\n",
      "                         'SOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON '\n",
      "                         'SCRIPT ingest_data.py\\n'\n",
      "                         'In the def main(params) add this line\\n'\n",
      "                         \"parquet_name= 'output.parquet'\\n\"\n",
      "                         'Then edit the code which downloads the files\\n'\n",
      "                         'os.system(f\"wget {url} -O {parquet_name}\")\\n'\n",
      "                         'Convert the download .parquet file to csv and rename '\n",
      "                         'as csv_name to keep it relevant to the rest of the '\n",
      "                         'code\\n'\n",
      "                         'df = pd.read_parquet(parquet_name)\\n'\n",
      "                         'df.to_csv(csv_name, index=False)'},\n",
      "                {'question': 'lwget is not recognized as an internal or '\n",
      "                             'external command',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': '“wget is not recognized as an internal or external '\n",
      "                         'command”, you need to install it.\\n'\n",
      "                         'On Ubuntu, run:\\n'\n",
      "                         '$ sudo apt-get install wget\\n'\n",
      "                         'On MacOS, the easiest way to install wget is to use '\n",
      "                         'Brew:\\n'\n",
      "                         '$ brew install wget\\n'\n",
      "                         'On Windows, the easiest way to install wget is to '\n",
      "                         'use Chocolatey:\\n'\n",
      "                         '$ choco install wget\\n'\n",
      "                         'Or you can download a binary '\n",
      "                         '(https://gnuwin32.sourceforge.net/packages/wget.htm) '\n",
      "                         'and put it to any location in your PATH (e.g. '\n",
      "                         'C:/tools/)\\n'\n",
      "                         'Also, you can following this step to install Wget on '\n",
      "                         'MS Windows\\n'\n",
      "                         '* Download the latest wget binary for windows from '\n",
      "                         '[eternallybored] '\n",
      "                         '(https://eternallybored.org/misc/wget/) (they are '\n",
      "                         'available as a zip with documentation, or just an '\n",
      "                         'exe)\\n'\n",
      "                         '* If you downloaded the zip, extract all (if windows '\n",
      "                         'built in zip utility gives an error, use [7-zip] '\n",
      "                         '(https://7-zip.org/)).\\n'\n",
      "                         '* Rename the file `wget64.exe` to `wget.exe` if '\n",
      "                         'necessary.\\n'\n",
      "                         '* Move wget.exe to your `Git\\\\mingw64\\\\bin\\\\`.\\n'\n",
      "                         'Alternatively, you can use a Python wget library, '\n",
      "                         'but instead of simply using “wget” you’ll need to '\n",
      "                         'use\\n'\n",
      "                         'python -m wget\\n'\n",
      "                         'You need to install it with pip first:\\n'\n",
      "                         'pip install wget\\n'\n",
      "                         'Alternatively, you can just paste the file URL into '\n",
      "                         'your web browser and download the file normally that '\n",
      "                         'way. You’ll want to move the resulting file into '\n",
      "                         'your working directory.\\n'\n",
      "                         'Also recommended a look at the python library '\n",
      "                         'requests for the loading gz file  '\n",
      "                         'https://pypi.org/project/requests'},\n",
      "                {'question': 'wget - ERROR: cannot verify <website> '\n",
      "                             'certificate  (MacOS)',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Firstly, make sure that you add “!” before wget if '\n",
      "                         'you’re running your command in a Jupyter Notebook or '\n",
      "                         'CLI. Then, you can check one of this 2 things (from '\n",
      "                         'CLI):\\n'\n",
      "                         'Using the Python library wget you installed with '\n",
      "                         'pip, try python -m wget <url>\\n'\n",
      "                         'Write the usual command and add '\n",
      "                         '--no-check-certificate at the end. So it should be:\\n'\n",
      "                         '!wget <website_url> --no-check-certificate'},\n",
      "                {'question': 'Git Bash - Backslash as an escape character in '\n",
      "                             'Git Bash for Windows',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'For those who wish to use the backslash as an escape '\n",
      "                         'character in Git Bash for Windows (as Alexey '\n",
      "                         'normally does), type in the terminal: '\n",
      "                         'bash.escapeChar=\\\\ (no need to include in .bashrc)'},\n",
      "                {'question': 'GitHub Codespaces - How to store secrets',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Instruction on how to store secrets that will be '\n",
      "                         'avialable in GitHub  Codespaces.\\n'\n",
      "                         'Managing your account-specific secrets for GitHub '\n",
      "                         'Codespaces - GitHub Docs'},\n",
      "                {'question': 'Docker - Cannot connect to Docker daemon at '\n",
      "                             'unix:///var/run/docker.sock. Is the docker '\n",
      "                             'daemon running?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': \"Make sure you're able to start the Docker daemon, \"\n",
      "                         'and check the issue immediately down below:\\n'\n",
      "                         'And don’t forget to update the wsl in powershell '\n",
      "                         'the  command is wsl –update'},\n",
      "                {'question': 'Docker - Error during connect: In the default '\n",
      "                             'daemon configuration on Windows, the docker '\n",
      "                             'client must be run with elevated privileges to '\n",
      "                             'connect.: Post: '\n",
      "                             '\"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" '\n",
      "                             ': open //./pipe/docker_engine: The system cannot '\n",
      "                             'find the file specified',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'As the official Docker for Windows documentation '\n",
      "                         'says, the Docker engine can either use the\\n'\n",
      "                         'Hyper-V or WSL2 as its backend. However, a few '\n",
      "                         'constraints might apply\\n'\n",
      "                         'Windows 10 Pro / 11 Pro Users: \\n'\n",
      "                         'In order to use Hyper-V as its back-end, you MUST '\n",
      "                         'have it enabled first, which you can do by following '\n",
      "                         'the tutorial: Enable Hyper-V Option on Windows 10 / '\n",
      "                         '11\\n'\n",
      "                         'Windows 10 Home / 11 Home Users: \\n'\n",
      "                         \"On the other hand, Users of the 'Home' version do \"\n",
      "                         'NOT have the option Hyper-V option enabled, which '\n",
      "                         'means, you can only get Docker up and running using '\n",
      "                         'the WSL2 credentials(Windows Subsystem for Linux). '\n",
      "                         'Url\\n'\n",
      "                         'You can find the detailed instructions to do so '\n",
      "                         'here: rt '\n",
      "                         'ghttps://pureinfotech.com/install-wsl-windows-11/\\n'\n",
      "                         'In case, you run into another issue while trying to '\n",
      "                         'install WSL2 (WslRegisterDistribution failed with '\n",
      "                         'error: 0x800701bc), Make sure you update the WSL2 '\n",
      "                         'Linux Kernel, following the guidelines here: \\n'\n",
      "                         '\\n'\n",
      "                         'https://github.com/microsoft/WSL/issues/5393'},\n",
      "                {'question': 'Docker - docker pull dbpage',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Whenever a `docker pull is performed (either '\n",
      "                         'manually or by `docker-compose up`), it attempts to '\n",
      "                         'fetch the given image name (pgadmin4, for the '\n",
      "                         'example above) from a repository (dbpage).\\n'\n",
      "                         'IF the repository is public, the fetch and download '\n",
      "                         'happens without any issue whatsoever.\\n'\n",
      "                         'For instance:\\n'\n",
      "                         'docker pull postgres:13\\n'\n",
      "                         'docker pull dpage/pgadmin4\\n'\n",
      "                         'BE ADVISED:\\n'\n",
      "                         '\\n'\n",
      "                         \"The Docker Images we'll be using throughout the Data \"\n",
      "                         'Engineering Zoomcamp are all public (except when or '\n",
      "                         'if explicitly said otherwise by the instructors or '\n",
      "                         'co-instructors).\\n'\n",
      "                         '\\n'\n",
      "                         'Meaning: you are NOT required to perform a docker '\n",
      "                         'login to fetch them. \\n'\n",
      "                         '\\n'\n",
      "                         'So if you get the message above saying \"docker '\n",
      "                         \"login': denied: requested access to the resource is \"\n",
      "                         'denied. That is most likely due to a typo in your '\n",
      "                         'image name:\\n'\n",
      "                         '\\n'\n",
      "                         'For instance:\\n'\n",
      "                         '$ docker pull dbpage/pgadmin4\\n'\n",
      "                         'Will throw that exception telling you \"repository '\n",
      "                         \"does not exist or may require 'docker login'\\n\"\n",
      "                         'Error response from daemon: pull access denied for '\n",
      "                         'dbpage/pgadmin4, repository does not exist or \\n'\n",
      "                         \"may require 'docker login': denied: requested access \"\n",
      "                         'to the resource is denied\\n'\n",
      "                         'But that actually happened because the actual image '\n",
      "                         'is dpage/pgadmin4 and NOT dbpage/pgadmin4\\n'\n",
      "                         'How to fix it:\\n'\n",
      "                         '$ docker pull dpage/pgadmin4\\n'\n",
      "                         'EXTRA NOTES:\\n'\n",
      "                         \"In the real world, occasionally, when you're working \"\n",
      "                         'for a company or closed organisation, the Docker '\n",
      "                         \"image you're trying to fetch might be under a \"\n",
      "                         'private repo that your DockerHub Username was '\n",
      "                         'granted access to.\\n'\n",
      "                         'For which cases, you must first execute:\\n'\n",
      "                         '$ docker login\\n'\n",
      "                         'Fill in the details of your username and password.\\n'\n",
      "                         'And only then perform the `docker pull` against that '\n",
      "                         'private repository\\n'\n",
      "                         'Why am I encountering a \"permission denied\" error '\n",
      "                         'when creating a PostgreSQL Docker container for the '\n",
      "                         'New York Taxi Database with a mounted volume on '\n",
      "                         'macOS M1?\\n'\n",
      "                         'Issue Description:\\n'\n",
      "                         'When attempting to run a Docker command similar to '\n",
      "                         'the one below:\\n'\n",
      "                         'docker run -it \\\\\\n'\n",
      "                         '-e POSTGRES_USER=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_PASSWORD=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_DB=\"ny_taxi\" \\\\\\n'\n",
      "                         '-v '\n",
      "                         '$(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data '\n",
      "                         '\\\\\\n'\n",
      "                         '-p 5432:5432 \\\\mount\\n'\n",
      "                         'postgres:13\\n'\n",
      "                         'You encounter the error message:\\n'\n",
      "                         'docker: Error response from daemon: error while '\n",
      "                         'creating mount source path '\n",
      "                         \"'/path/to/ny_taxi_postgres_data': chown \"\n",
      "                         '/path/to/ny_taxi_postgres_data: permission denied.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         '1- Stop Rancher Desktop:\\n'\n",
      "                         'If you are using Rancher Desktop and face this '\n",
      "                         'issue, stop Rancher Desktop to resolve compatibility '\n",
      "                         'problems.\\n'\n",
      "                         '2- Install Docker Desktop:\\n'\n",
      "                         'Install Docker Desktop, ensuring that it is properly '\n",
      "                         'configured and has the required permissions.\\n'\n",
      "                         '2-Retry Docker Command:\\n'\n",
      "                         'Run the Docker command again after switching to '\n",
      "                         'Docker Desktop. This step resolves compatibility '\n",
      "                         'issues on some systems.\\n'\n",
      "                         'Note: The issue occurred because Rancher Desktop was '\n",
      "                         'in use. Switching to Docker Desktop resolves '\n",
      "                         'compatibility problems and allows for the successful '\n",
      "                         'creation of PostgreSQL containers with mounted '\n",
      "                         'volumes for the New York Taxi Database on macOS M1.'},\n",
      "                {'question': 'Docker - can’t delete local folder that mounted '\n",
      "                             'to docker volume',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'When I runned command to create postgre in docker '\n",
      "                         'container it created folder on my local machine to '\n",
      "                         'mount it to volume inside container. It has write '\n",
      "                         'and read protection and owned by user 999, so I '\n",
      "                         'could not delete it by simply drag to trash.  My '\n",
      "                         'obsidian could not started due to access error, so I '\n",
      "                         'had to change placement of this folder and delete '\n",
      "                         'old folder by this command:\\n'\n",
      "                         'sudo rm -r -f docker_test/\\n'\n",
      "                         '- where `rm` - remove, `-r` - recursively, `-f` - '\n",
      "                         'force, `docker_test/` - folder.'},\n",
      "                {'question': \"Docker - Docker won't start or is stuck in \"\n",
      "                             'settings (Windows 10 / 11)',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': \"First off, make sure you're running the latest \"\n",
      "                         'version of Docker for Windows, which you can '\n",
      "                         'download from here. Sometimes using the menu to '\n",
      "                         '\"Upgrade\" doesn\\'t work (which is another clear '\n",
      "                         'indicator for you to uninstall, and reinstall with '\n",
      "                         'the latest version)\\n'\n",
      "                         'If docker is stuck on starting, first try to switch '\n",
      "                         'containers by right clicking the docker symbol from '\n",
      "                         'the running programs and switch the containers from '\n",
      "                         'windows to linux or vice versa\\n'\n",
      "                         '[Windows 10 / 11 Pro Edition] The Pro Edition of '\n",
      "                         'Windows can run Docker either by using Hyper-V or '\n",
      "                         'WSL2 as its backend (Docker Engine)\\n'\n",
      "                         'In order to use Hyper-V as its back-end, you MUST '\n",
      "                         'have it enabled first, which you can do by following '\n",
      "                         'the tutorial: Enable Hyper-V Option on Windows 10 / '\n",
      "                         '11\\n'\n",
      "                         'If you opt-in for WSL2, you can follow the same '\n",
      "                         'steps as detailed in the tutorial here'},\n",
      "                {'question': 'Should I run docker commands from the windows '\n",
      "                             'file system or a file system of a Linux '\n",
      "                             'distribution in WSL?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'It is recommended by the Docker do\\n'\n",
      "                         \"[Windows 10 / 11 Home Edition] If you're running a \"\n",
      "                         'Home Edition, you can still make it work with WSL2 '\n",
      "                         '(Windows Subsystem for Linux) by following the '\n",
      "                         'tutorial here\\n'\n",
      "                         'If even after making sure your WSL2 (or Hyper-V) is '\n",
      "                         'set up accordingly, Docker remains stuck, you can '\n",
      "                         'try the option to Reset to Factory Defaults or do a '\n",
      "                         'fresh install.'},\n",
      "                {'question': 'Docker - cs to store all code in your default '\n",
      "                             'Linux distro to get the best out of file system '\n",
      "                             'performance (since Docker runs on WSL2 backend '\n",
      "                             'by default for Windows 10 Home / Windows 11 Home '\n",
      "                             'users).',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'More info in the Docker Docs on Best Practises'},\n",
      "                {'question': 'Docker - The input device is not a TTY (Docker '\n",
      "                             'run for Windows)',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'You may have this error:\\n'\n",
      "                         '$ docker run -it ubuntu bash\\n'\n",
      "                         'the input device is not a TTY. If you are using '\n",
      "                         \"mintty, try prefixing the command with 'winpty'\\n\"\n",
      "                         'error:\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Use winpty before docker command (source)\\n'\n",
      "                         '$ winpty docker run -it ubuntu bash\\n'\n",
      "                         'You also can make an alias:\\n'\n",
      "                         'echo \"alias docker=\\'winpty docker\\'\" >> ~/.bashrc\\n'\n",
      "                         'OR\\n'\n",
      "                         'echo \"alias docker=\\'winpty docker\\'\" >> '\n",
      "                         '~/.bash_profile'},\n",
      "                {'question': 'Docker - Cannot pip install on Docker container '\n",
      "                             '(Windows)',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'You may have this error:\\n'\n",
      "                         'Retrying (Retry(total=4, connect=None, read=None, '\n",
      "                         'redirect=None, status=None)) after connection broken '\n",
      "                         \"by 'NewConnectionError('<pip._vendor.u\\n\"\n",
      "                         'rllib3.connection.HTTPSConnection object at '\n",
      "                         '0x7efe331cf790>: Failed to establish a new '\n",
      "                         'connection: [Errno -3] Temporary failure in name '\n",
      "                         \"resolution')':\\n\"\n",
      "                         '/simple/pandas/\\n'\n",
      "                         'Possible solution might be:\\n'\n",
      "                         '$ winpty docker run -it --dns=8.8.8.8 '\n",
      "                         '--entrypoint=bash python:3.9'},\n",
      "                {'question': 'Docker - ny_taxi_postgres_data is empty',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Even after properly running the docker script the '\n",
      "                         'folder is empty in the vs code  then try this (For '\n",
      "                         'Windows)\\n'\n",
      "                         'winpty docker run -it \\\\\\n'\n",
      "                         '-e POSTGRES_USER=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_PASSWORD=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_DB=\"ny_taxi\" \\\\\\n'\n",
      "                         '-v '\n",
      "                         '\"C:\\\\Users\\\\abhin\\\\dataengg\\\\DE_Project_git_connected\\\\DE_OLD\\\\week1_set_up\\\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" '\n",
      "                         '\\\\\\n'\n",
      "                         '-p 5432:5432 \\\\\\n'\n",
      "                         'postgres:13\\n'\n",
      "                         'Here quoting the absolute path in  the -v parameter '\n",
      "                         'is solving the issue and all the files are visible '\n",
      "                         'in the Vs-code ny_taxi folder as shown in the video'},\n",
      "                {'question': 'dasDocker - Setting up Docker on Mac',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Check this article for details - Setting up docker '\n",
      "                         'in macOS\\n'\n",
      "                         'From researching it seems this method might be out '\n",
      "                         'of date, it seems that since docker changed their '\n",
      "                         'licensing model, the above is a bit hit and miss. '\n",
      "                         'What worked for me was to just go to the docker '\n",
      "                         'website and download their dmg. Haven’t had an issue '\n",
      "                         'with that method.'},\n",
      "                {'question': '1Docker - Could not change permissions of '\n",
      "                             'directory \"/var/lib/postgresql/data\": Operation '\n",
      "                             'not permitted',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': '$ docker run -it\\\\\\n'\n",
      "                         '-e POSTGRES_USER=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_PASSWORD=\"admin\" \\\\\\n'\n",
      "                         '-e POSTGRES_DB=\"ny_taxi\" \\\\\\n'\n",
      "                         '-v '\n",
      "                         '\"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" '\n",
      "                         '\\\\\\n'\n",
      "                         '-p 5432:5432 \\\\\\n'\n",
      "                         'postgres:13\\n'\n",
      "                         'CCW\\n'\n",
      "                         'The files belonging to this database system will be '\n",
      "                         'owned by user \"postgres\".\\n'\n",
      "                         'This use The database cluster will be initialized '\n",
      "                         'with locale \"en_US.utf8\".\\n'\n",
      "                         'The default databerrorase encoding has accordingly '\n",
      "                         'been set to \"UTF8\".\\n'\n",
      "                         'xt search configuration will be set to \"english\".\\n'\n",
      "                         'Data page checksums are disabled.\\n'\n",
      "                         'fixing permissions on existing directory '\n",
      "                         '/var/lib/postgresql/data ... initdb: f\\n'\n",
      "                         'error: could not change permissions of directory '\n",
      "                         '\"/var/lib/postgresql/data\": Operation not permitted  '\n",
      "                         'volume\\n'\n",
      "                         'One way to solve this issue is to create a local '\n",
      "                         'docker volume and map it to postgres data directory '\n",
      "                         '/var/lib/postgresql/data\\n'\n",
      "                         'The input dtc_postgres_volume_local must match in '\n",
      "                         'both commands below\\n'\n",
      "                         '$ docker volume create --name '\n",
      "                         'dtc_postgres_volume_local -d local\\n'\n",
      "                         '$ docker run -it\\\\\\n'\n",
      "                         '-e POSTGRES_USER=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_PASSWORD=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_DB=\"ny_taxi\" \\\\\\n'\n",
      "                         '-v '\n",
      "                         'dtc_postgres_volume_local:/var/lib/postgresql/data '\n",
      "                         '\\\\\\n'\n",
      "                         '-p 5432:5432\\\\\\n'\n",
      "                         'postgres:13\\n'\n",
      "                         'To verify the above command works in (WSL2 Ubuntu '\n",
      "                         '22.04, verified 2024-Jan), go to the Docker Desktop '\n",
      "                         'app and look under Volumes - '\n",
      "                         'dtc_postgres_volume_local would be listed there. The '\n",
      "                         'folder ny_taxi_postgres_data would however be empty, '\n",
      "                         'since we used an alternative config.\\n'\n",
      "                         'An alternate error could be:\\n'\n",
      "                         'initdb: error: directory \"/var/lib/postgresql/data\" '\n",
      "                         'exists but is not empty\\n'\n",
      "                         'If you want to create a new database system, either '\n",
      "                         'remove or empthe directory '\n",
      "                         '\"/var/lib/postgresql/data\" or run initdb\\n'\n",
      "                         'witls'},\n",
      "                {'question': 'Docker - invalid reference format: repository '\n",
      "                             'name must be lowercase (Mounting volumes with '\n",
      "                             'Docker on Windows)',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Mapping volumes on Windows could be tricky. The way '\n",
      "                         'it was done in the course video doesn’t work for '\n",
      "                         'everyone.\\n'\n",
      "                         'First, if yo\\n'\n",
      "                         'move your data to some folder without spaces. E.g. '\n",
      "                         'if your code is in “C:/Users/Alexey Grigorev/git/…”, '\n",
      "                         'move it to “C:/git/…”\\n'\n",
      "                         'Try replacing the “-v” part with one of the '\n",
      "                         'following options:\\n'\n",
      "                         '-v '\n",
      "                         '/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n'\n",
      "                         '-v '\n",
      "                         '//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n'\n",
      "                         '-v '\n",
      "                         '/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n'\n",
      "                         '-v '\n",
      "                         '//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n'\n",
      "                         '--volume '\n",
      "                         '//driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\\n'\n",
      "                         'winpty docker run -it\\n'\n",
      "                         '-e POSTGRES_USER=\"root\"\\n'\n",
      "                         '-e POSTGRES_PASSWORD=\"root\"\\n'\n",
      "                         '-e POSTGRES_DB=\"ny_taxi\"\\n'\n",
      "                         '-v '\n",
      "                         '/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n'\n",
      "                         '-p 5432:5432\\n'\n",
      "                         'postgres:1\\n'\n",
      "                         'Try adding winpty before the whole command\\n'\n",
      "                         '3\\n'\n",
      "                         'win\\n'\n",
      "                         'Try adding quotes:\\n'\n",
      "                         '-v '\n",
      "                         '\"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n'\n",
      "                         '-v '\n",
      "                         '\"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n'\n",
      "                         '-v '\n",
      "                         '“/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n'\n",
      "                         '-v '\n",
      "                         '\"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n'\n",
      "                         '-v '\n",
      "                         '\"c:\\\\some\\\\path\\\\ny_taxi_postgres_data\":/var/lib/postgresql/data\\n'\n",
      "                         'Note:  (Window) if it automatically creates a folder '\n",
      "                         'called “ny_taxi_postgres_data;C” suggests you have '\n",
      "                         'problems with volume mapping, try deleting both '\n",
      "                         'folders and replacing “-v” part with other options. '\n",
      "                         'For me “//c/” works instead of “/c/”. And it will '\n",
      "                         'work by automatically creating a correct folder '\n",
      "                         'called “ny_taxi_postgres_data”.\\n'\n",
      "                         'A possible solution to this error would be to use '\n",
      "                         '/”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data '\n",
      "                         '(with quotes’ position varying as in the above '\n",
      "                         'list).\\n'\n",
      "                         'Yes for windows use the command it works perfectly '\n",
      "                         'fine\\n'\n",
      "                         '-v '\n",
      "                         '/”$(pwd)”/ny_taxi_postgres_data:/var/lib/postgresql/data\\n'\n",
      "                         'Important: note how the quotes are placed.\\n'\n",
      "                         'If none of these options work, you can use a volume '\n",
      "                         'name instead of the path:\\n'\n",
      "                         '-v ny_taxi_postgres_data:/var/lib/postgresql/data\\n'\n",
      "                         'For Mac: You can wrap $(pwd) with quotes like the '\n",
      "                         'highlighted.\\n'\n",
      "                         'docker run -it \\\\\\n'\n",
      "                         '-e POSTGRES_USER=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_PASSWORD=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_DB=\"ny_taxi\" \\\\\\n'\n",
      "                         '-v '\n",
      "                         '\"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data '\n",
      "                         '\\\\\\n'\n",
      "                         '-p 5432:5432 \\\\\\n'\n",
      "                         'Postgres:13\\n'\n",
      "                         'docker run -it \\\\\\n'\n",
      "                         '-e POSTGRES_USER=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_PASSWORD=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_DB=\"ny_taxi\" \\\\\\n'\n",
      "                         '-v '\n",
      "                         '\"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data '\n",
      "                         '\\\\\\n'\n",
      "                         '-p 5432:5432 \\\\\\n'\n",
      "                         'postgres:13\\n'\n",
      "                         'Source:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase'},\n",
      "                {'question': 'Docker - Error response from daemon: invalid '\n",
      "                             'mode: \\\\Program '\n",
      "                             'Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Change the mounting path. Replace it with one of '\n",
      "                         'following:\\n'\n",
      "                         '-v /e/zoomcamp/...:/var/lib/postgresql/data\\n'\n",
      "                         '-v '\n",
      "                         '/c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\\\ '\n",
      "                         '(leading slash in front of c:)'},\n",
      "                {'question': 'Docker - Error response from daemon: error while '\n",
      "                             'creating buildmount source path '\n",
      "                             \"'/run/desktop/mnt/host/c/<your path>': mkdir \"\n",
      "                             '/run/desktop/mnt/host/c: file exists',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'When you run this command second time\\n'\n",
      "                         'docker run -it \\\\\\n'\n",
      "                         '-e POSTGRES_USER=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_PASSWORD=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_DB=\"ny_taxi\" \\\\\\n'\n",
      "                         '-v <your path>:/var/lib/postgresql/data \\\\\\n'\n",
      "                         '-p 5432:5432 \\\\\\n'\n",
      "                         'postgres:13\\n'\n",
      "                         'The error message above could happen. That means you '\n",
      "                         'should not mount on the second run. This command '\n",
      "                         'helped me:\\n'\n",
      "                         'When you run this command second time\\n'\n",
      "                         'docker run -it \\\\\\n'\n",
      "                         '-e POSTGRES_USER=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_PASSWORD=\"root\" \\\\\\n'\n",
      "                         '-e POSTGRES_DB=\"ny_taxi\" \\\\\\n'\n",
      "                         '-p 5432:5432 \\\\\\n'\n",
      "                         'postgres:13'},\n",
      "                {'question': 'Docker - build error: error checking context: '\n",
      "                             \"'can't stat \"\n",
      "                             \"'/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\",\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'This error appeared when running the command: docker '\n",
      "                         'build -t taxi_ingest:v001 .\\n'\n",
      "                         'When feeding the database with the data the user id '\n",
      "                         'of the directory ny_taxi_postgres_data was changed '\n",
      "                         'to 999, so my user couldn’t access it when running '\n",
      "                         'the above command. Even though this is not the '\n",
      "                         'problem here it helped to raise the error due to the '\n",
      "                         'permission issue.\\n'\n",
      "                         'Since at this point we only need the files '\n",
      "                         'Dockerfile and ingest_data.py, to fix this error one '\n",
      "                         'can run the docker build command on a different '\n",
      "                         'directory (having only these two files).\\n'\n",
      "                         'A more complete explanation can be found here: '\n",
      "                         'https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\n'\n",
      "                         'You can fix the problem by changing the permission '\n",
      "                         'of the directory on ubuntu with following command:\\n'\n",
      "                         'sudo chown -R $USER dir_path\\n'\n",
      "                         'On windows follow the link: '\n",
      "                         'https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n'\n",
      "                         '\\n'\n",
      "                         '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n'\n",
      "                         '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay'},\n",
      "                {'question': 'Docker - ERRO[0000] error waiting for container: '\n",
      "                             'context canceled',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'You might have installed docker via snap. Run “sudo '\n",
      "                         'snap status docker” to verify.\\n'\n",
      "                         'If you have “error: unknown command \"status\", see '\n",
      "                         \"'snap help'.” as a response than deinstall docker \"\n",
      "                         'and install via the official website\\n'\n",
      "                         'Bind for 0.0.0.0:5432 failed: port is a'},\n",
      "                {'question': 'Docker - build error checking context: can’t '\n",
      "                             'stat '\n",
      "                             '‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Found the issue in the PopOS linux. It happened '\n",
      "                         'because our user didn’t have authorization rights to '\n",
      "                         'the host folder ( which also caused folder seems '\n",
      "                         'empty, but it didn’t!).\\n'\n",
      "                         '✅Solution:\\n'\n",
      "                         'Just add permission for everyone to the '\n",
      "                         'corresponding folder\\n'\n",
      "                         'sudo chmod -R 777 <path_to_folder>\\n'\n",
      "                         'Example:\\n'\n",
      "                         'sudo chmod -R 777 ny_taxi_postgres_data/'},\n",
      "                {'question': 'Docker - failed to solve with frontend '\n",
      "                             'dockerfile.v0: failed to read dockerfile: error '\n",
      "                             'from sender: open ny_taxi_postgres_data: '\n",
      "                             'permission denied.',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'This happens on Ubuntu/Linux systems when trying to '\n",
      "                         'run the command to build the Docker container '\n",
      "                         'again.\\n'\n",
      "                         '$ docker build -t taxi_ingest:v001 .\\n'\n",
      "                         'A folder is created to host the Docker files. When '\n",
      "                         'the build command is executed again to rebuild the '\n",
      "                         'pipeline or create a new one the error is raised as '\n",
      "                         'there are no permissions on this new folder. Grant '\n",
      "                         'permissions by running this comtionmand;\\n'\n",
      "                         '$ sudo chmod -R 755 ny_taxi_postgres_data\\n'\n",
      "                         'Or use 777 if you still see problems. 755 grants '\n",
      "                         'write access to only the owner.'},\n",
      "                {'question': 'Docker - Docker network name',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Get the network name via: $ docker network ls.'},\n",
      "                {'question': 'Docker - Error response from daemon: Conflict. '\n",
      "                             'The container name \"pg-database\" is already in '\n",
      "                             'use by container “xxx”.  You have to remove (or '\n",
      "                             'rename) that container to be able to reuse that '\n",
      "                             'name.',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Sometimes, when you try to restart a docker image '\n",
      "                         'configured with a network name, the above message '\n",
      "                         'appears. In this case, use the following command '\n",
      "                         'with the appropriate container name:\\n'\n",
      "                         '>>> If the container is running state, use docker '\n",
      "                         'stop <container_name>\\n'\n",
      "                         '>>> then, docker rm pg-database\\n'\n",
      "                         'Or use docker start instead of docker run in order '\n",
      "                         'to restart the docker image without removing it.'},\n",
      "                {'question': 'Docker - ingestion when using docker-compose '\n",
      "                             'could not translate host name',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Typical error: sqlalchemy.exc.OperationalError: '\n",
      "                         '(psycopg2.OperationalError) could not translate host '\n",
      "                         'name \"pgdatabase\" to address: Name or service not '\n",
      "                         'known\\n'\n",
      "                         'When running docker-compose up -d see which network '\n",
      "                         'is created and use this for the ingestions script '\n",
      "                         'instead of pg-network and see the name of the '\n",
      "                         'database to use instead of pgdatabase\\n'\n",
      "                         'E.g.:\\n'\n",
      "                         'pg-network becomes 2docker_default\\n'\n",
      "                         'Pgdatabase becomes 2docker-pgdatabase-1'},\n",
      "                {'question': 'Docker - Cannot install docker on MacOS/Windows '\n",
      "                             '11 VM running on top of Linux (due to Nested '\n",
      "                             'virtualization).',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'terraformRun this command before starting your VM:\\n'\n",
      "                         'On Intel CPU:\\n'\n",
      "                         'modprobe -r kvm_intel\\n'\n",
      "                         'modprobe kvm_intel nested=1\\n'\n",
      "                         'On AMD CPU:\\n'\n",
      "                         'modprobe -r kvm_amd\\n'\n",
      "                         'modprobe kvm_amd nested=1'},\n",
      "                {'question': 'Docker - Connecting from VS Code',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'It’s very easy to manage your docker container, '\n",
      "                         'images, network and compose projects from VS Code.\\n'\n",
      "                         'Just install the official extension and launch it '\n",
      "                         'from the left side icon.\\n'\n",
      "                         'It will work even if your Docker runs on WSL2, as VS '\n",
      "                         'Code can easily connect with your Linux.\\n'\n",
      "                         'Docker - How to stop a container?\\n'\n",
      "                         'Use the following command:\\n'\n",
      "                         '$ docker stop <container_id>'},\n",
      "                {'question': 'Docker - PostgreSQL Database directory appears '\n",
      "                             'to contain a database. Database system is shut '\n",
      "                             'down',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'When you see this in logs, your container with '\n",
      "                         'postgres is not accepting any requests, so if you '\n",
      "                         \"attempt to connect, you'll get this error:\\n\"\n",
      "                         'connection failed: server closed the connection '\n",
      "                         'unexpectedly\\n'\n",
      "                         'This probably means the server terminated abnormally '\n",
      "                         'before or while processing the request.\\n'\n",
      "                         'In this case, you need to delete the directory with '\n",
      "                         'data (the one you map to the container with the -v '\n",
      "                         'flag) and restart the container.'},\n",
      "                {'question': 'Docker not installable on Ubuntu',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'On few versions of Ubuntu, snap command can be used '\n",
      "                         'to install Docker.\\n'\n",
      "                         'sudo snap install docker'},\n",
      "                {'question': 'Docker-Compose - mounting error',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'error: could not change permissions of directory '\n",
      "                         '\"/var/lib/postgresql/data\": Operation not permitted  '\n",
      "                         'volume\\n'\n",
      "                         'if you have used the prev answer (just before this) '\n",
      "                         'and have created a local docker volume, then you '\n",
      "                         'need to tell the compose file about the named '\n",
      "                         'volume:\\n'\n",
      "                         'volumes:\\n'\n",
      "                         'dtc_postgres_volume_local:  # Define the named '\n",
      "                         'volume here\\n'\n",
      "                         '# services mentioned in the compose file auto become '\n",
      "                         'part of the same network!\\n'\n",
      "                         'services:\\n'\n",
      "                         'your remaining code here . . .\\n'\n",
      "                         'now use docker volume inspect '\n",
      "                         'dtc_postgres_volume_local to see the location by '\n",
      "                         'checking the value of Mountpoint\\n'\n",
      "                         'In my case, after i ran docker compose up the '\n",
      "                         'mounting dir created was named '\n",
      "                         '‘docker_sql_dtc_postgres_volume_local’ whereas it '\n",
      "                         'should have used the already existing '\n",
      "                         '‘dtc_postgres_volume_local’\\n'\n",
      "                         'All i did to fix this is that I renamed the existing '\n",
      "                         '‘dtc_postgres_volume_local’ to '\n",
      "                         '‘docker_sql_dtc_postgres_volume_local’ and removed '\n",
      "                         'the newly created one (just be careful when doing '\n",
      "                         'this)\\n'\n",
      "                         'run docker compose up again and check if the table '\n",
      "                         'is there or not!'},\n",
      "                {'question': 'Docker-Compose - Error translating host name to '\n",
      "                             'address',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Couldn’t translate host name to address\\n'\n",
      "                         'Make sure postgres database is running.\\n'\n",
      "                         '\\n'\n",
      "                         '\\u200b\\u200bUse the command to start containers in '\n",
      "                         'detached mode: docker-compose up -d\\n'\n",
      "                         '(data-engineering-zoomcamp) hw % docker compose up '\n",
      "                         '-d\\n'\n",
      "                         '[+] Running 2/2\\n'\n",
      "                         '⠿ Container pg-admin     '\n",
      "                         'Started                                                                                                                                                                      '\n",
      "                         '0.6s\\n'\n",
      "                         '⠿ Container pg-database  Started\\n'\n",
      "                         'To view the containers use: docker ps.\\n'\n",
      "                         '(data-engineering-zoomcamp) hw % docker ps\\n'\n",
      "                         'CONTAINER ID   IMAGE            '\n",
      "                         'COMMAND                  CREATED          '\n",
      "                         'STATUS          PORTS                           '\n",
      "                         'NAMES\\n'\n",
      "                         'faf05090972e   postgres:13      '\n",
      "                         '\"docker-entrypoint.s…\"   39 seconds ago   Up 37 '\n",
      "                         'seconds   0.0.0.0:5432->5432/tcp          '\n",
      "                         'pg-database\\n'\n",
      "                         '6344dcecd58f   dpage/pgadmin4   '\n",
      "                         '\"/entrypoint.sh\"         39 seconds ago   Up 37 '\n",
      "                         'seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\\n'\n",
      "                         'hw\\n'\n",
      "                         'To view logs for a container: docker logs '\n",
      "                         '<containerid>\\n'\n",
      "                         '(data-engineering-zoomcamp) hw % docker logs '\n",
      "                         'faf05090972e\\n'\n",
      "                         'PostgreSQL Database directory appears to contain a '\n",
      "                         'database; Skipping initialization\\n'\n",
      "                         '2022-01-25 05:58:45.948 UTC [1] LOG:  starting '\n",
      "                         'PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on '\n",
      "                         'aarch64-unknown-linux-gnu, compiled by gcc (Debian '\n",
      "                         '10.2.1-6) 10.2.1 20210110, 64-bit\\n'\n",
      "                         '2022-01-25 05:58:45.948 UTC [1] LOG:  listening on '\n",
      "                         'IPv4 address \"0.0.0.0\", port 5432\\n'\n",
      "                         '2022-01-25 05:58:45.948 UTC [1] LOG:  listening on '\n",
      "                         'IPv6 address \"::\", port 5432\\n'\n",
      "                         '2022-01-25 05:58:45.954 UTC [1] LOG:  listening on '\n",
      "                         'Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\\n'\n",
      "                         '2022-01-25 05:58:45.984 UTC [28] LOG:  database '\n",
      "                         'system was interrupted; last known up at 2022-01-24 '\n",
      "                         '17:48:35 UTC\\n'\n",
      "                         '2022-01-25 05:58:48.581 UTC [28] LOG:  database '\n",
      "                         'system was not properly shut down; automatic '\n",
      "                         'recovery in\\n'\n",
      "                         'progress\\n'\n",
      "                         '2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts '\n",
      "                         'at 0/872A5910\\n'\n",
      "                         '2022-01-25 05:59:33.726 UTC [28] LOG:  invalid '\n",
      "                         'record length at 0/98A3C160: wanted 24, got 0\\n'\n",
      "                         '2022-01-25 05:59:33.726 UTC [28\\n'\n",
      "                         '] LOG:  redo done at 0/98A3C128\\n'\n",
      "                         '2022-01-25 05:59:48.051 UTC [1] LOG:  database '\n",
      "                         'system is ready to accept connections\\n'\n",
      "                         'If docker ps doesn’t show pgdatabase running, run: '\n",
      "                         'docker ps -a\\n'\n",
      "                         'This should show all containers, either running or '\n",
      "                         'stopped.\\n'\n",
      "                         'Get the container id for pgdatabase-1, and run'},\n",
      "                {'question': 'Docker-Compose -  Data retention (could not '\n",
      "                             'translate host name \"pg-database\" to address: '\n",
      "                             'Name or service not known)',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'After executing `docker-compose up` - if you lose '\n",
      "                         'database data and are unable to successfully execute '\n",
      "                         'your Ingestion script (to re-populate your database) '\n",
      "                         'but receive the following error:\\n'\n",
      "                         'sqlalchemy.exc.OperationalError: '\n",
      "                         '(psycopg2.OperationalError) could not translate host '\n",
      "                         'name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to '\n",
      "                         'address: Name or service not known\\n'\n",
      "                         'Docker compose is creating its own default network '\n",
      "                         'since it is no longer specified in a docker '\n",
      "                         'execution command or file. Docker Compose will emit '\n",
      "                         'to logs the new network name. See the logs after '\n",
      "                         'executing `docker compose up` to find the network '\n",
      "                         'name and change the network name argument in your '\n",
      "                         'Ingestion script.\\n'\n",
      "                         'If problems persist with pgcli, we can use '\n",
      "                         'HeidiSQL,usql\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'Docker-Compose - Hostname does not resolve',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'It returns --> Error response from daemon: network '\n",
      "                         '66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 '\n",
      "                         'not found\\n'\n",
      "                         'Try:\\n'\n",
      "                         'docker ps -a to see all the stopped & running '\n",
      "                         'containers\\n'\n",
      "                         'd to nuke all the containers\\n'\n",
      "                         'Try: docker-compose up -d again ports\\n'\n",
      "                         'On localhost:8080 server → Unable to connect to '\n",
      "                         \"server: could not translate host name 'pg-database' \"\n",
      "                         'to address: Name does not resolve\\n'\n",
      "                         'Try: new host name, best without “ - ” e.g. '\n",
      "                         'pgdatabase\\n'\n",
      "                         'And on docker-compose.yml, should specify docker '\n",
      "                         'network & specify the same network in both  '\n",
      "                         'containers\\n'\n",
      "                         'services:\\n'\n",
      "                         'pgdatabase:\\n'\n",
      "                         'image: postgres:13\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- POSTGRES_USER=root\\n'\n",
      "                         '- POSTGRES_PASSWORD=root\\n'\n",
      "                         '- POSTGRES_DB=ny_taxi\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- '\n",
      "                         '\"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\\n'\n",
      "                         'ports:\\n'\n",
      "                         '- \"5431:5432\"\\n'\n",
      "                         'networks:\\n'\n",
      "                         '- pg-network\\n'\n",
      "                         'pgadmin:\\n'\n",
      "                         'image: dpage/pgadmin4\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n'\n",
      "                         '- PGADMIN_DEFAULT_PASSWORD=root\\n'\n",
      "                         'ports:\\n'\n",
      "                         '- \"8080:80\"\\n'\n",
      "                         'networks:\\n'\n",
      "                         '- pg-network\\n'\n",
      "                         'networks:\\n'\n",
      "                         'pg-network:\\n'\n",
      "                         'name: pg-network'},\n",
      "                {'question': 'Docker-Compose - Persist PGAdmin docker contents '\n",
      "                             'on GCP',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'So one common issue is when you run docker-compose '\n",
      "                         'on GCP, postgres won’t persist it’s data to '\n",
      "                         'mentioned path for example:\\n'\n",
      "                         'services:\\n'\n",
      "                         '…\\n'\n",
      "                         '…\\n'\n",
      "                         'pgadmin:\\n'\n",
      "                         '…\\n'\n",
      "                         '…\\n'\n",
      "                         'Volumes:\\n'\n",
      "                         '“./pgadmin”:/var/lib/pgadmin:wr”\\n'\n",
      "                         'Might not work so in this use you can use Docker '\n",
      "                         'Volume to make it persist, by simply changing\\n'\n",
      "                         'services:\\n'\n",
      "                         '…\\n'\n",
      "                         '….\\n'\n",
      "                         'pgadmin:\\n'\n",
      "                         '…\\n'\n",
      "                         '…\\n'\n",
      "                         'Volumes:\\n'\n",
      "                         'pgadmin:/var/lib/pgadmin\\n'\n",
      "                         'volumes:\\n'\n",
      "                         'Pgadmin:'},\n",
      "                {'question': 'Docker engine stopped_failed to fetch extensions',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'The docker will keep on crashing continuously\\n'\n",
      "                         'Not working after restart\\n'\n",
      "                         'docker engine stopped\\n'\n",
      "                         'And failed to fetch extensions pop ups will on '\n",
      "                         'screen non-stop\\n'\n",
      "                         'Solution :\\n'\n",
      "                         'Try checking if latest version of docker is '\n",
      "                         'installed / Try updating the docker\\n'\n",
      "                         'If Problem still persist then final solution is to '\n",
      "                         'reinstall docker\\n'\n",
      "                         '(Just have to fetch images again else no issues)'},\n",
      "                {'question': 'Docker-Compose - Persist PGAdmin configuration',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'As per the lessons,\\n'\n",
      "                         'Persisting pgAdmin configuration (i.e. server name) '\n",
      "                         'is done by adding a “volumes” section:\\n'\n",
      "                         'services:\\n'\n",
      "                         'pgdatabase:\\n'\n",
      "                         '[...]\\n'\n",
      "                         'pgadmin:\\n'\n",
      "                         'image: dpage/pgadmin4\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n'\n",
      "                         '- PGADMIN_DEFAULT_PASSWORD=root\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\\n'\n",
      "                         'ports:\\n'\n",
      "                         '- \"8080:80\"\\n'\n",
      "                         'In the example above, ”pgAdmin_data” is a folder on '\n",
      "                         'the host machine, and “/var/lib/pgadmin/sessions” is '\n",
      "                         'the session settings folder in the pgAdmin '\n",
      "                         'container.\\n'\n",
      "                         'Before running docker-compose up on the YAML file, '\n",
      "                         'we also need to give the pgAdmin container access to '\n",
      "                         'write to the “pgAdmin_data” folder. The container '\n",
      "                         'runs with a username called “5050” and user group '\n",
      "                         '“5050”. The bash command to give access over the '\n",
      "                         'mounted volume is:\\n'\n",
      "                         'sudo chown -R 5050:5050 pgAdmin_data'},\n",
      "                {'question': 'Docker-Compose - dial unix /var/run/docker.sock: '\n",
      "                             'connect: permission denied',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'This happens if you did not create the docker group '\n",
      "                         'and added your user. Follow these steps from the '\n",
      "                         'link:\\n'\n",
      "                         'guides/docker-without-sudo.md at main · '\n",
      "                         'sindresorhus/guides · GitHub\\n'\n",
      "                         'And then press ctrl+D to log-out and log-in again. '\n",
      "                         'pgAdmin: Maintain state so that it remembers your '\n",
      "                         'previous connection\\n'\n",
      "                         'If you are tired of having to setup your database '\n",
      "                         'connection each time that you fire up the '\n",
      "                         'containers, all you have to do is create a volume '\n",
      "                         'for pgAdmin:\\n'\n",
      "                         'In your docker-compose.yaml file, enter the '\n",
      "                         'following into your pgAdmin declaration:\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- type: volume\\n'\n",
      "                         'source: pgadmin_data\\n'\n",
      "                         'target: /var/lib/pgadmin\\n'\n",
      "                         'Also add the following to the end of the file:ls\\n'\n",
      "                         'volumes:\\n'\n",
      "                         'Pgadmin_data:'},\n",
      "                {'question': 'Docker-Compose - docker-compose still not '\n",
      "                             'available after changing .bashrc',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'This is happen to me after following 1.4.1 video '\n",
      "                         'where we are installing docker compose in our Google '\n",
      "                         'Cloud VM. In my case, the docker-compose file '\n",
      "                         'downloaded from github named '\n",
      "                         'docker-compose-linux-x86_64 while it is more '\n",
      "                         'convenient to use docker-compose command instead. So '\n",
      "                         'just change the docker-compose-linux-x86_64 into '\n",
      "                         'docker-compose.'},\n",
      "                {'question': 'Docker-Compose - Error getting credentials after '\n",
      "                             'running docker-compose up -d',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Installing pass via ‘sudo apt install pass’ helped '\n",
      "                         'to solve the issue. More about this can be found '\n",
      "                         'here: https://github.com/moby/buildkit/issues/1078'},\n",
      "                {'question': 'Docker-Compose - Errors pertaining to '\n",
      "                             'docker-compose.yml and pgadmin setup',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': \"For everyone who's having problem with Docker \"\n",
      "                         'compose, getting the data in postgres and similar '\n",
      "                         'issues, please take care of the following:\\n'\n",
      "                         'create a new volume on docker (either using the '\n",
      "                         'command line or docker desktop app)\\n'\n",
      "                         'make the following changes to your '\n",
      "                         'docker-compose.yml file (see attachment)\\n'\n",
      "                         'set low_memory=false when importing the csv file (df '\n",
      "                         \"= pd.read_csv('yellow_tripdata_2021-01.csv', \"\n",
      "                         'nrows=1000, low_memory=False))\\n'\n",
      "                         'use the below function (in the upload-data.ipynb) '\n",
      "                         'for better tracking of your ingestion process (see '\n",
      "                         'attachment)\\n'\n",
      "                         'Order of execution:\\n'\n",
      "                         '(1) open terminal in 2_docker_sql folder and run '\n",
      "                         'docker compose up\\n'\n",
      "                         '(2) ensure no other containers are running except '\n",
      "                         'the one you just executed (pgadmin and pgdatabase)\\n'\n",
      "                         '(3) open jupyter notebook and begin the data '\n",
      "                         'ingestion\\n'\n",
      "                         '(4) open pgadmin and set up a server (make sure you '\n",
      "                         'use the same configurations as your '\n",
      "                         'docker-compose.yml file like the same name '\n",
      "                         '(pgdatabase), port, databasename (ny_taxi) etc.'},\n",
      "                {'question': 'Docker Compose up -d error getting credentials - '\n",
      "                             'err: exec: \"docker-credential-desktop\": '\n",
      "                             'executable file not found in %PATH%, out: ``',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Locate config.json file for docker (check your home '\n",
      "                         'directory; Users/username/.docker).\\n'\n",
      "                         'Modify credsStore to credStore\\n'\n",
      "                         'Save and re-run'},\n",
      "                {'question': 'Docker-Compose - Which docker-compose binary to '\n",
      "                             'use for WSL?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'To figure out which docker-compose you need to '\n",
      "                         'download from '\n",
      "                         'https://github.com/docker/compose/releases you can '\n",
      "                         'check your system with these commands:\\n'\n",
      "                         'uname -s  -> return Linux most likely\\n'\n",
      "                         'uname -m -> return \"flavor\"\\n'\n",
      "                         'Or try this command -\\n'\n",
      "                         'sudo curl -L '\n",
      "                         '\"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname '\n",
      "                         '-s)-$(uname -m)\" -o /usr/local/bin/docker-compose'},\n",
      "                {'question': 'Docker-Compose - Error undefined volume in '\n",
      "                             'Windows/WSL',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'If you wrote the docker-compose.yaml file exactly '\n",
      "                         'like the video, you might run into an error like '\n",
      "                         'this:dev\\n'\n",
      "                         'service \"pgdatabase\" refers to undefined volume '\n",
      "                         'dtc_postgres_volume_local: invalid compose project\\n'\n",
      "                         'In order to make it work, you need to include the '\n",
      "                         'volume in your docker-compose file. Just add the '\n",
      "                         'following:\\n'\n",
      "                         'volumes:\\n'\n",
      "                         'dtc_postgres_volume_local:\\n'\n",
      "                         '(Make sure volumes are at the same level as '\n",
      "                         'services.)'},\n",
      "                {'question': 'WSL Docker directory permissions error',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Error:  initdb: error: could not change permissions '\n",
      "                         'of directory\\n'\n",
      "                         'Issue: WSL and Windows do not manage permissions in '\n",
      "                         'the same way causing conflict if using the Windows '\n",
      "                         'file system rather than the WSL file system.\\n'\n",
      "                         'Solution: Use Docker volumes.\\n'\n",
      "                         'Why: Volume is used for storage of persistent data '\n",
      "                         'and not for use of transferring files. A local '\n",
      "                         'volume is unnecessary.\\n'\n",
      "                         'Benefit: This resolves permission issues and allows '\n",
      "                         'for better management of volumes.\\n'\n",
      "                         'NOTE: the ‘user:’ is not necessary if using docker '\n",
      "                         'volumes, but is if using local drive.\\n'\n",
      "                         '</>  docker-compose.yaml\\n'\n",
      "                         'services:\\n'\n",
      "                         'postgres:\\n'\n",
      "                         'image: postgres:15-alpine\\n'\n",
      "                         'container_name: postgres\\n'\n",
      "                         'user: \"0:0\"\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- POSTGRES_USER=postgres\\n'\n",
      "                         '- POSTGRES_PASSWORD=postgres\\n'\n",
      "                         '- POSTGRES_DB=ny_taxi\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- \"pg-data:/var/lib/postgresql/data\"\\n'\n",
      "                         'ports:\\n'\n",
      "                         '- \"5432:5432\"\\n'\n",
      "                         'networks:\\n'\n",
      "                         '- pg-network\\n'\n",
      "                         'pgadmin:\\n'\n",
      "                         'image: dpage/pgadmin4\\n'\n",
      "                         'container_name: pgadmin\\n'\n",
      "                         'user: \"${UID}:${GID}\"\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n'\n",
      "                         '- PGADMIN_DEFAULT_PASSWORD=pgadmin\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- \"pg-admin:/var/lib/pgadmin\"\\n'\n",
      "                         'ports:\\n'\n",
      "                         '- \"8080:80\"\\n'\n",
      "                         'networks:\\n'\n",
      "                         '- pg-network\\n'\n",
      "                         'networks:\\n'\n",
      "                         'pg-network:\\n'\n",
      "                         'name: pg-network\\n'\n",
      "                         'volumes:\\n'\n",
      "                         'pg-data:\\n'\n",
      "                         'name: ingest_pgdata\\n'\n",
      "                         'pg-admin:\\n'\n",
      "                         'name: ingest_pgadmin'},\n",
      "                {'question': 'Docker - If pgadmin is not working for Querying '\n",
      "                             'in Postgres Use PSQL',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Cause : If Running on git bash or vm in windows '\n",
      "                         'pgadmin doesnt work easily LIbraries like psycopg2 '\n",
      "                         'and libpq ar required still the error persists.\\n'\n",
      "                         'Solution- I use psql instead of pgadmin totally '\n",
      "                         'same\\n'\n",
      "                         'Pip install psycopg2\\n'\n",
      "                         'dock'},\n",
      "                {'question': 'WSL - Insufficient system resources exist to '\n",
      "                             'complete the requested service.',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Cause:\\n'\n",
      "                         'It happens because the apps are not updated. To be '\n",
      "                         'specific, search for any pending updates for Windows '\n",
      "                         'Terminal, WSL and Windows Security updates.\\n'\n",
      "                         'Solution\\n'\n",
      "                         'for updating Windows terminal which worked for me:\\n'\n",
      "                         'Go to Microsoft Store.\\n'\n",
      "                         'Go to the library of apps installed in your system.\\n'\n",
      "                         'Search for Windows terminal.\\n'\n",
      "                         'Update the app and restart your system to  see the '\n",
      "                         'changes.\\n'\n",
      "                         'For updating the Windows security updates:\\n'\n",
      "                         'Go to Windows updates and check if there are any '\n",
      "                         'pending updates from Windows, especially security '\n",
      "                         'updates.\\n'\n",
      "                         'Do restart your system once the updates are '\n",
      "                         'downloaded and installed successfully.'},\n",
      "                {'question': 'WSL - WSL integration with distro Ubuntu '\n",
      "                             'unexpectedly stopped with exit code 1.',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Up restardoting the same issue appears. Happens out '\n",
      "                         'of the blue on windows.\\n'\n",
      "                         'Solution 1: Fixing DNS Issue (credit: reddit) this '\n",
      "                         'worked for me personally\\n'\n",
      "                         'reg add '\n",
      "                         '\"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" '\n",
      "                         '/v \"Start\" /t REG_DWORD /d \"4\" /f\\n'\n",
      "                         'Restart your computer and then enable it with the '\n",
      "                         'following\\n'\n",
      "                         'reg add '\n",
      "                         '\"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" '\n",
      "                         '/v \"Start\" /t REG_DWORD /d \"2\" /f\\n'\n",
      "                         'Restart your OS again. It should work.\\n'\n",
      "                         'Solution 2: right click on running Docker icon (next '\n",
      "                         'to clock) and chose \"Switch to Linux containers\"\\n'\n",
      "                         'bash: conda: command not found\\n'\n",
      "                         'Database is uninitialized and superuser password is '\n",
      "                         'not specified.\\n'\n",
      "                         'Database is uninitialized and superuser password is '\n",
      "                         'not specified.'},\n",
      "                {'question': 'WSL - Permissions too open at Windows',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Issue when trying to run the GPC VM through SSH '\n",
      "                         'through WSL2,  probably because WSL2 isn’t looking '\n",
      "                         'for .ssh keys in the correct folder. My case I was '\n",
      "                         'trying to run this command in the terminal and '\n",
      "                         'getting an error\\n'\n",
      "                         'PC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my '\n",
      "                         'external IP]\\n'\n",
      "                         'You can try to use sudo before the command\\n'\n",
      "                         'Sudo .ssh$ ssh -i gpc [username]@[my external IP]\\n'\n",
      "                         'You can also try to cd to your folder and change the '\n",
      "                         'permissions for the private key SSH file.\\n'\n",
      "                         'chmod 600 gpc\\n'\n",
      "                         'If that doesn’t work, create a .ssh folder in the '\n",
      "                         'home diretory of WSL2 and copy the content of '\n",
      "                         'windows .ssh folder to that new folder.\\n'\n",
      "                         'cd ~\\n'\n",
      "                         'mkdir .ssh\\n'\n",
      "                         'cp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\n'\n",
      "                         'You might need to adjust the permissions of the '\n",
      "                         'files and folders in the .ssh directory.'},\n",
      "                {'question': 'WSL - Could not resolve host name',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Such as the issue above, WSL2 may not be referencing '\n",
      "                         'the correct .ssh/config path from Windows. You can '\n",
      "                         'create a config file at the home directory of WSL2.\\n'\n",
      "                         'cd ~\\n'\n",
      "                         'mkdir .ssh\\n'\n",
      "                         'Create a config file in this new .ssh/ folder '\n",
      "                         'referencing this folder:\\n'\n",
      "                         'HostName [GPC VM external IP]\\n'\n",
      "                         'User [username]\\n'\n",
      "                         'IdentityFile ~/.ssh/[private key]'},\n",
      "                {'question': 'PGCLI - connection failed: :1), port 5432 '\n",
      "                             'failed: could not receive data from server: '\n",
      "                             'Connection refused could not send SSL '\n",
      "                             'negotiation packet: Connection refused',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Change TO Socket\\n'\n",
      "                         'pgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\\n'\n",
      "                         'pgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi'},\n",
      "                {'question': 'PGCLI --help error',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'probably some installation error, check out sy'},\n",
      "                {'question': 'PGCLI - INKhould we run pgcli inside another '\n",
      "                             'docker container?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'In this section of the course, the 5432 port of '\n",
      "                         'pgsql is mapped to your computer’s 5432 port. Which '\n",
      "                         'means you can access the postgres database via pgcli '\n",
      "                         'directly from your computer.\\n'\n",
      "                         'So No, you don’t need to run it inside another '\n",
      "                         'container. Your local system will do.'},\n",
      "                {'question': 'PGCLI - FATAL: password authentication failed '\n",
      "                             'for user \"root\" (You already have Postgres)',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'FATAL:  password authentication failed for user '\n",
      "                         '\"root\"\\n'\n",
      "                         'observations: Below in bold do not forget the folder '\n",
      "                         'that was created ny_taxi_postgres_data\\n'\n",
      "                         'This happens if you have a local Postgres '\n",
      "                         'installation in your computer. To mitigate this, use '\n",
      "                         'a different port, like 5431, when creating the '\n",
      "                         'docker container, as in: -p 5431: 5432\\n'\n",
      "                         'Then, we need to use this port when connecting to '\n",
      "                         'pgcli, as shown below:\\n'\n",
      "                         'pgcli -h localhost -p 5431 -u root -d ny_taxi\\n'\n",
      "                         'This will connect you to your postgres docker '\n",
      "                         'container, which is mapped to your host’s 5431 port '\n",
      "                         '(though you might choose any port of your liking as '\n",
      "                         'long as it is not occupied).\\n'\n",
      "                         'For a more visual and detailed explanation, feel '\n",
      "                         'free to check the video 1.4.2 - Port Mapping and '\n",
      "                         'Networks in Docker\\n'\n",
      "                         'If you want to debug: the following can help (on a '\n",
      "                         'MacOS)\\n'\n",
      "                         'To find out if something is blocking your port (on a '\n",
      "                         'MacOS):\\n'\n",
      "                         'You can use the lsof command to find out which '\n",
      "                         'application is using a specific port on your local '\n",
      "                         'machine. `lsof -i :5432`wi\\n'\n",
      "                         'Or list the running postgres services on your local '\n",
      "                         'machine with launchctl\\n'\n",
      "                         'To unload the running service on your local machine '\n",
      "                         '(on a MacOS):\\n'\n",
      "                         'unload the launch agent for the PostgreSQL service, '\n",
      "                         'which will stop the service and free up the port  \\n'\n",
      "                         '`launchctl unload -w '\n",
      "                         '~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\n'\n",
      "                         'this one to start it again\\n'\n",
      "                         '`launchctl load -w '\n",
      "                         '~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\n'\n",
      "                         'Changing port from 5432:5432 to 5431:5432 helped me '\n",
      "                         'to avoid this error.'},\n",
      "                {'question': 'PGCLI - PermissionError: [Errno 13] Permission '\n",
      "                             \"denied: '/some/path/.config/pgcli'\",\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'I get this error\\n'\n",
      "                         'pgcli -h localhost -p 5432 -U root -d ny_taxi\\n'\n",
      "                         'Traceback (most recent call last):\\n'\n",
      "                         'File \"/opt/anaconda3/bin/pgcli\", line 8, in '\n",
      "                         '<module>\\n'\n",
      "                         'sys.exit(cli())\\n'\n",
      "                         'File '\n",
      "                         '\"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", '\n",
      "                         'line 1128, in __call__\\n'\n",
      "                         'return self.main(*args, **kwargs)\\n'\n",
      "                         'File '\n",
      "                         '\"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", '\n",
      "                         'line\\n'\n",
      "                         '1053, in main\\n'\n",
      "                         'rv = self.invoke(ctx)\\n'\n",
      "                         'File '\n",
      "                         '\"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", '\n",
      "                         'line 1395, in invoke\\n'\n",
      "                         'return ctx.invoke(self.callback, **ctx.params)\\n'\n",
      "                         'File '\n",
      "                         '\"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", '\n",
      "                         'line 754, in invoke\\n'\n",
      "                         'return __callback(*args, **kwargs)\\n'\n",
      "                         'File '\n",
      "                         '\"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", '\n",
      "                         'line 880, in cli\\n'\n",
      "                         'os.makedirs(config_dir)\\n'\n",
      "                         'File \"/opt/anaconda3/lib/python3.9/os.py\", line 225, '\n",
      "                         'in makedirspython\\n'\n",
      "                         'mkdir(name, mode)PermissionError: [Errno 13] '\n",
      "                         \"Permission denied: '/Users/vray/.config/pgcli'\\n\"\n",
      "                         'Make sure you install pgcli without sudo.\\n'\n",
      "                         'The recommended approach is to use conda/anaconda to '\n",
      "                         'make sure your system python is not affected.\\n'\n",
      "                         'If conda install gets stuck at \"Solving environment\" '\n",
      "                         'try these alternatives: '\n",
      "                         'https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda'},\n",
      "                {'question': 'PGCLI - no pq wrapper available.',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'ImportError: no pq wrapper available.\\n'\n",
      "                         'Attempts made:\\n'\n",
      "                         \"- couldn't import \\\\dt\\n\"\n",
      "                         \"opg 'c' implementation: No module named 'psycopg_c'\\n\"\n",
      "                         \"- couldn't import psycopg 'binary' implementation: \"\n",
      "                         \"No module named 'psycopg_binary'\\n\"\n",
      "                         \"- couldn't import psycopg 'python' implementation: \"\n",
      "                         'libpq library not found\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'First, make sure your Python is set to 3.9, at '\n",
      "                         'least.\\n'\n",
      "                         'And the reason for that is we have had cases of '\n",
      "                         \"'psycopg2-binary' failing to install because of an \"\n",
      "                         'old version of Python (3.7.3). \\n'\n",
      "                         '\\n'\n",
      "                         '0. You can check your current python version with: \\n'\n",
      "                         '$ python -V(the V must be capital)\\n'\n",
      "                         \"1. Based on the previous output, if you've got a \"\n",
      "                         '3.9, skip to Step #2\\n'\n",
      "                         '   Otherwispye better off with a new environment '\n",
      "                         'with 3.9\\n'\n",
      "                         '$ conda create –name de-zoomcamp python=3.9\\n'\n",
      "                         '$ conda activate de-zoomcamp\\n'\n",
      "                         '2. Next, you should be able to install the lib for '\n",
      "                         'postgres like this:\\n'\n",
      "                         '```\\n'\n",
      "                         '$ e\\n'\n",
      "                         '$ pip install psycopg2_binary\\n'\n",
      "                         '```\\n'\n",
      "                         \"3. Finally, make sure you're also installing pgcli, \"\n",
      "                         'but use conda for that:\\n'\n",
      "                         '```\\n'\n",
      "                         '$ pgcli -h localhost -U root -d ny_taxisudo\\n'\n",
      "                         '```\\n'\n",
      "                         'There, you should be good to go now!\\n'\n",
      "                         'Another solution:\\n'\n",
      "                         'Run this\\n'\n",
      "                         'pip install \"psycopg[binary,pool]\"'},\n",
      "                {'question': 'PGCLI -  stuck on password prompt',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'If your Bash prompt is stuck on the password command '\n",
      "                         'for postgres\\n'\n",
      "                         'Use winpty:\\n'\n",
      "                         'winpty pgcli -h localhost -p 5432 -u root -d '\n",
      "                         'ny_taxi\\n'\n",
      "                         'Alternatively, try using Windows terminal or '\n",
      "                         'terminal in VS code.\\n'\n",
      "                         'EditPGCLI -connection failed: FATAL:  password '\n",
      "                         'authentication failed for user \"root\"\\n'\n",
      "                         'The error above was faced continually despite '\n",
      "                         'inputting the correct password\\n'\n",
      "                         'Solution\\n'\n",
      "                         'Option 1: Stop the PostgreSQL service on Windows\\n'\n",
      "                         'Option 2 (using WSL): Completely uninstall Protgres '\n",
      "                         '12 from Windows and install postgresql-client on WSL '\n",
      "                         '(sudo apt install postgresql-client-common '\n",
      "                         'postgresql-client libpq-dev)\\n'\n",
      "                         'Option 3: Change the port of the docker container\\n'\n",
      "                         'NEW SOLUTION: 27/01/2024\\n'\n",
      "                         'PGCLI -connection failed: FATAL:  password '\n",
      "                         'authentication failed for user \"root\"\\n'\n",
      "                         'If you’ve got the error above, it’s probably because '\n",
      "                         'you were just like me, closed the connection to the '\n",
      "                         'Postgres:13 image in the previous step of the '\n",
      "                         'tutorial, which is\\n'\n",
      "                         '\\n'\n",
      "                         'docker run -it \\\\\\n'\n",
      "                         '-e POSTGRES_USER=root \\\\\\n'\n",
      "                         '-e POSTGRES_PASSWORD=root \\\\\\n'\n",
      "                         '-e POSTGRES_DB=ny_taxi \\\\\\n'\n",
      "                         '-v '\n",
      "                         'd:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data '\n",
      "                         '\\\\\\n'\n",
      "                         '-p 5432:5432 \\\\\\n'\n",
      "                         'postgres:13\\n'\n",
      "                         'So keep the database connected and you will be able '\n",
      "                         'to implement all the next steps of the tutorial.'},\n",
      "                {'question': 'PGCLI - pgcli: command not found',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Problem: If you have already installed pgcli but '\n",
      "                         \"bash doesn't recognize pgcli\\n\"\n",
      "                         'On Git bash: bash: pgcli: command not found\\n'\n",
      "                         \"On Windows Terminal: pgcli: The term 'pgcli' is not \"\n",
      "                         'recognized…\\n'\n",
      "                         'Solution: Try adding a Python path '\n",
      "                         'C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts '\n",
      "                         'to Windows PATH\\n'\n",
      "                         'For details:\\n'\n",
      "                         'Get the location: pip list -v\\n'\n",
      "                         'Copy '\n",
      "                         'C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\n'\n",
      "                         '3. Replace site-packages with Scripts: '\n",
      "                         'C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts\\n'\n",
      "                         'It can also be that you have Python installed '\n",
      "                         'elsewhere.\\n'\n",
      "                         'For me it was under '\n",
      "                         'c:\\\\python310\\\\lib\\\\site-packages\\n'\n",
      "                         'So I had to add c:\\\\python310\\\\lib\\\\Scripts to PATH, '\n",
      "                         'as shown below.\\n'\n",
      "                         'Put the above path in \"Path\" (or \"PATH\") in System '\n",
      "                         'Variables\\n'\n",
      "                         'Reference: https://stackoverflow.com/a/68233660'},\n",
      "                {'question': 'PGCLI - running in a Docker container',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'In case running pgcli  locally causes issues or you '\n",
      "                         'do not want to install it locally you can use it '\n",
      "                         'running in a Docker container instead.\\n'\n",
      "                         'Below the usage with values used in the videos of '\n",
      "                         'the course for:\\n'\n",
      "                         'network name (docker network)\\n'\n",
      "                         'postgres related variables for pgcli\\n'\n",
      "                         'Hostname\\n'\n",
      "                         'Username\\n'\n",
      "                         'Port\\n'\n",
      "                         'Database name\\n'\n",
      "                         '$ docker run -it --rm --network pg-network '\n",
      "                         'ai2ys/dockerized-pgcli:4.0.1\\n'\n",
      "                         '175dd47cda07:/# pgcli -h pg-database -U root -p 5432 '\n",
      "                         '-d ny_taxi\\n'\n",
      "                         'Password for root:\\n'\n",
      "                         'Server: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\n'\n",
      "                         'Version: 4.0.1\\n'\n",
      "                         'Home: http://pgcli.com\\n'\n",
      "                         'root@pg-database:ny_taxi> \\\\dt\\n'\n",
      "                         '+--------+------------------+-------+-------+\\n'\n",
      "                         '| Schema | Name             | Type  | Owner |\\n'\n",
      "                         '|--------+------------------+-------+-------|\\n'\n",
      "                         '| public | yellow_taxi_data | table | root  |\\n'\n",
      "                         '+--------+------------------+-------+-------+\\n'\n",
      "                         'SELECT 1\\n'\n",
      "                         'Time: 0.009s\\n'\n",
      "                         'root@pg-database:ny_taxi>'},\n",
      "                {'question': 'PGCLI - case sensitive use “Quotations” around '\n",
      "                             'columns with capital letters',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'PULocationID will not be recognized but '\n",
      "                         '“PULocationID” will be. This is because unquoted '\n",
      "                         '\"Localidentifiers are case insensitive. See docs.'},\n",
      "                {'question': 'PGCLI - error column c.relhasoids does not exist',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'When using the command `\\\\d <database name>` you get '\n",
      "                         'the error column `c.relhasoids does not exist`.\\n'\n",
      "                         'Resolution:\\n'\n",
      "                         'Uninstall pgcli\\n'\n",
      "                         'Reinstall pgclidatabase \"ny_taxi\" does not exist\\n'\n",
      "                         'Restart pc'},\n",
      "                {'question': 'Postgres - OperationalError: '\n",
      "                             '(psycopg2.OperationalError) connection to server '\n",
      "                             'at \"localhost\" (::1), port 5432 failed: FATAL:  '\n",
      "                             'password authentication failed for user \"root\"',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'This happens while uploading data via the connection '\n",
      "                         'in jupyter notebook\\n'\n",
      "                         'engine = '\n",
      "                         \"create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\n\"\n",
      "                         'The port 5432 was taken by another postgres. We are '\n",
      "                         'not connecting to the port in docker, but to the '\n",
      "                         'port on our machine. Substitute 5431 or whatever '\n",
      "                         'port you mapped to for port 5432.\\n'\n",
      "                         'Also if this error is still persistent , kindly '\n",
      "                         'check if you have a service in windows running '\n",
      "                         'postgres , Stopping that service will resolve the '\n",
      "                         'issue'},\n",
      "                {'question': 'Postgres - OperationalError: '\n",
      "                             '(psycopg2.OperationalError) connection to server '\n",
      "                             'at \"localhost\" (::1), port 5432 failed: FATAL:  '\n",
      "                             'role \"root\" does not exist',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Can happen when connecting via pgcli\\n'\n",
      "                         'pgcli -h localhost -p 5432 -U root -d ny_taxi\\n'\n",
      "                         'Or while uploading data via the connection in '\n",
      "                         'jupyter notebook\\n'\n",
      "                         'engine = '\n",
      "                         \"create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\n\"\n",
      "                         'This can happen when Postgres is already installed '\n",
      "                         'on your computer. Changing the port can resolve that '\n",
      "                         '(e.g. from 5432 to 5431).\\n'\n",
      "                         'To check whether there even is a root user with the '\n",
      "                         'ability to login:\\n'\n",
      "                         'Try: docker exec -it <your_container_name> '\n",
      "                         '/bin/bash\\n'\n",
      "                         'And then run\\n'\n",
      "                         '???\\n'\n",
      "                         'Also, you could change port from 5432:5432 to '\n",
      "                         '5431:5432\\n'\n",
      "                         'Other solution that worked:\\n'\n",
      "                         'Changing `POSTGRES_USER=juroot` to '\n",
      "                         '`PGUSER=postgres`\\n'\n",
      "                         'Based on this: postgres with docker compose gives '\n",
      "                         'FATAL: role \"root\" does not exist error - Stack '\n",
      "                         'Overflow\\n'\n",
      "                         'Also `docker compose down`, removing folder that had '\n",
      "                         'postgres volume, running `docker compose up` again.'},\n",
      "                {'question': 'Postgres - OperationalError: '\n",
      "                             '(psycopg2.OperationalError) connection to server '\n",
      "                             'at \"localhost\" (::1), port 5432 failed: FATAL:  '\n",
      "                             'dodatabase \"ny_taxi\" does not exist',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': '~\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py '\n",
      "                         'in connect(dsn, connection_factory, cursor_factory, '\n",
      "                         '**kwargs)\\n'\n",
      "                         '120\\n'\n",
      "                         '121     dsn = _ext.make_dsn(dsn, **kwargs)\\n'\n",
      "                         '--> 122     conn = _connect(dsn, '\n",
      "                         'connection_factory=connection_factory, **kwasync)\\n'\n",
      "                         '123     if cursor_factory is not None:\\n'\n",
      "                         '124         conn.cursor_factory = cursor_factory\\n'\n",
      "                         'OperationalError: (psycopg2.OperationalError) '\n",
      "                         'connection to server at \"localhost\" (::1), port 5432 '\n",
      "                         'failed: FATAL:  database \"ny_taxi\" does not exist\\n'\n",
      "                         'Make sure postgres is running. You can check that by '\n",
      "                         'running `docker ps`\\n'\n",
      "                         '✅Solution: If you have postgres software installed '\n",
      "                         'on your computer before now, build your instance on '\n",
      "                         'a different port like 8080 instead of 5432'},\n",
      "                {'question': 'Postgres - ModuleNotFoundError: No module named '\n",
      "                             \"'psycopg2'\",\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Issue:\\n'\n",
      "                         'e…\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'pip install psycopg2-binary\\n'\n",
      "                         'If you already have it, you might need to update '\n",
      "                         'it:\\n'\n",
      "                         'pip install psycopg2-binary --upgrade\\n'\n",
      "                         'Other methods, if the above fails:\\n'\n",
      "                         'if you are getting the “ ModuleNotFoundError: No '\n",
      "                         \"module named 'psycopg2' “ error even after the above \"\n",
      "                         'installation, then try updating conda using the '\n",
      "                         'command conda update -n base -c defaults conda. Or '\n",
      "                         'if you are using pip, then try updating it before '\n",
      "                         'installing the psycopg packages i.e\\n'\n",
      "                         'First uninstall the psycopg package\\n'\n",
      "                         'Then update conda or pip\\n'\n",
      "                         'Then install psycopg again using pip.\\n'\n",
      "                         'if you are still facing error with r pcycopg2 and '\n",
      "                         'showing pg_config not found then you will have to '\n",
      "                         'install postgresql. in MAC it is brew install '\n",
      "                         'postgresql'},\n",
      "                {'question': 'Postgres - \"Column does not exist\" but it '\n",
      "                             'actually does (Pyscopg2 error in MacBook Pro M2)',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'In the join queries, if we mention the column name '\n",
      "                         'directly or enclosed in single quotes it’ll throw an '\n",
      "                         'error says “column does not exist”.\\n'\n",
      "                         '✅Solution: But if we enclose the column names in '\n",
      "                         'double quotes then it will work'},\n",
      "                {'question': 'pgAdmin - Create server dialog does not appear',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'pgAdmin has a new version. Create server dialog may '\n",
      "                         'not appear. Try using register-> server instead.'},\n",
      "                {'question': 'pgAdmin - Blank/white screen after login '\n",
      "                             '(browser)',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Using GitHub Codespaces in the browser resulted in a '\n",
      "                         'blank screen after the login to pgAdmin (running in '\n",
      "                         'a Docker container). The terminal of the pgAdmin '\n",
      "                         'container was showing the following error message:\\n'\n",
      "                         'CSRFError: 400 Bad Request: The referrer does not '\n",
      "                         'match the host.\\n'\n",
      "                         'Solution #1:\\n'\n",
      "                         'As recommended in the following issue  '\n",
      "                         'https://github.com/pgadmin-org/pgadmin4/issues/5432 '\n",
      "                         'setting the following environment variable solved '\n",
      "                         'it.\\n'\n",
      "                         'PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\\n'\n",
      "                         'Modified “docker run” command\\n'\n",
      "                         'docker run --rm -it \\\\\\n'\n",
      "                         '-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n'\n",
      "                         '-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\\\\n'\n",
      "                         '-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n'\n",
      "                         '-p \"8080:80\" \\\\\\n'\n",
      "                         '--name pgadmin \\\\\\n'\n",
      "                         '--network=pg-network \\\\\\n'\n",
      "                         'dpage/pgadmin4:8.2\\n'\n",
      "                         'Solution #2:\\n'\n",
      "                         'Using the local installed VSCode to display GitHub '\n",
      "                         'Codespaces.\\n'\n",
      "                         'When using GitHub Codespaces in the locally '\n",
      "                         'installed VSCode (opening a Codespace or '\n",
      "                         'creating/starting one) this issue did not occur.'},\n",
      "                {'question': 'pgAdmin - Can not access/open the PgAdmin '\n",
      "                             'address via browser',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'I am using a Mac Pro device and connect to the GCP '\n",
      "                         'Compute Engine via Remote SSH - VSCode. But when I '\n",
      "                         'trying to run the PgAdmin container via docker run '\n",
      "                         'or docker compose command, I am failed to access the '\n",
      "                         'pgAdmin address via my browser. I have switched to '\n",
      "                         'another browser, but still can not access the '\n",
      "                         'pgAdmin address. So I modified a little bit the '\n",
      "                         'configuration from the previous DE Zoomcamp '\n",
      "                         'repository like below and can access the pgAdmin '\n",
      "                         'address:\\n'\n",
      "                         'Solution #1:\\n'\n",
      "                         'Modified “docker run” command\\n'\n",
      "                         'docker run --rm -it \\\\\\n'\n",
      "                         '-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n'\n",
      "                         '-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\\\\n'\n",
      "                         '-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n'\n",
      "                         '-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\\\\n'\n",
      "                         '-e PGADMIN_LISTEN_PORT=5050 \\\\\\n'\n",
      "                         '-p 5050:5050 \\\\\\n'\n",
      "                         '--network=de-zoomcamp-network \\\\\\n'\n",
      "                         '--name pgadmin-container \\\\\\n'\n",
      "                         '--link postgres-container \\\\\\n'\n",
      "                         '-t dpage/pgadmin4\\n'\n",
      "                         'Solution #2:\\n'\n",
      "                         'Modified docker-compose.yaml configuration (via '\n",
      "                         '“docker compose up” command)\\n'\n",
      "                         'pgadmin:\\n'\n",
      "                         'image: dpage/pgadmin4\\n'\n",
      "                         'container_name: pgadmin-conntainer\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n'\n",
      "                         '- PGADMIN_DEFAULT_PASSWORD=pgadmin\\n'\n",
      "                         '- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\\n'\n",
      "                         '- PGADMIN_LISTEN_ADDRESS=0.0.0.0\\n'\n",
      "                         '- PGADMIN_LISTEN_PORT=5050\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- \"./pgadmin_data:/var/lib/pgadmin/data\"\\n'\n",
      "                         'ports:\\n'\n",
      "                         '- \"5050:5050\"\\n'\n",
      "                         'networks:\\n'\n",
      "                         '- de-zoomcamp-network\\n'\n",
      "                         'depends_on:\\n'\n",
      "                         '- postgres-conntainer\\n'\n",
      "                         'Python - ModuleNotFoundError: No module named '\n",
      "                         \"'pysqlite2'\\n\"\n",
      "                         'ImportError: DLL load failed while importing '\n",
      "                         '_sqlite3: The specified module could not be found. '\n",
      "                         \"ModuleNotFoundError: No module named 'pysqlite2'\\n\"\n",
      "                         'The issue seems to arise from the missing of '\n",
      "                         'sqlite3.dll in path \".\\\\Anaconda\\\\Dlls\\\\\".\\n'\n",
      "                         '✅I solved it by simply copying that .dll file from '\n",
      "                         '\\\\Anaconda3\\\\Library\\\\bin and put it under the path '\n",
      "                         'mentioned above. (if you are using anaconda)'},\n",
      "                {'question': 'Python - Ingestion with Jupyter notebook - '\n",
      "                             'missing 100000 records',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'If you follow the video 1.2.2 - Ingesting NY Taxi '\n",
      "                         'Data to Postgres and you execute all the same\\n'\n",
      "                         'steps as Alexey does, you will ingest all the data '\n",
      "                         '(~1.3 million rows) into the table yellow_taxi_data '\n",
      "                         'as expected.\\n'\n",
      "                         'However, if you try to run the whole script in the '\n",
      "                         'Jupyter notebook for a second time from top to '\n",
      "                         'bottom, you will be missing the first chunk of '\n",
      "                         '100000 records. This is because there is a call to '\n",
      "                         'the iterator before the while loop that puts the '\n",
      "                         'data in the table. The while loop therefore starts '\n",
      "                         'by ingesting the second chunk, not the first.\\n'\n",
      "                         '✅Solution: remove the cell “df=next(df_iter)” that '\n",
      "                         'appears higher up in the notebook than the while '\n",
      "                         'loop. The first time w(df_iter) is called should be '\n",
      "                         'within the while loop.\\n'\n",
      "                         '📔Note: As this notebook is just used as a way to '\n",
      "                         'test the code, it was not intended to be run top to '\n",
      "                         'bottom, and the logic is tidied up in a later step '\n",
      "                         'when it is instead inserted into a .py file for the '\n",
      "                         'pipeline'},\n",
      "                {'question': 'Python - Iteration csv without error',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': '{t_end - t_start} seconds\")\\n'\n",
      "                         'import pandas as pd\\n'\n",
      "                         \"df = pd.read_csv('path/to/file.csv.gz', \"\n",
      "                         '/app/ingest_data.py:1: DeprecationWarning:)\\n'\n",
      "                         'If you prefer to keep the uncompressed csv (easier '\n",
      "                         'preview in vscode and similar), gzip files can be '\n",
      "                         'unzipped using gunzip (but not unzip). On a Ubuntu '\n",
      "                         'local or virtual machine, you may need to apt-get '\n",
      "                         'install gunzip first.'},\n",
      "                {'question': 'iPython - Pandas parsing dates with ‘read_csv’',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Pandas can interpret “string” column values as '\n",
      "                         '“datetime” directly when reading the CSV file using '\n",
      "                         '“pd.read_csv” using the parameter “parse_dates”, '\n",
      "                         'which for example can contain a list of column names '\n",
      "                         'or column indices. Then the conversion afterwards is '\n",
      "                         'not required anymore.\\n'\n",
      "                         'pandas.read_csv — pandas 2.1.4 documentation '\n",
      "                         '(pydata.org)\\n'\n",
      "                         'Example from week 1\\n'\n",
      "                         'import pandas as pd\\n'\n",
      "                         'df = pd.read_csv(\\n'\n",
      "                         \"'yellow_tripdata_2021-01.csv',\\n\"\n",
      "                         'nrows=100,\\n'\n",
      "                         \"parse_dates=['tpep_pickup_datetime', \"\n",
      "                         \"'tpep_dropoff_datetime'])\\n\"\n",
      "                         'df.info()\\n'\n",
      "                         'which will output\\n'\n",
      "                         \"<class 'pandas.core.frame.DataFrame'>\\n\"\n",
      "                         'RangeIndex: 100 entries, 0 to 99\\n'\n",
      "                         'Data columns (total 18 columns):\\n'\n",
      "                         '#   Column                 Non-Null Count  Dtype\\n'\n",
      "                         '---  ------                 --------------  -----\\n'\n",
      "                         '0   VendorID               100 non-null    int64\\n'\n",
      "                         '1   tpep_pickup_datetime   100 non-null    '\n",
      "                         'datetime64[ns]\\n'\n",
      "                         '2   tpep_dropoff_datetime  100 non-null    '\n",
      "                         'datetime64[ns]\\n'\n",
      "                         '3   passenger_count        100 non-null    int64\\n'\n",
      "                         '4   trip_distance          100 non-null    float64\\n'\n",
      "                         '5   RatecodeID             100 non-null    int64\\n'\n",
      "                         '6   store_and_fwd_flag     100 non-null    object\\n'\n",
      "                         '7   PULocationID           100 non-null    int64\\n'\n",
      "                         '8   DOLocationID           100 non-null    int64\\n'\n",
      "                         '9   payment_type           100 non-null    int64\\n'\n",
      "                         '10  fare_amount            100 non-null    float64\\n'\n",
      "                         '11  extra                  100 non-null    float64\\n'\n",
      "                         '12  mta_tax                100 non-null    float64\\n'\n",
      "                         '13  tip_amount             100 non-null    float64\\n'\n",
      "                         '14  tolls_amount           100 non-null    float64\\n'\n",
      "                         '15  improvement_surcharge  100 non-null    float64\\n'\n",
      "                         '16  total_amount           100 non-null    float64\\n'\n",
      "                         '17  congestion_surcharge   100 non-null    float64\\n'\n",
      "                         'dtypes: datetime64[ns](2), float64(9), int64(6), '\n",
      "                         'object(1)\\n'\n",
      "                         'memory usage: 14.2+ KB'},\n",
      "                {'question': 'Python - Python cant ingest data from the github '\n",
      "                             'link provided using curl',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'os.system(f\"curl -LO {url} -o {csv_name}\")'},\n",
      "                {'question': 'Python - Pandas can read *.csv.gzip',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'When a CSV file is compressed using Gzip, it is '\n",
      "                         'saved with a \".csv.gz\" file extension. This file '\n",
      "                         'type is also known as a Gzip compressed CSV file. '\n",
      "                         'When you want to read a Gzip compressed CSV file '\n",
      "                         'using Pandas, you can use the read_csv() function, '\n",
      "                         'which is specifically designed to read CSV files. '\n",
      "                         'The read_csv() function accepts several parameters, '\n",
      "                         'including a file path or a file-like object. To read '\n",
      "                         'a Gzip compressed CSV file, you can pass the file '\n",
      "                         'path of the \".csv.gz\" file as an argument to the '\n",
      "                         'read_csv() function.\\n'\n",
      "                         'Here is an example of how to read a Gzip compressed '\n",
      "                         'CSV file using Pandas:\\n'\n",
      "                         \"df = pd.read_csv('file.csv.gz'\\n\"\n",
      "                         \", compression='gzip'\\n\"\n",
      "                         ', low_memory=False\\n'\n",
      "                         ')'},\n",
      "                {'question': 'Python - How to iterate through and ingest '\n",
      "                             'parquet file',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Contrary to panda’s read_csv method there’s no such '\n",
      "                         'easy way to iterate through and set chunksize for '\n",
      "                         'parquet files. We can use PyArrow (Apache Arrow '\n",
      "                         'Python bindings) to resolve that.\\n'\n",
      "                         'import pyarrow.parquet as pq\\n'\n",
      "                         'output_name = '\n",
      "                         '“https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet”\\n'\n",
      "                         'parquet_file = pq.ParquetFile(output_name)\\n'\n",
      "                         'parquet_size = parquet_file.metadata.num_rows\\n'\n",
      "                         'engine = '\n",
      "                         \"create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\\n\"\n",
      "                         'table_name=”yellow_taxi_schema”\\n'\n",
      "                         '# Clear table if exists\\n'\n",
      "                         'pq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, '\n",
      "                         \"con=engine, if_exists='replace')\\n\"\n",
      "                         '# default (and max) batch size\\n'\n",
      "                         'index = 65536\\n'\n",
      "                         'for i in '\n",
      "                         'parquet_file.iter_batches(use_threads=True):\\n'\n",
      "                         't_start = time()\\n'\n",
      "                         \"print(f'Ingesting {index} out of {parquet_size} rows \"\n",
      "                         \"({index / parquet_size:.0%})')\\n\"\n",
      "                         'i.to_pandas().to_sql(name=table_name, con=engine, '\n",
      "                         \"if_exists='append')\\n\"\n",
      "                         'index += 65536\\n'\n",
      "                         't_end = time()\\n'\n",
      "                         \"print(f'\\\\t- it took %.1f seconds' % (t_end - \"\n",
      "                         't_start))'},\n",
      "                {'question': 'Python - SQLAlchemy - ImportError: cannot import '\n",
      "                             \"name 'TypeAliasType' from 'typing_extensions'.\",\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Error raised during the jupyter notebook’s cell '\n",
      "                         'execution:\\n'\n",
      "                         'from sqlalchemy import create_engine.\\n'\n",
      "                         'Solution: Version of Python module '\n",
      "                         '“typing_extensions” >= 4.6.0. Can be updated by '\n",
      "                         'Conda or pip.'},\n",
      "                {'question': \"Python - SQLALchemy - TypeError 'module' object \"\n",
      "                             'is not callable',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': \"create_engine('postgresql://root:root@localhost:5432/ny_taxi')  \"\n",
      "                         'I get the error \"TypeError: \\'module\\' object is not '\n",
      "                         'callable\"\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'conn_string = '\n",
      "                         '\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\n'\n",
      "                         'engine = create_engine(conn_string)'},\n",
      "                {'question': 'Python - SQLAlchemy - ModuleNotFoundError: No '\n",
      "                             \"module named 'psycopg2'.\",\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Error raised during the jupyter notebook’s cell '\n",
      "                         'execution:\\n'\n",
      "                         'engine = '\n",
      "                         \"create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\n\"\n",
      "                         'Solution: Need to install Python module “psycopg2”. '\n",
      "                         'Can be installed by Conda or pip.'},\n",
      "                {'question': 'GCP - Unable to add Google Cloud SDK PATH to '\n",
      "                             'Windows',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Unable to add Google Cloud SDK PATH to Windows\\n'\n",
      "                         'Windows error: The installer is unable to '\n",
      "                         'automatically update your system PATH. Please add  '\n",
      "                         'C:\\\\tools\\\\google-cloud-sdk\\\\bin\\n'\n",
      "                         'if you are constantly getting this feedback. Might '\n",
      "                         'be that you needed to add Gitbash to your Windows '\n",
      "                         'path:\\n'\n",
      "                         'One way of doing that is to use conda: ‘If you are '\n",
      "                         'not already using it\\n'\n",
      "                         'Download the Anaconda Navigator\\n'\n",
      "                         'Make sure to check the box (add conda to the path '\n",
      "                         'when installing navigator: although not recommended '\n",
      "                         'do it anyway)\\n'\n",
      "                         'You might also need to install git bash if you are '\n",
      "                         'not already using it(or you might need to uninstall '\n",
      "                         'it to reinstall it properly)\\n'\n",
      "                         'Make sure to check the following boxes while you '\n",
      "                         'install Gitbash\\n'\n",
      "                         'Add a GitBash to Windows Terminal\\n'\n",
      "                         'Use Git and optional Unix tools from the command '\n",
      "                         'prompt\\n'\n",
      "                         'Now open up git bash and type conda init bash This '\n",
      "                         'should modify your bash profile\\n'\n",
      "                         'Additionally, you might want to use Gitbash as your '\n",
      "                         'default terminal.\\n'\n",
      "                         'Open your Windows terminal and go to settings, on '\n",
      "                         'the default profile change Windows power shell to '\n",
      "                         'git bash'},\n",
      "                {'question': 'GCP - Project creation failed: HttpError '\n",
      "                             'accessing … Requested entity '\n",
      "                             'alreadytpep_pickup_datetime exists',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'It asked me to create a project. This should be done '\n",
      "                         'from the cloud console. So maybe we don’t need this '\n",
      "                         'FAQ.\\n'\n",
      "                         'WARNING: Project creation failed: HttpError '\n",
      "                         'accessing '\n",
      "                         '<https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: '\n",
      "                         \"response: <{'vtpep_pickup_datetimeary': 'Origin, \"\n",
      "                         \"X-Origin, Referer', 'content-type': \"\n",
      "                         \"'application/json; charset=UTF-8', \"\n",
      "                         \"'content-encoding': 'gzip', 'date': 'Mon, 24 Jan \"\n",
      "                         \"2022 19:29:12 GMT', 'server': 'ESF', \"\n",
      "                         \"'cache-control': 'private', 'x-xss-protection': '0', \"\n",
      "                         \"'x-frame-options': 'SAMEORIGIN', \"\n",
      "                         \"'x-content-type-options': 'nosniff', \"\n",
      "                         \"'server-timing': 'gfet4t7; dur=189', 'alt-svc': \"\n",
      "                         '\\'h3=\":443\"; ma=2592000,h3-29=\":443\"; '\n",
      "                         'ma=2592000,h3-Q050=\":443\"; '\n",
      "                         'ma=2592000,h3-Q046=\":443\"; '\n",
      "                         'ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; '\n",
      "                         'ma=2592000; v=\"46,43\"\\', \\'transfer-encoding\\': '\n",
      "                         \"'chunked', 'status': 409}>, content <{\\n\"\n",
      "                         '\"error\": {\\n'\n",
      "                         '\"code\": 409,\\n'\n",
      "                         '\"message\": \"Requested entity '\n",
      "                         'alreadytpep_pickup_datetime exists\",\\n'\n",
      "                         '\"status\": \"ALREADY_EXISTS\"\\n'\n",
      "                         '}\\n'\n",
      "                         '}\\n'\n",
      "                         'From Stackoverflow: '\n",
      "                         'https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\\n'\n",
      "                         'Project IDs are unique across all projects. That '\n",
      "                         'means if any user ever had a project with that ID, '\n",
      "                         'you cannot use it. testproject is pretty common, so '\n",
      "                         \"it's not surprising it's already taken.\"},\n",
      "                {'question': 'GCP - The project to be billed is associated '\n",
      "                             'with an absent billing account',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'If you receive the error: “Error 403: The project to '\n",
      "                         'be billed is associated with an absent billing '\n",
      "                         'account., accountDisabled” It is most likely because '\n",
      "                         'you did not enter YOUR project ID. The snip below is '\n",
      "                         'from video 1.3.2\\n'\n",
      "                         'The value you enter here will be unique to each '\n",
      "                         'student. You can find this value on your GCP '\n",
      "                         'Dashboard when you login.\\n'\n",
      "                         'Ashish Agrawal\\n'\n",
      "                         'Another possibility is that you have not linked your '\n",
      "                         'billing account to your current project'},\n",
      "                {'question': 'GCP - OR-CBAT-15 ERROR Google cloud free trial '\n",
      "                             'account',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'GCP Account Suspension Inquiry\\n'\n",
      "                         'If Google refuses your credit/debit card, try '\n",
      "                         'another - I’ve got an issue with Kaspi (Kazakhstan) '\n",
      "                         'but it worked with TBC (Georgia).\\n'\n",
      "                         'Unfortunately, there’s small hope that support will '\n",
      "                         'help.\\n'\n",
      "                         'It seems that Pyypl web-card should work too.'},\n",
      "                {'question': 'GCP - Where can I find the “ny-rides.json” file?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'The ny-rides.json is your private file in Google '\n",
      "                         'Cloud Platform (GCP). \\n'\n",
      "                         '\\n'\n",
      "                         'And here’s the way to find it:\\n'\n",
      "                         'GCP -> Select project with your  instance -> IAM & '\n",
      "                         'Admin -> Service Accounts Keys tab -> add key, JSON '\n",
      "                         'as key type, then click create\\n'\n",
      "                         'Note: Once you go into Service Accounts Keys tab, '\n",
      "                         'click the email, then you can see the “KEYS” tab '\n",
      "                         'where you can add key as a JSON as its key type'},\n",
      "                {'question': 'GCP - Do I need to delete my instance in Google '\n",
      "                             'Cloud?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'In this lecture, Alexey deleted his instance in '\n",
      "                         'Google Cloud. Do I have to do it?\\n'\n",
      "                         'Nope. Do not delete your instance in Google Cloud '\n",
      "                         'platform. Otherwise, you have to do this twice for '\n",
      "                         'the week 1 readings.'},\n",
      "                {'question': 'Commands to inspect the health of your VM:',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'System Resource Usage:\\n'\n",
      "                         'top or htop: Shows real-time information about '\n",
      "                         'system resource usage, including CPU, memory, and '\n",
      "                         'processes.\\n'\n",
      "                         'free -h: Displays information about system memory '\n",
      "                         'usage and availability.\\n'\n",
      "                         'df -h: Shows disk space usage of file systems.\\n'\n",
      "                         'du -h <directory>: Displays disk usage of a specific '\n",
      "                         'directory.\\n'\n",
      "                         'Running Processes:\\n'\n",
      "                         'ps aux: Lists all running processes along with '\n",
      "                         'detailed information.\\n'\n",
      "                         'Network:\\n'\n",
      "                         'ifconfig or ip addr show: Shows network interface '\n",
      "                         'configuration.\\n'\n",
      "                         'netstat -tuln: Displays active network connections '\n",
      "                         'and listening ports.\\n'\n",
      "                         'Hardware Information:\\n'\n",
      "                         'lscpu: Displays CPU information.\\n'\n",
      "                         'lsblk: Lists block devices (disks and partitions).\\n'\n",
      "                         'lshw: Lists hardware configuration.\\n'\n",
      "                         'User and Permissions:\\n'\n",
      "                         'who: Shows who is logged on and their activities.\\n'\n",
      "                         'w: Displays information about currently logged-in '\n",
      "                         'users and their processes.\\n'\n",
      "                         'Package Management:\\n'\n",
      "                         'apt list --installed: Lists installed packages (for '\n",
      "                         'Ubuntu and Debian-based systems)'},\n",
      "                {'question': 'Billing account has not been enabled for this '\n",
      "                             'project. But you’ve done it indeed!',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'if you’ve got the error\\n'\n",
      "                         '│ Error: Error updating Dataset '\n",
      "                         '\"projects/<your-project-id>/datasets/demo_dataset\": '\n",
      "                         'googleapi: Error 403: Billing has not been enabled '\n",
      "                         'for this project. Enable billing at '\n",
      "                         'https://console.cloud.google.com/billing. The '\n",
      "                         'default table expiration time must be less than 60 '\n",
      "                         'days, billingNotEnabled\\n'\n",
      "                         'but you’ve set your billing account indeed, then try '\n",
      "                         'to disable billing for the project and enable it '\n",
      "                         'again. It worked for ME!'},\n",
      "                {'question': 'GCP - Windows Google Cloud SDK install issue:gcp',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'for windows if you having trouble install SDK try '\n",
      "                         'follow these steps on the link, if you getting this '\n",
      "                         'error:\\n'\n",
      "                         'These credentials will be used by any library that '\n",
      "                         'requests Application Default Credentials (ADC).\\n'\n",
      "                         'WARNING:\\n'\n",
      "                         'Cannot find a quota project to add to ADC. You might '\n",
      "                         'receive a \"quota exceeded\" or \"API not enabled\" '\n",
      "                         'error. Run $ gcloud auth application-default '\n",
      "                         'set-quota-project to add a quota project.\\n'\n",
      "                         'For me:\\n'\n",
      "                         'I reinstalled the sdk using unzip file '\n",
      "                         '“install.bat”,\\n'\n",
      "                         'after successfully checking gcloud version,\\n'\n",
      "                         'run gcloud init to set up project before\\n'\n",
      "                         'you run gcloud auth application-default login\\n'\n",
      "                         'https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\\n'\n",
      "                         'GCP VM - I cannot get my Virtual Machine to start '\n",
      "                         'because GCP has no resources.\\n'\n",
      "                         'Click on your VM\\n'\n",
      "                         'Create an image of your VM\\n'\n",
      "                         'On the page of the image, tell GCP to create a new '\n",
      "                         'VM instance via the image\\n'\n",
      "                         'On the settings page, change the location'},\n",
      "                {'question': 'GCP VM - Is it necessary to use a GCP VM? When '\n",
      "                             'is it useful?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'The reason this video about the GCP VM exists is '\n",
      "                         'that many students had problems configuring their '\n",
      "                         'env. You can use your own env if it works for you.\\n'\n",
      "                         'And the advantage of using your own environment is '\n",
      "                         'that if you are working in a Github repo where you '\n",
      "                         'can commit, you will be able to commit the changes '\n",
      "                         'that you do. In the VM the repo is cloned via HTTPS '\n",
      "                         'so it is not possible to directly commit, even if '\n",
      "                         'you are the owner of the repo.'},\n",
      "                {'question': 'GCP VM - mkdir: cannot create directory ‘.ssh’: '\n",
      "                             'Permission denied',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': \"I am trying to create a directory but it won't let \"\n",
      "                         'me do it\\n'\n",
      "                         'User1@DESKTOP-PD6UM8A MINGW64 /\\n'\n",
      "                         '$ mkdir .ssh\\n'\n",
      "                         'mkdir: cannot create directory ‘.ssh’: Permission '\n",
      "                         'denied\\n'\n",
      "                         'You should do it in your home directory. Should be '\n",
      "                         'your home (~)\\n'\n",
      "                         \"Local. But it seems you're trying to do it in the \"\n",
      "                         'root folder (/). Should be your home (~)\\n'\n",
      "                         'Link to Video 1.4.1'},\n",
      "                {'question': 'GCP VM - Error while saving the file in VM via '\n",
      "                             'VS Code',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': \"Failed to save '<file>': Unable to write file \"\n",
      "                         \"'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' \"\n",
      "                         '(NoPermissions (FileSystemError): Error: EACCES: '\n",
      "                         'permission denied, open '\n",
      "                         \"'/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\\n\"\n",
      "                         'You need to change the owner of the files you are '\n",
      "                         'trying to edit via VS Code. You can run the '\n",
      "                         'following command to change the ownership.\\n'\n",
      "                         'ssh\\n'\n",
      "                         'sudo chown -R <user> <path to your directory>'},\n",
      "                {'question': '. GCP VM - VM connection request timeout',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Question: I connected to my VM perfectly fine last '\n",
      "                         'week (ssh) but when I tried again this week, the '\n",
      "                         'connection request keeps timing out.\\n'\n",
      "                         '✅Answer: Start your VM. Once the VM is running, copy '\n",
      "                         'its External IP and paste that into your config file '\n",
      "                         'within the ~/.ssh folder.\\n'\n",
      "                         'cd ~/.ssh\\n'\n",
      "                         'code config ← this opens the config file in VSCode'},\n",
      "                {'question': 'GCP VM -  connect to host port 22 no route to '\n",
      "                             'host',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': '(reference: '\n",
      "                         'https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go '\n",
      "                         'to edit your VM.\\n'\n",
      "                         'Go to section Automation\\n'\n",
      "                         'Add Startup script\\n'\n",
      "                         '```\\n'\n",
      "                         '#!/bin/bash\\n'\n",
      "                         'sudo ufw allow ssh\\n'\n",
      "                         '```\\n'\n",
      "                         'Stop and Start VM.'},\n",
      "                {'question': 'GCP VM - Port forwarding from GCP without using '\n",
      "                             'VS Code',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'You can easily forward the ports of pgAdmin, '\n",
      "                         'postgres and Jupyter Notebook using the built-in '\n",
      "                         'tools in Ubuntu and without any additional client:\\n'\n",
      "                         'First, in the VM machine, launch docker-compose up '\n",
      "                         '-d and jupyter notebook in the correct folder.\\n'\n",
      "                         'From the local machine, execute: ssh -i ~/.ssh/gcp '\n",
      "                         '-L 5432:localhost:5432 username@external_ip_of_vm\\n'\n",
      "                         'Execute the same command but with ports 8080 and '\n",
      "                         '8888.\\n'\n",
      "                         'Now you can access pgAdmin on local machine in '\n",
      "                         'browser typing localhost:8080\\n'\n",
      "                         'For Jupyter Notebook, type localhost:8888 in the '\n",
      "                         'browser of your local machine. If you have problems '\n",
      "                         'with the credentials, it is possible that you have '\n",
      "                         'to copy the link with the access token provided in '\n",
      "                         'the logs of the terminal of the VM machine when you '\n",
      "                         'launched the jupyter notebook command.\\n'\n",
      "                         'To forward both pgAdmin and postgres use, ssh -i '\n",
      "                         '~/.ssh/gcp -L 5432:localhost:5432 -L '\n",
      "                         '8080:localhost:8080 modito@35.197.218.128'},\n",
      "                {'question': 'GCP gcloud + MS VS Code - gcloud auth hangs',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'If you are using MS VS Code and running gcloud in '\n",
      "                         'WSL2, when you first try to login to gcp via the '\n",
      "                         'gcloud cli gcloud auth application-default login, '\n",
      "                         'you will see a message like this, and nothing will '\n",
      "                         'happen\\n'\n",
      "                         'And there might be a prompt to ask if you want to '\n",
      "                         'open it via browser, if you click on it, it will '\n",
      "                         'open up a page with error message\\n'\n",
      "                         'Solution : you should instead hover on the long '\n",
      "                         'link, and ctrl + click the long link\\n'\n",
      "                         '\\n'\n",
      "                         'Click configure Trusted Domains here\\n'\n",
      "                         '\\n'\n",
      "                         'Popup will appear, pick first or second entry\\n'\n",
      "                         'Next time you gcloud auth, the login page should '\n",
      "                         'popup via default browser without issues'},\n",
      "                {'question': 'Terraform - Error: Failed to query available '\n",
      "                             'provider packages │ Could not retrieve the list '\n",
      "                             'of available versions for provider '\n",
      "                             'hashicorp/google: could not query │ provider '\n",
      "                             'registry for '\n",
      "                             'registry.terrafogorm.io/hashicorp/google: the '\n",
      "                             'request failed after 2 attempts, │ please try '\n",
      "                             'again later',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'It is an internet connectivity error, terraform is '\n",
      "                         'somehow not able to access the online registry. '\n",
      "                         'Check your VPN/Firewall settings (or just clear '\n",
      "                         'cookies or restart your network). Try terraform init '\n",
      "                         'again after this, it should work.'},\n",
      "                {'question': 'Terraform - Error:Post '\n",
      "                             '\"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": '\n",
      "                             'oauth2: cannot fetch token: Post '\n",
      "                             '\"https://oauth2.googleapis.com/token\": dial tcp '\n",
      "                             '172.217.163.42:443: i/o timeout',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'The issue was with the network. Google is not '\n",
      "                         'accessible in my country, I am using a VPN. And The '\n",
      "                         'terminal program does not automatically follow the '\n",
      "                         'system proxy and requires separate proxy '\n",
      "                         'configuration settings.I opened a Enhanced Mode in '\n",
      "                         \"Clash, which is a VPN app, and 'terraform apply' \"\n",
      "                         'works! So if you encounter the same issue, you can '\n",
      "                         'ask help for your vpn provider.'},\n",
      "                {'question': 'Terraform - Install for WSL',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845'},\n",
      "                {'question': 'Terraform - Error acquiring the state lock',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'https://github.com/hashicorp/terraform/issues/14513'},\n",
      "                {'question': 'Terraform - Error 400 Bad Request.  Invalid JWT '\n",
      "                             'Token  on WSL.',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'When running\\n'\n",
      "                         'terraform apply\\n'\n",
      "                         \"on wsl2 I've got this error:\\n\"\n",
      "                         '│ Error: Post '\n",
      "                         '\"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": '\n",
      "                         'oauth2: cannot fetch token: 400 Bad Request\\n'\n",
      "                         '│ Response: '\n",
      "                         '{\"error\":\"invalid_grant\",\"error_description\":\"Invalid '\n",
      "                         'JWT: Token must be a short-lived token (60 minutes) '\n",
      "                         'and in a reasonable timeframe. Check your iat and '\n",
      "                         'exp values in the JWT claim.\"}\\n'\n",
      "                         'IT happens because there may be time desync on your '\n",
      "                         'machine which affects computing JWT\\n'\n",
      "                         'To fix this, run the command\\n'\n",
      "                         'sudo hwclock -s\\n'\n",
      "                         'which fixes your system time.\\n'\n",
      "                         'Reference'},\n",
      "                {'question': 'Terraform - Error 403 : Access denied',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': '│ Error: googleapi: Error 403: Access denied., '\n",
      "                         'forbidden\\n'\n",
      "                         'Your $GOOGLE_APPLICATION_CREDENTIALS might not be '\n",
      "                         'pointing to the correct file \\n'\n",
      "                         'run = export '\n",
      "                         'GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\\n'\n",
      "                         'And then = gcloud auth activate-service-account '\n",
      "                         '--key-file $GOOGLE_APPLICATION_CREDENTIALS'},\n",
      "                {'question': 'Terraform - Do I need to make another service '\n",
      "                             'account for terraform before I get the keys '\n",
      "                             '(.json file)?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'One service account is enough for all the '\n",
      "                         \"services/resources you'll use in this course. After \"\n",
      "                         'you get the file with your credentials and set your '\n",
      "                         'environment variable, you should be good to go.'},\n",
      "                {'question': 'Terraform - Where can I find the Terraform 1.1.3 '\n",
      "                             'Linux (AMD 64)?',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Here: '\n",
      "                         'https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip'},\n",
      "                {'question': 'Terraform - Terraform initialized in an empty '\n",
      "                             'directory! The directory has no Terraform '\n",
      "                             'configuration files. You may begin working with '\n",
      "                             'Terraform immediately by creating Terraform '\n",
      "                             'configuration files.g',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'You get this error because I run the command '\n",
      "                         'terraform init outside the working directory, and '\n",
      "                         'this is wrong.You need first to navigate to the '\n",
      "                         'working directory that contains terraform '\n",
      "                         'configuration files, and and then run the command.'},\n",
      "                {'question': 'Terraform - Error creating Dataset: googleapi: '\n",
      "                             'Error 403: Request had insufficient '\n",
      "                             'authentication scopes',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'The error:\\n'\n",
      "                         'Error: googleapi: Error 403: Access denied., '\n",
      "                         'forbidden\\n'\n",
      "                         '│\\n'\n",
      "                         'and\\n'\n",
      "                         '│ Error: Error creating Dataset: googleapi: Error '\n",
      "                         '403: Request had insufficient authentication '\n",
      "                         'scopes.\\n'\n",
      "                         'For this solution make sure to run:\\n'\n",
      "                         'echo $GOOGLE_APPLICATION_CREDENTIALS\\n'\n",
      "                         'echo $?\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'You have to set again the '\n",
      "                         'GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the '\n",
      "                         'environment set-up video in week1:\\n'\n",
      "                         'export '\n",
      "                         'GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json'},\n",
      "                {'question': 'Terraform - Error creating Bucket: googleapi: '\n",
      "                             'Error 403: Permission denied to access '\n",
      "                             '‘storage.buckets.create’',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'The error:\\n'\n",
      "                         'Error: googleapi: Error 403: '\n",
      "                         'terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com '\n",
      "                         'does not have storage.buckets.create access to the '\n",
      "                         'Google Cloud project. Permission '\n",
      "                         \"'storage.buckets.create' denied on resource (or it \"\n",
      "                         'may not exist)., forbidden\\n'\n",
      "                         'The solution:\\n'\n",
      "                         'You have to declare the project name as your Project '\n",
      "                         'ID, and not your Project name, available on GCP '\n",
      "                         'console Dashboard.'},\n",
      "                {'question': 'To ensure the sensitivity of the credentials '\n",
      "                             'file, I had to spend lot of time to input that '\n",
      "                             'as a file.',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'provider \"google\" {\\n'\n",
      "                         'project     = var.projectId\\n'\n",
      "                         'credentials = file(\"${var.gcpkey}\")\\n'\n",
      "                         '#region      = var.region\\n'\n",
      "                         'zone = var.zone\\n'\n",
      "                         '}'},\n",
      "                {'question': 'SQL - SELECT * FROM zones_taxi WHERE '\n",
      "                             \"Zone='Astoria Zone'; Error Column Zone doesn't \"\n",
      "                             'exist',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'For the HW1 I encountered this issue. The solution '\n",
      "                         'is\\n'\n",
      "                         'SELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria '\n",
      "                         \"Zone';\\n\"\n",
      "                         'I think columns which start with uppercase need to '\n",
      "                         'go between “Column”. I ran into a lot of issues like '\n",
      "                         'this and “ ” made it work out.\\n'\n",
      "                         'Addition to the above point, for me, there is no '\n",
      "                         '‘Astoria Zone’, only ‘Astoria’ is existing in the '\n",
      "                         'dataset.\\n'\n",
      "                         'SELECT * FROM zones AS z WHERE z.\"Zone\" = '\n",
      "                         \"'Astoria’;\"},\n",
      "                {'question': 'SQL - SELECT Zone FROM taxi_zones Error Column '\n",
      "                             \"Zone doesn't exist\",\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'It is inconvenient to use quotation marks all the '\n",
      "                         'time, so it is better to put the data to the '\n",
      "                         'database all in lowercase, so in Pandas after\\n'\n",
      "                         'df = pd.read_csv(‘taxi+_zone_lookup.csv’)\\n'\n",
      "                         'Add the row:\\n'\n",
      "                         'df.columns = df.columns.str.lower()'},\n",
      "                {'question': 'CURL - curl: (6) Could not resolve host: '\n",
      "                             'output.csv',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Solution (for mac users): os.system(f\"curl {url} '\n",
      "                         '--output {csv_name}\")'},\n",
      "                {'question': 'SSH Error: ssh: Could not resolve hostname '\n",
      "                             'linux: Name or service not known',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'To resolve this, ensure that your config file is in '\n",
      "                         'C/User/Username/.ssh/config'},\n",
      "                {'question': \"'pip' is not recognized as an internal or \"\n",
      "                             'external command, operable program or batch '\n",
      "                             'file.',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'If you use Anaconda (recommended for the course), it '\n",
      "                         'comes with pip, so the issues is probably that the '\n",
      "                         'anaconda’s Python is not on the PATH.\\n'\n",
      "                         'Adding it to the PATH is different for each '\n",
      "                         'operation system.\\n'\n",
      "                         'For Linux and MacOS:\\n'\n",
      "                         'Open a terminal.\\n'\n",
      "                         'Find the path to your Anaconda installation. This is '\n",
      "                         'typically `~/anaconda3` or `~/opt/anaconda3`.\\n'\n",
      "                         'Add Anaconda to your PATH with the command: `export '\n",
      "                         'PATH=\"/path/to/anaconda3/bin:$PATH\"`.\\n'\n",
      "                         'To make this change permanent, add the command to '\n",
      "                         'your `.bashrc` (Linux) or `.bash_profile` (MacOS) '\n",
      "                         'file.\\n'\n",
      "                         'On Windows, python and pip are in different '\n",
      "                         'locations (python is in the anaconda root, and pip '\n",
      "                         'is in Scripts). With GitBash:\\n'\n",
      "                         'Locate your Anaconda installation. The default path '\n",
      "                         'is usually `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3`.\\n'\n",
      "                         'Determine the correct path format for Git Bash. '\n",
      "                         'Paths in Git Bash follow the Unix-style, so convert '\n",
      "                         'the Windows path to a Unix-style path. For example, '\n",
      "                         '`C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` becomes '\n",
      "                         '`/c/Users/[YourUsername]/Anaconda3`.\\n'\n",
      "                         'Add Anaconda to your PATH with the command: `export '\n",
      "                         'PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\\n'\n",
      "                         'To make this change permanent, add the command to '\n",
      "                         'your `.bashrc` file in your home directory.\\n'\n",
      "                         'Refresh your environment with the command: `source '\n",
      "                         '~/.bashrc`.\\n'\n",
      "                         'For Windows (without Git Bash):\\n'\n",
      "                         \"Right-click on 'This PC' or 'My Computer' and select \"\n",
      "                         \"'Properties'.\\n\"\n",
      "                         \"Click on 'Advanced system settings'.\\n\"\n",
      "                         'In the System Properties window, click on '\n",
      "                         \"'Environment Variables'.\\n\"\n",
      "                         'In the Environment Variables window, select the '\n",
      "                         \"'Path' variable in the 'System variables' section \"\n",
      "                         \"and click 'Edit'.\\n\"\n",
      "                         \"In the Edit Environment Variable window, click 'New' \"\n",
      "                         'and add the path to your Anaconda installation '\n",
      "                         '(typically `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` '\n",
      "                         'and '\n",
      "                         'C:\\\\Users\\\\[YourUsername]\\\\Anaconda3\\\\Scripts`).\\n'\n",
      "                         \"Click 'OK' in all windows to apply the changes.\\n\"\n",
      "                         'After adding Anaconda to the PATH, you should be '\n",
      "                         'able to use `pip` from the command line. Remember to '\n",
      "                         'restart your terminal (or command prompt in Windows) '\n",
      "                         'to apply these changes.'},\n",
      "                {'question': 'Error: error starting userland proxy: listen '\n",
      "                             'tcp4 0.0.0.0:8080: bind: address already in use',\n",
      "                 'section': 'Module 1: Docker and Terraform',\n",
      "                 'text': 'Resolution: You need to stop the services which is '\n",
      "                         'using the port.\\n'\n",
      "                         'Run the following:\\n'\n",
      "                         '```\\n'\n",
      "                         'sudo kill -9 `sudo lsof -t -i:<port>`\\n'\n",
      "                         '```\\n'\n",
      "                         '<port> being 8080 in this case. This will free up '\n",
      "                         'the port for use.\\n'\n",
      "                         '~ Abhijit Chakraborty\\n'\n",
      "                         'Error: error response from daemon: cannot stop '\n",
      "                         'container: '\n",
      "                         '1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: '\n",
      "                         'permission denied\\n'\n",
      "                         'Resolution: In my case, I had to stop docker and '\n",
      "                         'restart the service to get it running properly\\n'\n",
      "                         'Use the following command:\\n'\n",
      "                         '```\\n'\n",
      "                         'sudo systemctl restart docker.socket docker.service\\n'\n",
      "                         '```\\n'\n",
      "                         '~ Abhijit Chakraborty\\n'\n",
      "                         'Error: cannot import module psycopg2\\n'\n",
      "                         'Resolution: Run the following command in linux:\\n'\n",
      "                         '```\\n'\n",
      "                         'sudo apt-get install libpq-dev\\n'\n",
      "                         'pip install psycopg2\\n'\n",
      "                         '```\\n'\n",
      "                         '~ Abhijit Chakraborty\\n'\n",
      "                         \"Error: docker build Error checking context: 'can't \"\n",
      "                         \"stat '<path-to-file>'\\n\"\n",
      "                         'Resolution: This happens due to insufficient '\n",
      "                         'permission for docker to access a certain file '\n",
      "                         'within the directory which hosts the Dockerfile.\\n'\n",
      "                         '1. You can create a .dockerignore file and add the '\n",
      "                         'directory/file which you want Dockerfile to ignore '\n",
      "                         'while build.\\n'\n",
      "                         '2. If the above does not work, then put the '\n",
      "                         'dockerfile and corresponding script, `\\t1.py` in our '\n",
      "                         'case to a subfolder. and run `docker build ...`\\n'\n",
      "                         'from inside the new folder.\\n'\n",
      "                         '~ Abhijit Chakraborty'},\n",
      "                {'question': 'Anaconda to PIP',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'To get a pip-friendly requirements.txt file file '\n",
      "                         'from Anaconda use\\n'\n",
      "                         'conda install pip then `pip list –format=freeze > '\n",
      "                         'requirements.txt`.\\n'\n",
      "                         '`conda list -d > requirements.txt` will not work and '\n",
      "                         '`pip freeze > requirements.txt` may give odd '\n",
      "                         'pathing.'},\n",
      "                {'question': 'Where are the FAQ questions from the previous '\n",
      "                             'cohorts for the orchestration module?',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Prefect: '\n",
      "                         'https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\\n'\n",
      "                         'Airflow: '\n",
      "                         'https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing'},\n",
      "                {'question': 'Docker - 2.2.2 Configure Mage',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Issue : Docker containers exit instantly with code '\n",
      "                         '132, upon docker compose up\\n'\n",
      "                         'Mage documentation has it listing the cause as '\n",
      "                         '\"older architecture\" .\\n'\n",
      "                         'This might be a hardware issue, so unless you have '\n",
      "                         \"another computer, you can't solve it without \"\n",
      "                         'purchasing a new one, so the next best solution is a '\n",
      "                         'VM.\\n'\n",
      "                         'This is from a student running on a VirtualBox VM, '\n",
      "                         'Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not '\n",
      "                         'having the context on how the vbox was spin up with '\n",
      "                         '(CPU, RAM, network, etc), it’s really inconclusive '\n",
      "                         'at this time.'},\n",
      "                {'question': 'WSL - 2.2.3 Mage - Unexpected Kernel Restarts; '\n",
      "                             'Kernel Running out of memory:',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'This issue was occurring with Windows WSL 2\\n'\n",
      "                         'For me this was because WSL 2 was not dedicating '\n",
      "                         'enough cpu cores to Docker.The load seems to take up '\n",
      "                         'at least one cpu core so I recommend dedicating at '\n",
      "                         'least two.\\n'\n",
      "                         'Open Bash and run the following code:\\n'\n",
      "                         '$ cd ~\\n'\n",
      "                         '$ ls -la\\n'\n",
      "                         'Look for the .wsl config file:\\n'\n",
      "                         '-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  '\n",
      "                         '.wslconfig\\n'\n",
      "                         'Using a text editing tool of your choice edit or '\n",
      "                         'create your .wslconfig file:\\n'\n",
      "                         '$ nano .wslconfig\\n'\n",
      "                         'Paste the following into the new file/ edit the '\n",
      "                         'existing file in this format and save:\\n'\n",
      "                         '*** Note - for memory– this is the RAM on your '\n",
      "                         'machine you can dedicate to Docker, your situation '\n",
      "                         'may be different than mine ***\\n'\n",
      "                         '[wsl2]\\n'\n",
      "                         'processors=<Number of Processors - at least 2!> '\n",
      "                         'example: 4\\n'\n",
      "                         'memory=<memory> example:4GB\\n'\n",
      "                         'Example:\\n'\n",
      "                         'Once you do that run:\\n'\n",
      "                         '$ wsl --shutdown\\n'\n",
      "                         'This shuts down WSL\\n'\n",
      "                         'Then Restart Docker Desktop - You should now be able '\n",
      "                         'to load the .csv.gz file without the error into a '\n",
      "                         'pandas dataframe'},\n",
      "                {'question': '2.2.3 Configuring Postgres',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'The issue and solution on the link:\\n'\n",
      "                         'https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG'},\n",
      "                {'question': 'MAGE - 2.2.3 OperationalError: '\n",
      "                             '(psycopg2.OperationalError) connection to server '\n",
      "                             'at \"localhost\" (::1), port 5431 failed: '\n",
      "                             'Connection refused',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Check that the POSTGRES_PORT variable in the '\n",
      "                         'io_config.yml  file is set to port 5432, which is '\n",
      "                         'the default postgres port. The POSTGRES_PORT '\n",
      "                         'variable is the mage container port, not the host '\n",
      "                         'port. Hence, there’s no need to set the '\n",
      "                         'POSTGRES_PORT to 5431 just because you already have '\n",
      "                         'a conflicting postgres installation in your host '\n",
      "                         'machine.'},\n",
      "                {'question': 'MAGE - 2.2.4 executing SELECT 1; results in '\n",
      "                             'KeyError',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'You forgot to select ‘dev’ profile in the dropdown '\n",
      "                         'menu next to where you select ‘PostgreSQL’ in the '\n",
      "                         'connection drop down.'},\n",
      "                {'question': \"MAGE -2.2.4 ConnectionError: ('Connection \"\n",
      "                             \"aborted.', TimeoutError('The write operation \"\n",
      "                             \"timed out'))\",\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'If you are getting this error. Update your mage '\n",
      "                         'io_config.yaml file, and specify a timeout value set '\n",
      "                         'to 600 like this.\\n'\n",
      "                         'Make sure to save your changes.\\n'\n",
      "                         'MAGE - 2.2.4 Testing BigQuery connection using SQL '\n",
      "                         '404 error:\\n'\n",
      "                         'NotFound: 404 Not found: Dataset '\n",
      "                         'ny-rides-diegogutierrez:None was not found in '\n",
      "                         'location northamerica-northeast1\\n'\n",
      "                         'If you get this error even with all '\n",
      "                         'roles/permissions given to the service account check '\n",
      "                         'if you have ticked the box where it says “Use raw '\n",
      "                         'SQL”, just like the image below.'},\n",
      "                {'question': \"Problem: RefreshError: ('invalid_grant: Invalid \"\n",
      "                             'JWT: Token must be a short-lived token (60 '\n",
      "                             'minutes) and in a reasonable timeframe. Check '\n",
      "                             \"your iat and exp values in the JWT claim.', \"\n",
      "                             \"{'error': 'invalid_grant', 'error_description': \"\n",
      "                             \"'Invalid JWT: Token must be a short-lived token \"\n",
      "                             '(60 minutes) and in a reasonable timeframe. '\n",
      "                             'Check your iat and exp values in the JWT '\n",
      "                             \"claim.'})\",\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Solution: '\n",
      "                         'https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token'},\n",
      "                {'question': 'Mage - 2.2.4 IndexError: list index out of range',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Origin of Solution (Mage Slack-Channel): '\n",
      "                         'https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\\n'\n",
      "                         'Problem: This error can often be seen after solving '\n",
      "                         'the error mentioned in 2.2.4. The error can be found '\n",
      "                         'in Mage version 0.9.61 and is a side-effect of the '\n",
      "                         'update of the code for data-loader blocks.\\n'\n",
      "                         'Note: Mage 0.9.62 has been released, as of Feb 5 '\n",
      "                         '2024. Please recheck. Solution below may be '\n",
      "                         'obsolete\\n'\n",
      "                         'Solution: Using a “fixed” version of the docker '\n",
      "                         'container\\n'\n",
      "                         'Pull updated docker image from docker-hub\\n'\n",
      "                         'mageai/mageaidocker pull:alpha\\n'\n",
      "                         'Update docker-compose.yaml\\n'\n",
      "                         \"version: '3'\\n\"\n",
      "                         'services:\\n'\n",
      "                         'magic:\\n'\n",
      "                         'image: mageai/mageai:alpha  <--- instead of '\n",
      "                         '“latest”-tag\\n'\n",
      "                         'docker-compose up\\n'\n",
      "                         'The original Error is still present, but the '\n",
      "                         'SQL-query will return the desired result:\\n'\n",
      "                         '--------------------------------------------------------------------------------------'},\n",
      "                {'question': '2.2.6 OSError: Cannot save file into a '\n",
      "                             'non-existent directory: '\n",
      "                             '\\'..\\\\\\\\..\\\\\\\\data\\\\\\\\yellow\\'\\\\n\")',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Add\\n'\n",
      "                         'if not path.parent.is_dir():\\n'\n",
      "                         'path.parent.mkdir(parents=True)\\n'\n",
      "                         'path = Path(path).as_posix()\\n'\n",
      "                         'see:\\n'\n",
      "                         'https://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG'},\n",
      "                {'question': 'GCP - 2.2.7d Deploying Mage to GCP',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'The video DE Zoomcamp 2.2.7 is missing  the actual '\n",
      "                         'deployment of Mage using Terraform to GCP. The steps '\n",
      "                         'for the deployment were not covered in the video.\\n'\n",
      "                         'I successfully deployed it and wanted to share some '\n",
      "                         'key points:\\n'\n",
      "                         'In variables.tf, set the project_id default value to '\n",
      "                         'your GCP project ID.\\n'\n",
      "                         'Enable the Cloud Filestore API:\\n'\n",
      "                         'Visit the Google Cloud Console.to\\n'\n",
      "                         'Navigate to \"APIs & Services\" > \"Library.\"\\n'\n",
      "                         'Search for \"Cloud Filestore API.\"\\n'\n",
      "                         'Click on the API and enable it.\\n'\n",
      "                         'To perform the deployment:\\n'\n",
      "                         'terraform init\\n'\n",
      "                         'terraform apply\\n'\n",
      "                         'Please note that during the terraform apply step, '\n",
      "                         'Terraform will prompt you to enter the PostgreSQL '\n",
      "                         'password. After that, it will ask for confirmation '\n",
      "                         'to proceed with the deployment. Review the changes, '\n",
      "                         \"type 'yes' when prompted, and press Enter.\"},\n",
      "                {'question': 'Ruuning Multiple Mage instances in Docker from '\n",
      "                             'different directories',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'If you want to rune multiple docker containers from '\n",
      "                         'different directories. Then make sure to change the '\n",
      "                         'port mappings in the docker-compose.yml file.\\n'\n",
      "                         'ports:\\n'\n",
      "                         '- 8088:6789\\n'\n",
      "                         'The 8088 port in above case is hostport, where mage '\n",
      "                         'will run on your local machine. You can customize '\n",
      "                         'this as long as the port is available. If you are '\n",
      "                         'running on VM, make sure to forward the port too. '\n",
      "                         'You need to keep the container port to 6789 as this '\n",
      "                         'is the port where mage is running.\\n'\n",
      "                         'GCP - 2.2.7d Deploying Mage to Google Cloud\\n'\n",
      "                         'While terraforming all the resources inside a VM '\n",
      "                         'created in GCS the following error is shown.\\n'\n",
      "                         'Error log:\\n'\n",
      "                         'module.lb-http.google_compute_backend_service.default[\"default\"]: '\n",
      "                         'Creating...\\n'\n",
      "                         '╷\\n'\n",
      "                         '│ Error: Error creating GlobalAddress: googleapi: '\n",
      "                         'Error 403: Request had insufficient authentication '\n",
      "                         'scopes.\\n'\n",
      "                         '│ Details:\\n'\n",
      "                         '│ [\\n'\n",
      "                         '│   {\\n'\n",
      "                         '│     \"@type\": '\n",
      "                         '\"type.googleapis.com/google.rpc.ErrorInfo\",\\n'\n",
      "                         '│     \"domain\": \"googleapis.com\",\\n'\n",
      "                         '│     \"metadatas\": {\\n'\n",
      "                         '│       \"method\": '\n",
      "                         '\"compute.beta.GlobalAddressesService.Insert\",\\n'\n",
      "                         '│       \"service\": \"compute.googleapis.com\"\\n'\n",
      "                         '│     },\\n'\n",
      "                         '│     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\\n'\n",
      "                         '│   }\\n'\n",
      "                         '│ ]\\n'\n",
      "                         '│\\n'\n",
      "                         '│ More details:\\n'\n",
      "                         '│ Reason: insufficientPermissions, Message: '\n",
      "                         'Insufficient Permission\\n'\n",
      "                         'This error might happen when you are using a VM '\n",
      "                         'inside GCS. To use the Google APIs from a GCP '\n",
      "                         'virtual machine you need to add the cloud platform '\n",
      "                         'scope '\n",
      "                         '(\"https://www.googleapis.com/auth/cloud-platform\") '\n",
      "                         'to your VM when it is created.\\n'\n",
      "                         'Since ours is already created you can just stop it '\n",
      "                         'and change the permissions. You can do it in the '\n",
      "                         'console, just go to \"EDIT\", g99o all the way down '\n",
      "                         'until you find \"Cloud API access scopes\". There you '\n",
      "                         'can \"Allow full access to all Cloud APIs\". I did '\n",
      "                         'this and all went smoothly generating all the '\n",
      "                         'resources needed. Hope it helps if you encounter '\n",
      "                         'this same error.\\n'\n",
      "                         'Resources: '\n",
      "                         'https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu'},\n",
      "                {'question': 'GCP - 2.2.7d Load Balancer Problem (Security '\n",
      "                             'Policies quota)',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'If you are on the free trial account on GCP you will '\n",
      "                         'face this issue when trying to deploy the '\n",
      "                         'infrastructures with terraform. This service is not '\n",
      "                         'available for this kind of account.\\n'\n",
      "                         'The solution I found was to delete the '\n",
      "                         'load_balancer.tf file and to comment or delete the '\n",
      "                         'rows that differentiate it on the main.tf file. '\n",
      "                         'After this just do terraform destroy to delete any '\n",
      "                         'infrastructure created on the fail attempts and '\n",
      "                         're-run the terraform apply.\\n'\n",
      "                         'Code on main.tf to comment/delete:\\n'\n",
      "                         'Line 166, 167, 168'},\n",
      "                {'question': 'GCP - 2.2.7d Part 2 - Getting error when you run '\n",
      "                             'terraform apply',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'If you get the following error\\n'\n",
      "                         'You have to edit variables.tf on the gcp folder, set '\n",
      "                         'your project-id and region and zones properly. Then, '\n",
      "                         'run terraform apply again.\\n'\n",
      "                         'You can find correct regions/zones here: '\n",
      "                         'https://cloud.google.com/compute/docs/regions-zones\\n'\n",
      "                         'Deploying MAGE to GCP  with Terraform via the VM '\n",
      "                         '(2.2.7)\\n'\n",
      "                         'FYI - It can take up to 20 minutes to deploy the '\n",
      "                         'MAGE Terraform files if you are using a GCP Virtual '\n",
      "                         'Machine. It is normal, so don’t interrupt the '\n",
      "                         'process or think it’s taking too long. If you have, '\n",
      "                         'make sure you run a terraform destroy before trying '\n",
      "                         'again as you will have likely partially created '\n",
      "                         'resources which will cause errors next time you run '\n",
      "                         '`terraform apply`.\\n'\n",
      "                         '`terraform destroy` may not completely delete '\n",
      "                         'partial resources - go to Google Cloud Console and '\n",
      "                         'use the search bar at the top to search for the '\n",
      "                         '‘app.name’ you declared in your variables.tf file; '\n",
      "                         'this will list all resources with that name - make '\n",
      "                         'sure you delete them all before running `terraform '\n",
      "                         'apply` again.\\n'\n",
      "                         'Why are my GCP free credits going so fast? MAGE .tf '\n",
      "                         'files - Terraform Destroy not destroying all '\n",
      "                         'Resources\\n'\n",
      "                         'I checked my GCP billing last night & the MAGE '\n",
      "                         \"Terraform IaC didn't destroy a GCP Resource called \"\n",
      "                         'Filestore as ‘mage-data-prep- it has been costing '\n",
      "                         '£5.01 of my free credits each day  I now have £151 '\n",
      "                         'left - Alexey has assured me that This amount WILL '\n",
      "                         'BE SUFFICIENT funds to finish the course. Note to '\n",
      "                         'anyone who had issues deploying the MAGE terraform '\n",
      "                         \"code: check your billing account to see what you're \"\n",
      "                         'being charged for (main menu - billing) (even if '\n",
      "                         \"it's your free credits) and run a search for \"\n",
      "                         \"'mage-data-prep' in the top bar just to be sure that \"\n",
      "                         'your resources have been destroyed - if any come up '\n",
      "                         'delete them.'},\n",
      "                {'question': 'Question: Permission '\n",
      "                             \"'vpcaccess.connectors.create'\",\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': '```\\n'\n",
      "                         '│ Error: Error creating Connector: googleapi: Error '\n",
      "                         \"403: Permission 'vpcaccess.connectors.create' denied \"\n",
      "                         'on resource '\n",
      "                         \"'//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1' \"\n",
      "                         '(or it may not exist).\\n'\n",
      "                         '│ Details:\\n'\n",
      "                         '│ [\\n'\n",
      "                         '│   {\\n'\n",
      "                         '│     \"@type\": '\n",
      "                         '\"type.googleapis.com/google.rpc.ErrorInfo\",\\n'\n",
      "                         '│     \"domain\": \"vpcaccess.googleapis.com\",\\n'\n",
      "                         '│     \"metadata\": {\\n'\n",
      "                         '│       \"permission\": '\n",
      "                         '\"vpcaccess.connectors.create\",\\n'\n",
      "                         '│       \"resource\": '\n",
      "                         '\"projects/<ommit>/locations/us-west1\"\\n'\n",
      "                         '│     },\\n'\n",
      "                         '│     \"reason\": \"IAM_PERMISSION_DENIED\"\\n'\n",
      "                         '│   }\\n'\n",
      "                         '│ ]\\n'\n",
      "                         '│\\n'\n",
      "                         '│   with google_vpc_access_connector.connector,\\n'\n",
      "                         '│   on fs.tf line 19, in resource '\n",
      "                         '\"google_vpc_access_connector\" \"connector\":\\n'\n",
      "                         '│   19: resource \"google_vpc_access_connector\" '\n",
      "                         '\"connector\" {\\n'\n",
      "                         '│\\n'\n",
      "                         '```\\n'\n",
      "                         'Solution: Add Serverless VPC Access Admin to Service '\n",
      "                         'Account.\\n'\n",
      "                         'Line 148'},\n",
      "                {'question': 'File Path: Cannot save file into a non-existent '\n",
      "                             \"directory: 'data/green'\",\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Git won’t push an empty folder to GitHub, so if you '\n",
      "                         'put a file in that folder and then push, then you '\n",
      "                         'should be good to go.\\n'\n",
      "                         'Or - in your code- make the folder if it doesn’t '\n",
      "                         'exist using Pathlib as shown here: '\n",
      "                         'https://stackoverflow.com/a/273227/4590385.\\n'\n",
      "                         'For some reason, when using github storage, the '\n",
      "                         'relative path for writing locally no longer works. '\n",
      "                         'Try using two separate paths, one full path for the '\n",
      "                         'local write, and the original relative path for GCS '\n",
      "                         'bucket upload.'},\n",
      "                {'question': 'No column name lpep_pickup_datetime / '\n",
      "                             'tpep_pickup_datetime',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'The green dataset contains lpep_pickup_datetime '\n",
      "                         'while the yellow contains tpep_pickup_datetime. '\n",
      "                         'Modify the script(s) depending on  the dataset as '\n",
      "                         'required.'},\n",
      "                {'question': 'Process to download the VSC using Pandas is '\n",
      "                             'killed right away',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'pd.read_csv\\n'\n",
      "                         'df_iter = pd.read_csv(dataset_url, iterator=True, '\n",
      "                         'chunksize=100000)\\n'\n",
      "                         'The data needs to be appended to the parquet file '\n",
      "                         'using the fastparquet engine\\n'\n",
      "                         'df.to_parquet(path, compression=\"gzip\", '\n",
      "                         \"engine='fastparquet', append=True)\"},\n",
      "                {'question': 'Push to docker image failure',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'denied: requested access to the resource is denied\\n'\n",
      "                         'This can happen when you\\n'\n",
      "                         \"Haven't logged in properly to Docker Desktop (use \"\n",
      "                         'docker login -u \"myusername\")\\n'\n",
      "                         'Have used the wrong username when pushing to docker '\n",
      "                         'images. Use the same one as your username and as the '\n",
      "                         'one you build on\\n'\n",
      "                         'docker image build -t '\n",
      "                         '<myusername>/<imagename>:<tag>\\n'\n",
      "                         'docker image push <myusername>/<imagename>:<tag>'},\n",
      "                {'question': 'Flow script fails with “killed” message:',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': \"16:21:35.607 | INFO    | Flow run 'singing-malkoha' \"\n",
      "                         \"- Executing 'write_bq-b366772c-0' immediately...\\n\"\n",
      "                         'Killed\\n'\n",
      "                         'Solution:  You probably are running out of memory on '\n",
      "                         'your VM and need to add more.  For example, if you '\n",
      "                         'have 8 gigs of RAM on your VM, you may want to '\n",
      "                         'expand that to 16 gigs.'},\n",
      "                {'question': 'GCP VM: Disk Space is full',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'After playing around with prefect for a while this '\n",
      "                         'can happen.\\n'\n",
      "                         'Ssh to your VM and run sudo du -h --block-size=G | '\n",
      "                         'sort -n -r | head -n 30 to see which directory needs '\n",
      "                         'the most space.\\n'\n",
      "                         'Most likely it will be …/.prefect/storage, where '\n",
      "                         'your cached flows are stored. You can delete older '\n",
      "                         'flows from there. You also have to delete the '\n",
      "                         'corresponding flow in the UI, otherwise it will '\n",
      "                         'throw you an error, when you try to run your next '\n",
      "                         'flow.\\n'\n",
      "                         'SSL Certificate Verify: (I got it when trying to run '\n",
      "                         'flows on MAC): urllib.error.URLError: <urlopen error '\n",
      "                         '[SSL: CERTIFICATE_VERIFY_FAILED]\\n'\n",
      "                         'pip install certifi\\n'\n",
      "                         '/Applications/Python\\\\ {ver}/Install\\\\ '\n",
      "                         'Certificates.command\\n'\n",
      "                         'or\\n'\n",
      "                         'running the “Install Certificate.command” inside of '\n",
      "                         'the python{ver} folder'},\n",
      "                {'question': 'Docker: container crashed with status code 137.',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'It means your container consumed all available RAM '\n",
      "                         'allocated to it. It can happen in particular when '\n",
      "                         'working on Question#3 in the homework as the dataset '\n",
      "                         'is relatively large and containers eat a lot of '\n",
      "                         'memory in general.\\n'\n",
      "                         'I would recommend restarting your computer and only '\n",
      "                         'starting the necessary processes to run the '\n",
      "                         'container. If that doesn’t work, allocate more '\n",
      "                         'resources to docker. If also that doesn’t work '\n",
      "                         'because your workstation is a potato, you can use an '\n",
      "                         'online compute environment service like GitPod, '\n",
      "                         'which is free under under 50 hours / month of use.'},\n",
      "                {'question': 'Timeout due to slow upload internet',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'In Q3 there was a task to run the etl script from '\n",
      "                         'web to GCS. The problem was, it wasn’t really an ETL '\n",
      "                         'straight from web to GCS, but it was actually a web '\n",
      "                         'to local storage to local memory to GCS over network '\n",
      "                         'ETL. Yellow data is about 100 MB each per month '\n",
      "                         'compressed and ~700 MB after uncompressed on memory\\n'\n",
      "                         'This leads to a problem where i either got a network '\n",
      "                         'type error because my not so good 3rd world internet '\n",
      "                         'or i got my WSL2 crashed/hanged because out of '\n",
      "                         'memory error and/or 100% resource usage hang.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'if you have a lot of time at hand, try compressing '\n",
      "                         'it to parquet and writing it to GCS with the timeout '\n",
      "                         'argument set to a really high number (the default os '\n",
      "                         '60 seconds)\\n'\n",
      "                         'the yellow taxi data for feb 2019 is about 100MB as '\n",
      "                         'parquet file\\n'\n",
      "                         'gcp_cloud_storage_bucket_block.upload_from_path(\\n'\n",
      "                         'from_path=f\"{path}\",\\n'\n",
      "                         'to_path=path,\\n'\n",
      "                         'timeout=600\\n'\n",
      "                         ')'},\n",
      "                {'question': 'UndefinedColumn: column \"ratecode_id\", '\n",
      "                             '\"rate_code_id\" “vendor_id”, “pu_location_id”, '\n",
      "                             '“do_location_id” of relation \"green_taxi\" does '\n",
      "                             'not exist - Export transformed green_taxi data '\n",
      "                             'to PostgreSQL',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'This error occurs when you try to re-run the export '\n",
      "                         'block, of the transformed green_taxi data to '\n",
      "                         'PostgreSQL.\\n'\n",
      "                         'What you’ll need to do is to drop the table using '\n",
      "                         'SQL in Mage (screenshot below).\\n'\n",
      "                         'You should be able to re-run the block successfully '\n",
      "                         'after dropping the table.'},\n",
      "                {'question': 'Homework - Q3 SettingWithCopyWarning Error:',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'SettingWithCopyWarning:\\n'\n",
      "                         'A value is trying to be set on a copy of a slice '\n",
      "                         'from a DataFrame.\\n'\n",
      "                         'Use the data.loc[] = value syntax instead of df[] = '\n",
      "                         'value to ensure that the new column is being '\n",
      "                         'assigned to the original dataframe instead of a copy '\n",
      "                         'of a dataframe or a series.'},\n",
      "                {'question': 'Since I was using slow laptop, and we have so '\n",
      "                             'big csv files, I used pyspark kernel in mage '\n",
      "                             'instead of python, How to do it?',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'CSV Files are very big in nyc data, so we instead of '\n",
      "                         'using Pandas/Python kernel , we can try Pyspark '\n",
      "                         'Kernel\\n'\n",
      "                         'Documentation of Mage for using pyspark kernel: '\n",
      "                         'https://docs.mage.ai/integrations/spark-pyspark\\n'\n",
      "                         '?'},\n",
      "                {'question': 'I got an error when I was deleting  BLOCK IN A '\n",
      "                             'PIPELINE',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'So we will first delete the connection between '\n",
      "                         'blocks then we can remove the connection.'},\n",
      "                {'question': 'Mage UI won’t let you edit the Pipeline name?',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'While Editing the Pipeline Name It throws permission '\n",
      "                         'denied error.\\n'\n",
      "                         '(Work around)In that case proceed with the work and '\n",
      "                         'save later on revisit it will let you edit.'},\n",
      "                {'question': 'How do I make Mage load the partitioned files '\n",
      "                             'that we created on 2.2.4, to load them into '\n",
      "                             'BigQuery ?',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Solution n°1 if you want to download everything :\\n'\n",
      "                         '```\\n'\n",
      "                         'import pyarrow as pa\\n'\n",
      "                         'import pyarrow.parquet as pq\\n'\n",
      "                         'from pyarrow.fs import GcsFileSystem\\n'\n",
      "                         '…\\n'\n",
      "                         '@data_loader\\n'\n",
      "                         'def load_data(*args, **kwargs):\\n'\n",
      "                         \"    bucket_name = YOUR_BUCKET_NAME_HERE'\\n\"\n",
      "                         \"    blob_prefix = 'PATH / TO / WHERE / THE / \"\n",
      "                         \"PARTITIONS / ARE'\\n\"\n",
      "                         '    root_path = f\"{bucket_name}/{blob_prefix}\"\\n'\n",
      "                         'pa_table = pq.read_table(\\n'\n",
      "                         '        source=root_path,\\n'\n",
      "                         '        filesystem=GcsFileSystem(),        \\n'\n",
      "                         '    )\\n'\n",
      "                         '\\n'\n",
      "                         '    return pa_table.to_pandas()\\n'\n",
      "                         'Solution n°2 if you want to download only some dates '\n",
      "                         ':\\n'\n",
      "                         '@data_loader\\n'\n",
      "                         'def load_data(*args, **kwargs):\\n'\n",
      "                         'gcs = pa.fs.GcsFileSystem()\\n'\n",
      "                         \"bucket_name = 'YOUR_BUCKET_NAME_HERE'\\n\"\n",
      "                         \"blob_prefix = ''PATH / TO / WHERE / THE / PARTITIONS \"\n",
      "                         \"/ ARE''\\n\"\n",
      "                         'root_path = f\"{bucket_name}/{blob_prefix}\"\\n'\n",
      "                         'pa_dataset = pq.ParquetDataset(\\n'\n",
      "                         'path_or_paths=root_path,\\n'\n",
      "                         'filesystem=gcs,\\n'\n",
      "                         \"filters=[('lpep_pickup_date', '>=', '2020-10-01'), \"\n",
      "                         \"('lpep_pickup_date', '<=', '2020-10-31')]\\n\"\n",
      "                         ')\\n'\n",
      "                         'return pa_dataset.read().to_pandas()\\n'\n",
      "                         '# More information about the pq.Parquet.Dataset : '\n",
      "                         'Encapsulates details of reading a complete Parquet '\n",
      "                         'dataset possibly consisting of multiple files and '\n",
      "                         'partitions in subdirectories. Documentation here :\\n'\n",
      "                         'https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset\\n'\n",
      "                         'ERROR: UndefinedColumn: column \"vendor_id\" of '\n",
      "                         'relation \"green_taxi\" does not exist\\n'\n",
      "                         'Two possible solutions both of them work in the same '\n",
      "                         'way.\\n'\n",
      "                         'Open up a Data Loader connect using SQL - RUN the '\n",
      "                         'command \\n'\n",
      "                         '`DROP TABLE mage.green_taxi`\\n'\n",
      "                         'Else, Open up a Data Extractor of SQL  - increase '\n",
      "                         'the rows to above the number of rows in the '\n",
      "                         'dataframe (you can find that in the bottom of the '\n",
      "                         'transformer block) change the Write Policy to '\n",
      "                         '`Replace` and run the SELECT statement'},\n",
      "                {'question': 'Git - What Files Should I Submit for Homework 2 '\n",
      "                             '& How do I get them out of MAGE:',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'All mage files are in your /home/src/folder where '\n",
      "                         'you saved your credentials.json so you should be '\n",
      "                         'able to access them locally. You will see a folder '\n",
      "                         \"for ‘Pipelines’,  'data loaders', 'data \"\n",
      "                         \"transformers' & 'data exporters' - inside these will \"\n",
      "                         'be the .py or .sql files for the blocks you created '\n",
      "                         'in your pipeline.\\n'\n",
      "                         'Right click & ‘download’ the pipeline itself to your '\n",
      "                         'local machine (which gives you metadata, pycache and '\n",
      "                         'other files)\\n'\n",
      "                         'As above, download each .py/.sql file that '\n",
      "                         'corresponds to each block you created for the '\n",
      "                         \"pipeline. You'll find these under 'data loaders', \"\n",
      "                         \"'data transformers' 'data exporters'\\n\"\n",
      "                         'Move the downloaded files to your GitHub repo folder '\n",
      "                         '& commit your changes.'},\n",
      "                {'question': 'Git - How do I include the files in the Mage '\n",
      "                             'repo (including exercise files and homework) in '\n",
      "                             'a personal copy of the Data Engineering Zoomcamp '\n",
      "                             'repo?',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Assuming you downloaded the Mage repo in the week 2 '\n",
      "                         'folder of the Data Engineering Zoomcamp, you might '\n",
      "                         'want to include your mage copy, demo pipelines and '\n",
      "                         'homework within your personal copy of the Data '\n",
      "                         'Engineering Zoomcamp repo. This will not work by '\n",
      "                         'default, because GitHub sees them as two separate '\n",
      "                         'repositories, and one does not track the other. To '\n",
      "                         'add the Mage files to your main DE Zoomcamp repo, '\n",
      "                         'you will need to:\\n'\n",
      "                         'Move the contents of the .gitignore file in your '\n",
      "                         'main .gitignore.\\n'\n",
      "                         'Use the terminal to cd into the Mage folder and:\\n'\n",
      "                         'run “git remote remove origin” to de-couple the Mage '\n",
      "                         'repo,\\n'\n",
      "                         'run “rm -rf .git” to delete local git files,\\n'\n",
      "                         'run “git add .” to add the current folder as changes '\n",
      "                         'to stage, commit and push.'},\n",
      "                {'question': 'Got ValueError: The truth value of a Series is '\n",
      "                             'ambiguous. Use a.empty, a.bool(), a.item(), '\n",
      "                             'a.any() or a.all()',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'When try to add three assertions:\\n'\n",
      "                         'vendor_id is one of the existing values in the '\n",
      "                         'column (currently)\\n'\n",
      "                         'passenger_count is greater than 0\\n'\n",
      "                         'trip_distance is greater than 0\\n'\n",
      "                         'to test_output, I got ValueError: The truth value of '\n",
      "                         'a Series is ambiguous. Use a.empty, a.bool(), '\n",
      "                         'a.item(), a.any() or a.all(). Below is my code:\\n'\n",
      "                         \"data_filter = (data['passenger_count'] > 0) and \"\n",
      "                         \"(data['trip_distance'] > 0)\\n\"\n",
      "                         'After looking for solutions at Stackoverflow, I '\n",
      "                         'found great discussion about it. So I changed my '\n",
      "                         'code into:\\n'\n",
      "                         \"data_filter = (data['passenger_count'] > 0) & \"\n",
      "                         \"(data['trip_distance'] > 0)\"},\n",
      "                {'question': 'Mage AI Files are Gone/disappearing',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'This happened when I just booted up my PC, '\n",
      "                         'continuing from the progress I was doing from '\n",
      "                         'yesterday.\\n'\n",
      "                         'After cd-ing into your directory, and running docker '\n",
      "                         'compose up , the web interface for the Mage shows, '\n",
      "                         'but the files that I had yesterday was gone.\\n'\n",
      "                         'If your files are gone, go ahead and close the web '\n",
      "                         'interface, and properly shutting down the mage '\n",
      "                         'docker compose by doing Ctrl + C once. Try running '\n",
      "                         'it again. This worked for me more than once (yes the '\n",
      "                         'issue persisted with my PC twice)\\n'\n",
      "                         'Also, you should check if you’re in the correct '\n",
      "                         'repository before doing docker compose up . This was '\n",
      "                         'discussed in the Slack #course-data-engineering '\n",
      "                         'channel'},\n",
      "                {'question': 'Mage - Errors in io.config.yaml file',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'The above errors due to “ at the trailing side and '\n",
      "                         'it need to be modified with ‘ quotes at both ends\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'Mage - ArrowException Cannot open credentials '\n",
      "                             'file',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Problem: The following error occurs when attempting '\n",
      "                         'to export data from Mage to a GCS bucket using '\n",
      "                         'pyarrow suggesting Mage doesn’t have the necessary '\n",
      "                         'permissions to access the specified GCP credentials '\n",
      "                         '.json file.\\n'\n",
      "                         'ArrowException: Unknown error: '\n",
      "                         'google::cloud::Status(UNKNOWN: Permanent error '\n",
      "                         'GetBucketMetadata: Could not create a OAuth2 access '\n",
      "                         'token to authenticate the request. The request was '\n",
      "                         'not sent, as such an access token is required to '\n",
      "                         'complete the request successfully. Learn more about '\n",
      "                         'Google Cloud authentication at '\n",
      "                         'https://cloud.google.com/docs/authentication. The '\n",
      "                         'underlying error message was: Cannot open '\n",
      "                         'credentials file /home/src/...\\n'\n",
      "                         'Solution: Inside the Mage app:\\n'\n",
      "                         'Create a credentials folder (e.g. gcp-creds) within '\n",
      "                         'the magic-zoomcamp folder\\n'\n",
      "                         'In the credentials folder create a .json key file '\n",
      "                         '(e.g. mage-gcp-creds.json)\\n'\n",
      "                         'Copy/paste GCP service account credentials into the '\n",
      "                         '.json key file and save\\n'\n",
      "                         'Update code to point to this file. E.g.\\n'\n",
      "                         \"environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"\n",
      "                         \"'/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'\"},\n",
      "                {'question': 'Mage - OSError',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Oserror: google::cloud::status(unavailable: retry '\n",
      "                         'policy exhausted getbucketmetadata: could not create '\n",
      "                         'a OAuth2 access token to authenticate the request. '\n",
      "                         'the request was not sent, as such an access token is '\n",
      "                         'required to complete the request successfully. learn '\n",
      "                         'more about google cloud authentication at '\n",
      "                         'https://cloud.google.com/docs/authentication. the '\n",
      "                         'underlying error message was: performwork() - curl '\n",
      "                         \"error [6]=couldn't resolve host name)\"},\n",
      "                {'question': 'Mage - PermissionError service account does not '\n",
      "                             'have storage.buckets.get access to the Google '\n",
      "                             'Cloud Storage bucket',\n",
      "                 'section': 'Module 2: Workflow Orchestration',\n",
      "                 'text': 'Problem: The following error occurs when attempting '\n",
      "                         'to export data from Mage to a GCS bucket. Assigned '\n",
      "                         'service account doesn’t have the necessary '\n",
      "                         'permissions access Google Cloud Storage Bucket\\n'\n",
      "                         'PermissionError: [Errno 13] '\n",
      "                         'google::cloud::Status(PERMISSION_DENIED: Permanent '\n",
      "                         'error GetBucketMetadata:... .iam.gserviceaccount.com '\n",
      "                         'does not have storage.buckets.get access to the '\n",
      "                         'Google Cloud Storage bucket. Permission '\n",
      "                         \"'storage.buckets.get' denied on resource (or it may \"\n",
      "                         'not exist). error_info={reason=forbidden, '\n",
      "                         'domain=global, metadata={http_status_code=403}}). '\n",
      "                         'Detail: [errno 13] Permission denied\\n'\n",
      "                         'Solution: Add Cloud Storage Admin role to the '\n",
      "                         'service account:\\n'\n",
      "                         'Go to project in Google Cloud Console>IAM & '\n",
      "                         'Admin>IAM\\n'\n",
      "                         'Click Edit principal (pencil symbol) to the right of '\n",
      "                         'the service account you are using\\n'\n",
      "                         'Click + ADD ANOTHER ROLE\\n'\n",
      "                         'Select Cloud Storage>Storage Admin\\n'\n",
      "                         'Click Save'},\n",
      "                {'question': 'Trigger Dataproc from Mage',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': '1. Make sure your pyspark script is ready to be send '\n",
      "                         'to Dataproc cluster\\n'\n",
      "                         '2. Create a Dataproc Cluster in GCP Console\\n'\n",
      "                         '3. Make sure to edit the service account and add new '\n",
      "                         'role - Dataproc Editor\\n'\n",
      "                         '4. Copy the python script '\n",
      "                         './notebooks/pyspark_script.py and place it under GCS '\n",
      "                         'bucket path\\n'\n",
      "                         '5. Make sure gcloud cli is installed either in Mage '\n",
      "                         'manually or  via your Dockerfile and docker-compose '\n",
      "                         'files. This is needed to let Mage access google '\n",
      "                         'Dataproc and the script it needs to execute. Refer - '\n",
      "                         'Installing the latest gcloud CLI\\n'\n",
      "                         '6. Use the Bigquery/Dataproc script mentioned here - '\n",
      "                         'https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md '\n",
      "                         '. Use Mage to trigger the query'},\n",
      "                {'question': 'Docker-compose takes infinitely long to install '\n",
      "                             'zip unzip packages for linux, which are required '\n",
      "                             'to unpack datasets',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'A:\\n'\n",
      "                         '1 solution) Add -Y flag, so that apt-get '\n",
      "                         'automatically agrees to install additional packages\\n'\n",
      "                         '2) Use python ZipFile package, which is included in '\n",
      "                         'all modern python distributions'},\n",
      "                {'question': 'GCS Bucket - error when writing data from web to '\n",
      "                             'GCS:',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Make sure to use Nullable dataTypes, such as Int64 '\n",
      "                         'when appliable.'},\n",
      "                {'question': 'GCS Bucket - Failed to create table: Error while '\n",
      "                             'reading data, error message: Parquet column '\n",
      "                             \"'XYZ' has type INT which does not match the \"\n",
      "                             'target cpp_type DOUBLE. File: '\n",
      "                             'gs://path/to/some/blob.parquet',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Ultimately, when trying to ingest data into a '\n",
      "                         'BigQuery table, all files within a given directory '\n",
      "                         'must have the same schema.\\n'\n",
      "                         'When dealing for example with the FHV Datasets from '\n",
      "                         '2019, however (see image below), one can see that '\n",
      "                         \"the files for '2019-05', and 2019-06, have the \"\n",
      "                         'columns \"PUlocationID\" and \"DOlocationID\" as '\n",
      "                         \"Integers, while for the period of '2019-01' through \"\n",
      "                         \"'2019-04', the same column is defined as FLOAT.\\n\"\n",
      "                         'So while importing these files as parquet to '\n",
      "                         'BigQuery, the first one will be used to define the '\n",
      "                         'schema of the table, while all files following that '\n",
      "                         'will be used to append data on the existing table. '\n",
      "                         'Which means, they must all follow the very same '\n",
      "                         'schema of the file that created the table.\\n'\n",
      "                         'So, in order to prevent errors like that, make sure '\n",
      "                         'to enforce the data types for the columns on the '\n",
      "                         'DataFrame before you serialize/upload them to '\n",
      "                         'BigQuery. Like this:\\n'\n",
      "                         'pd.read_csv(\"path_or_url\").astype({\\n'\n",
      "                         '\\t\"col1_name\": \"datatype\",\\t\\n'\n",
      "                         '\\t\"col2_name\": \"datatype\",\\t\\n'\n",
      "                         '\\t...\\t\\t\\t\\t\\t\\n'\n",
      "                         '\\t\"colN_name\": \"datatype\" \\t\\n'\n",
      "                         '})'},\n",
      "                {'question': 'GCS Bucket - Fix Error when importing FHV data '\n",
      "                             'to GCS',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'If you receive the error gzip.BadGzipFile: Not a '\n",
      "                         \"gzipped file (b'\\\\n\\\\n'), this is because you have \"\n",
      "                         'specified the wrong URL to the FHV dataset. Make '\n",
      "                         'sure to use '\n",
      "                         'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\\n'\n",
      "                         'Emphasising the ‘/releases/download’ part of the '\n",
      "                         'URL.'},\n",
      "                {'question': 'GCS Bucket - Load Data From URL list in to GCP '\n",
      "                             'Bucket',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Krishna Anand'},\n",
      "                {'question': 'GCS Bucket - I query my dataset and get a Bad '\n",
      "                             'character (ASCII 0) error?',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Check the Schema\\n'\n",
      "                         'You might have a wrong formatting\\n'\n",
      "                         'Try to upload the CSV.GZ files without formatting or '\n",
      "                         'going through pandas via wget\\n'\n",
      "                         'See this Slack conversation for helpful tips'},\n",
      "                {'question': 'GCP BQ - “bq: command not found”',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Run the following command to check if “BigQuery '\n",
      "                         'Command Line Tool” is installed or not: gcloud '\n",
      "                         'components list\\n'\n",
      "                         'You can also use bq.cmd instead of bq to make it '\n",
      "                         'work.'},\n",
      "                {'question': 'GCP BQ - Caution in using bigquery:no',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Use big queries carefully,\\n'\n",
      "                         'I created by bigquery dataset on an account where my '\n",
      "                         'free trial was exhausted, and got a bill of $80.\\n'\n",
      "                         'Use big query in free credits and destroy all the '\n",
      "                         'datasets after creation.\\n'\n",
      "                         'Check your Billing daily! Especially if you’ve '\n",
      "                         'spinned up a VM.'},\n",
      "                {'question': 'GCP BQ - Cannot read and write in different '\n",
      "                             'locations: source: EU, destination: US - Loading '\n",
      "                             'data from GCS into BigQuery (different Region):',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Be careful when you create your resources on GCP, '\n",
      "                         'all of them have to share the same Region in order '\n",
      "                         'to allow load data from GCS Bucket to BigQuery. If '\n",
      "                         'you forgot it when you created them, you can create '\n",
      "                         'a new dataset on BigQuery using the same Region '\n",
      "                         'which you used on your GCS Bucket.\\n'\n",
      "                         'This means that your GCS Bucket and the BigQuery '\n",
      "                         'dataset are placed in different regions. You have to '\n",
      "                         'create a new dataset inside BigQuery in the same '\n",
      "                         'region with your GCS bucket and store the data in '\n",
      "                         'the newly created dataset.'},\n",
      "                {'question': 'GCP BQ - Cannot read and write in different '\n",
      "                             'locations: source: <REGION_HERE>, destination: '\n",
      "                             '<ANOTHER_REGION_HERE>',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Make sure to create the BigQuery dataset in the very '\n",
      "                         \"same location that you've created the GCS Bucket. \"\n",
      "                         'For instance, if your GCS Bucket was created in '\n",
      "                         '`us-central1`, then BigQuery dataset must be created '\n",
      "                         'in the same region (us-central1, in this example)'},\n",
      "                {'question': 'GCP BQ - Remember to save your queries',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'By the way, this isn’t a problem/solution, but a '\n",
      "                         'useful hint:\\n'\n",
      "                         'Please, remember to save your progress in BigQuery '\n",
      "                         'SQL Editor.\\n'\n",
      "                         'I was almost finishing the homework, when my Chrome '\n",
      "                         'Tab froze and I had to reload it. Then I lost my '\n",
      "                         'entire SQL script.\\n'\n",
      "                         'Save your script from time to time. Just click on '\n",
      "                         'the button at the top bar. Your saved file will be '\n",
      "                         'available on the left panel.\\n'\n",
      "                         'Alternatively, you can copy paste your queries into '\n",
      "                         'an .sql file in your preferred editor (Notepad++, VS '\n",
      "                         'Code, etc.). Using the .sql extension will provide '\n",
      "                         'convenient color formatting.'},\n",
      "                {'question': 'GCP BQ - Can I use BigQuery for real-time '\n",
      "                             'analytics in this project?',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Ans :  While real-time analytics might not be '\n",
      "                         'explicitly mentioned, BigQuery has real-time data '\n",
      "                         'streaming capabilities, allowing for potential '\n",
      "                         'integration in future project iterations.'},\n",
      "                {'question': 'GCP BQ - Unable to load data from external '\n",
      "                             'tables into a materialized table in BigQuery due '\n",
      "                             'to an invalid timestamp error that are added '\n",
      "                             'while appending data to the file in Google Cloud '\n",
      "                             'Storage',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': \"could not parse 'pickup_datetime' as timestamp for \"\n",
      "                         'field pickup_datetime (position 2)\\n'\n",
      "                         'This error is caused by invalid data in the '\n",
      "                         'timestamp column. A way to identify the problem is '\n",
      "                         'to define the schema from the external table using '\n",
      "                         'string datatype. This enables the queries to work at '\n",
      "                         'which point we can filter out the invalid rows from '\n",
      "                         'the import to the materialised table and insert the '\n",
      "                         'fields with the timestamp data type.'},\n",
      "                {'question': 'GCP BQ - Error Message in BigQuery: annotated as '\n",
      "                             'a valid Timestamp, please annotate it as '\n",
      "                             'TimestampType(MICROS) or TimestampType(MILLIS)',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Background:\\n'\n",
      "                         '`pd.read_parquet`\\n'\n",
      "                         '`pd.to_datetime`\\n'\n",
      "                         '`pq.write_to_dataset`\\n'\n",
      "                         'Reference:\\n'\n",
      "                         'https://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\\n'\n",
      "                         'https://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\\n'\n",
      "                         'https://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Add `use_deprecated_int96_timestamps=True` to '\n",
      "                         '`pq.write_to_dataset` function, like below\\n'\n",
      "                         'pq.write_to_dataset(\\n'\n",
      "                         'table,\\n'\n",
      "                         'root_path=root_path,\\n'\n",
      "                         'filesystem=gcs,\\n'\n",
      "                         'use_deprecated_int96_timestamps=True\\n'\n",
      "                         '# Write timestamps to INT96 Parquet format\\n'\n",
      "                         ')'},\n",
      "                {'question': 'GCP BQ - Datetime columns in Parquet files '\n",
      "                             'created from Pandas show up as integer columns '\n",
      "                             'in BigQuery',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Solution:\\n'\n",
      "                         'If you’re using Mage, in the last Data Exporter that '\n",
      "                         'writes to Google Cloud Storage use PyArrow to '\n",
      "                         'generate the Parquet file with the correct logical '\n",
      "                         \"type for the datetime columns, otherwise they won't \"\n",
      "                         'be converted to timestamp when loaded by BigQuery '\n",
      "                         'later on.\\n'\n",
      "                         'import pyarrow as pa\\n'\n",
      "                         'import pyarrow.parquet as pq\\n'\n",
      "                         'import os\\n'\n",
      "                         \"if 'data_exporter' not in globals():\\n\"\n",
      "                         'from mage_ai.data_preparation.decorators import '\n",
      "                         'data_exporter\\n'\n",
      "                         '# Replace with the location of your service account '\n",
      "                         'key JSON file.\\n'\n",
      "                         \"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"\n",
      "                         \"'/home/src/personal-gcp.json'\\n\"\n",
      "                         'bucket_name = \"<YOUR_BUCKET_NAME>\"\\n'\n",
      "                         \"object_key = 'nyc_taxi_data_2022.parquet'\\n\"\n",
      "                         \"where = f'{bucket_name}/{object_key}'\\n\"\n",
      "                         '@data_exporter\\n'\n",
      "                         'def export_data(data, *args, **kwargs):\\n'\n",
      "                         'table = pa.Table.from_pandas(data, '\n",
      "                         'preserve_index=False)\\n'\n",
      "                         'gcs = pa.fs.GcsFileSystem()\\n'\n",
      "                         'pq.write_table(\\n'\n",
      "                         'table,\\n'\n",
      "                         'where,\\n'\n",
      "                         '# Convert integer columns in Epoch milliseconds\\n'\n",
      "                         \"# to Timestamp columns in microseconds ('us') so\\n\"\n",
      "                         '# they can be loaded into BigQuery with the right\\n'\n",
      "                         '# data type\\n'\n",
      "                         \"coerce_timestamps='us',\\n\"\n",
      "                         'filesystem=gcs\\n'\n",
      "                         ')\\n'\n",
      "                         'Solution 2:\\n'\n",
      "                         'If you’re using Mage, in the last Data Exporter that '\n",
      "                         'writes to Google Cloud Storage, provide PyArrow with '\n",
      "                         'explicit schema to generate the Parquet file with '\n",
      "                         'the correct logical type for the datetime columns, '\n",
      "                         \"otherwise they won't be converted to timestamp when \"\n",
      "                         'loaded by BigQuery later on.\\n'\n",
      "                         'schema = pa.schema([\\n'\n",
      "                         \"('vendor_id', pa.int64()),\\n\"\n",
      "                         \"('lpep_pickup_datetime', pa.timestamp('ns')),\\n\"\n",
      "                         \"('lpep_dropoff_datetime', pa.timestamp('ns')),\\n\"\n",
      "                         \"('store_and_fwd_flag', pa.string()),\\n\"\n",
      "                         \"('ratecode_id', pa.int64()),\\n\"\n",
      "                         \"('pu_location_id', pa.int64()),\\n\"\n",
      "                         \"('do_location_id', pa.int64()),\\n\"\n",
      "                         \"('passenger_count', pa.int64()),\\n\"\n",
      "                         \"('trip_distance', pa.float64()),\\n\"\n",
      "                         \"('fare_amount', pa.float64()),\\n\"\n",
      "                         \"('extra', pa.float64()),\\n\"\n",
      "                         \"('mta_tax', pa.float64()),\\n\"\n",
      "                         \"('tip_amount', pa.float64()),\\n\"\n",
      "                         \"('tolls_amount', pa.float64()),\\n\"\n",
      "                         \"('improvement_surcharge', pa.float64()),\\n\"\n",
      "                         \"('total_amount', pa.float64()),\\n\"\n",
      "                         \"('payment_type', pa.int64()),\\n\"\n",
      "                         \"('trip_type', pa.int64()),\\n\"\n",
      "                         \"('congestion_surcharge', pa.float64()),\\n\"\n",
      "                         \"('lpep_pickup_month', pa.int64())\\n\"\n",
      "                         '])\\n'\n",
      "                         'table = pa.Table.from_pandas(data, schema=schema)'},\n",
      "                {'question': 'GCP BQ - Create External Table using Python',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Reference:\\n'\n",
      "                         'https://cloud.google.com/bigquery/docs/external-data-cloud-storage\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'from google.cloud import bigquery\\n'\n",
      "                         '# Set table_id to the ID of the table to create\\n'\n",
      "                         'table_id = '\n",
      "                         'f\"{project_id}.{dataset_name}.{table_name}\"\\n'\n",
      "                         '# Construct a BigQuery client object\\n'\n",
      "                         'client = bigquery.Client()\\n'\n",
      "                         '# Set the external source format of your table\\n'\n",
      "                         'external_source_format = \"PARQUET\"\\n'\n",
      "                         '# Set the source_uris to point to your data in '\n",
      "                         'Google Cloud\\n'\n",
      "                         'source_uris = [ '\n",
      "                         \"f'gs://{bucket_name}/{object_key}/*']\\n\"\n",
      "                         '# Create ExternalConfig object with external source '\n",
      "                         'format\\n'\n",
      "                         'external_config = '\n",
      "                         'bigquery.ExternalConfig(external_source_format)\\n'\n",
      "                         '# Set source_uris that point to your data in Google '\n",
      "                         'Cloud\\n'\n",
      "                         'external_config.source_uris = source_uris\\n'\n",
      "                         'external_config.autodetect = True\\n'\n",
      "                         'table = bigquery.Table(table_id)\\n'\n",
      "                         '# Set the external data configuration of the table\\n'\n",
      "                         'table.external_data_configuration = external_config\\n'\n",
      "                         'table = client.create_table(table)  # Make an API '\n",
      "                         'request.\\n'\n",
      "                         \"print(f'Created table with external source: \"\n",
      "                         \"{table_id}')\\n\"\n",
      "                         \"print(f'Format: \"\n",
      "                         \"{table.external_data_configuration.source_format}')\"},\n",
      "                {'question': 'GCP BQ - Check BigQuery Table Exist And Delete',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Reference:\\n'\n",
      "                         'https://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Combine with “Create External Table using Python”, '\n",
      "                         'use it before “client.create_table” function.\\n'\n",
      "                         'def tableExists(tableID, client):\\n'\n",
      "                         '\"\"\"\\n'\n",
      "                         'Check if a table already exists using the tableID.\\n'\n",
      "                         'return : (Boolean)\\n'\n",
      "                         '\"\"\"\\n'\n",
      "                         'try:\\n'\n",
      "                         'table = client.get_table(tableID)\\n'\n",
      "                         'return True\\n'\n",
      "                         'except Exception as e: # NotFound:\\n'\n",
      "                         'return False'},\n",
      "                {'question': 'GCP BQ - Error: Missing close double quote (\") '\n",
      "                             'character',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'To avoid this error you can upload data from Google '\n",
      "                         'Cloud Storage to BigQuery through BigQuery Cloud '\n",
      "                         'Shell using the command:\\n'\n",
      "                         '$ bq load  --autodetect --allow_quoted_newlines '\n",
      "                         '--source_format=CSV dataset_name.table_name '\n",
      "                         '\"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"'},\n",
      "                {'question': 'GCP BQ - Cannot read and write in different '\n",
      "                             'locations: source: asia-south2, destination: US',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'Solution: This problem arises if your gcs and '\n",
      "                         'bigquery storage is in different regions.\\n'\n",
      "                         'One potential way to solve it:\\n'\n",
      "                         'Go to your google cloud bucket and check the region '\n",
      "                         'in field named “Location”\\n'\n",
      "                         'Now in bigquery, click on three dot icon near your '\n",
      "                         'project name and select create dataset.\\n'\n",
      "                         'In region filed choose the same regions as you saw '\n",
      "                         'in your google cloud bucket'},\n",
      "                {'question': 'GCP BQ - Tip: Using Cloud Function to read '\n",
      "                             'csv.gz files from github directly to BigQuery in '\n",
      "                             'Google Cloud:',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'There are multiple benefits of using Cloud Functions '\n",
      "                         'to automate tasks in Google Cloud.\\n'\n",
      "                         'Use below Cloud Function python script to load files '\n",
      "                         'directly to BigQuery. Use your project id, dataset '\n",
      "                         'id & table id as defined by you.\\n'\n",
      "                         'import tempfile\\n'\n",
      "                         'import requests\\n'\n",
      "                         'import logging\\n'\n",
      "                         'from google.cloud import bigquery\\n'\n",
      "                         'def hello_world(request):\\n'\n",
      "                         '# table_id = <project_id.dataset_id.table_id>\\n'\n",
      "                         \"table_id = 'de-zoomcap-project.dezoomcamp.fhv-2019'\\n\"\n",
      "                         '# Create a new BigQuery client\\n'\n",
      "                         'client = bigquery.Client()\\n'\n",
      "                         'for month in range(4, 13):\\n'\n",
      "                         '# Define the schema for the data in the CSV.gz '\n",
      "                         'files\\n'\n",
      "                         'url = '\n",
      "                         \"'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz'.format(month)\\n\"\n",
      "                         '# Download the CSV.gz file from Github\\n'\n",
      "                         'response = requests.get(url)\\n'\n",
      "                         '# Create new table if loading first month data else '\n",
      "                         'append\\n'\n",
      "                         'write_disposition_string = \"WRITE_APPEND\" if month > '\n",
      "                         '1 else \"WRITE_TRUNCATE\"\\n'\n",
      "                         '# Defining LoadJobConfig with schema of table to '\n",
      "                         'prevent it from changing with every table\\n'\n",
      "                         'job_config = bigquery.LoadJobConfig(\\n'\n",
      "                         'schema=[\\n'\n",
      "                         'bigquery.SchemaField(\"dispatching_base_num\", '\n",
      "                         '\"STRING\"),\\n'\n",
      "                         'bigquery.SchemaField(\"pickup_datetime\", '\n",
      "                         '\"TIMESTAMP\"),\\n'\n",
      "                         'bigquery.SchemaField(\"dropOff_datetime\", '\n",
      "                         '\"TIMESTAMP\"),\\n'\n",
      "                         'bigquery.SchemaField(\"PUlocationID\", \"STRING\"),\\n'\n",
      "                         'bigquery.SchemaField(\"DOlocationID\", \"STRING\"),\\n'\n",
      "                         'bigquery.SchemaField(\"SR_Flag\", \"STRING\"),\\n'\n",
      "                         'bigquery.SchemaField(\"Affiliated_base_number\", '\n",
      "                         '\"STRING\"),\\n'\n",
      "                         '],\\n'\n",
      "                         'skip_leading_rows=1,\\n'\n",
      "                         'write_disposition=write_disposition_string,\\n'\n",
      "                         'autodetect=True,\\n'\n",
      "                         'source_format=\"CSV\",\\n'\n",
      "                         ')\\n'\n",
      "                         '# Load the data into BigQuery\\n'\n",
      "                         '# Create a temporary file to prevent the exception- '\n",
      "                         \"AttributeError: 'bytes' object has no attribute \"\n",
      "                         '\\'tell\\'\"\\n'\n",
      "                         'with tempfile.NamedTemporaryFile() as f:\\n'\n",
      "                         'f.write(response.content)\\n'\n",
      "                         'f.seek(0)\\n'\n",
      "                         'job = client.load_table_from_file(\\n'\n",
      "                         'f,\\n'\n",
      "                         'table_id,\\n'\n",
      "                         'location=\"US\",\\n'\n",
      "                         'job_config=job_config,\\n'\n",
      "                         ')\\n'\n",
      "                         'job.result()\\n'\n",
      "                         'logging.info(\"Data for month %d successfully loaded '\n",
      "                         'into table %s.\", month, table_id)\\n'\n",
      "                         \"return 'Data loaded into table \"\n",
      "                         \"{}.'.format(table_id)\"},\n",
      "                {'question': 'GCP BQ - When querying two different tables '\n",
      "                             'external and materialized you get the same '\n",
      "                             'result when count(distinct(*))',\n",
      "                 'section': 'Module 3: Data Warehousing',\n",
      "                 'text': 'You need to uncheck cache preferences in query '\n",
      "                         'settings'},\n",
      "                {'question': 'GCP BQ - How to handle type error from big query '\n",
      "                             'and parquet data?',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Problem: When you inject data into GCS using Pandas, '\n",
      "                         'there is a chance that some dataset has missing '\n",
      "                         'values on  DOlocationID and PUlocationID. Pandas by '\n",
      "                         'default will cast these columns as float data type, '\n",
      "                         'causing inconsistent data type between parquet in '\n",
      "                         'GCS and schema defined in big query. You will see '\n",
      "                         'something like this:\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Fix the data type issue in data pipeline\\n'\n",
      "                         'Before injecting data into GCS, use astype and Int64 '\n",
      "                         '(which is different from int64 and accept both '\n",
      "                         'missing value and integer exist in the column) to '\n",
      "                         'cast the columns.\\n'\n",
      "                         'Something like:\\n'\n",
      "                         'df[\"PUlocationID\"] = '\n",
      "                         'df.PUlocationID.astype(\"Int64\")\\n'\n",
      "                         'df[\"DOlocationID\"] = '\n",
      "                         'df.DOlocationID.astype(\"Int64\")\\n'\n",
      "                         'NOTE: It is best to define the data type of all the '\n",
      "                         'columns in the Transformation section of the ETL '\n",
      "                         'pipeline before loading to BigQuery'},\n",
      "                {'question': 'GCP BQ - Invalid project ID . Project IDs must '\n",
      "                             'contain 6-63 lowercase letters, digits, or '\n",
      "                             'dashes. Some project',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Problem occurs when misplacing content after fro``m '\n",
      "                         'clause in BigQuery SQLs.\\n'\n",
      "                         'Check to remove any extra apaces or any other '\n",
      "                         'symbols, keep in lowercases, digits and dashes only'},\n",
      "                {'question': 'GCP BQ - Does BigQuery support multiple columns '\n",
      "                             'partition?',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'No. Based on the documentation for Bigquery, it does '\n",
      "                         'not support more than 1 column to be partitioned.\\n'\n",
      "                         '[source]'},\n",
      "                {'question': 'GCP BQ - DATE() Error in BigQuery',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Error Message:\\n'\n",
      "                         'PARTITION BY expression must be '\n",
      "                         'DATE(<timestamp_column>), DATE(<datetime_column>), '\n",
      "                         'DATETIME_TRUNC(<datetime_column>, '\n",
      "                         'DAY/HOUR/MONTH/YEAR), a DATE column, '\n",
      "                         'TIMESTAMP_TRUNC(<timestamp_column>, '\n",
      "                         'DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, '\n",
      "                         'MONTH/YEAR), or RANGE_BUCKET(<int64_column>, '\n",
      "                         'GENERATE_ARRAY(<int64_value>, <int64_value>[, '\n",
      "                         '<int64_value>]))\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Convert the column to datetime first.\\n'\n",
      "                         'df[\"pickup_datetime\"] = '\n",
      "                         'pd.to_datetime(df[\"pickup_datetime\"])\\n'\n",
      "                         'df[\"dropOff_datetime\"] = '\n",
      "                         'pd.to_datetime(df[\"dropOff_datetime\"])'},\n",
      "                {'question': 'GCP BQ - Native tables vs External tables in '\n",
      "                             'BigQuery?',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Native tables are tables where the data is stored in '\n",
      "                         'BigQuery.  External tables store the data outside '\n",
      "                         'BigQuery, with BigQuery storing metadata about that '\n",
      "                         'external table.\\n'\n",
      "                         'Resources:\\n'\n",
      "                         'https://cloud.google.com/bigquery/docs/external-tables\\n'\n",
      "                         'https://cloud.google.com/bigquery/docs/tables-intro'},\n",
      "                {'question': 'GCP BQ ML - Unable to run command (shown in '\n",
      "                             'video) to export ML model from BQ to GCS',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Issue: Tried running command to export ML model from '\n",
      "                         'BQ to GCS from Week 3\\n'\n",
      "                         'bq --project_id taxi-rides-ny extract -m '\n",
      "                         'nytaxi.tip_model gs://taxi_ml_model/tip_model\\n'\n",
      "                         'It is failing on following error:\\n'\n",
      "                         'BigQuery error in extract operation: Error '\n",
      "                         'processing job Not found: Dataset was not found in '\n",
      "                         'location US\\n'\n",
      "                         'I verified the BQ data set and gcs bucket are in the '\n",
      "                         'same region- us-west1. Not sure how it gets location '\n",
      "                         'US. I couldn’t find the solution yet.\\n'\n",
      "                         'Solution:  Please enter correct project_id and '\n",
      "                         'gcs_bucket folder address. My gcs_bucket folder '\n",
      "                         'address is\\n'\n",
      "                         'gs://dtc_data_lake_optimum-airfoil-376815/tip_model'},\n",
      "                {'question': 'Dim_zones.sql Dataset was not found in location '\n",
      "                             'US When Running fact_trips.sql',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'To solve this error mention the location = US when '\n",
      "                         'creating the dim_zones table\\n'\n",
      "                         '{{ config(\\n'\n",
      "                         \"materialized='table',\\n\"\n",
      "                         \"location='US'\\n\"\n",
      "                         ') }}\\n'\n",
      "                         'Just Update this part to solve the issue and run the '\n",
      "                         'dim_zones again and then run the fact_trips'},\n",
      "                {'question': 'GCP BQ ML - Export ML model to make predictions '\n",
      "                             'does not work for MacBook with Apple M1 chip '\n",
      "                             '(arm architecture).',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Solution: proceed with setting up serving_dir on '\n",
      "                         'your computer as in the extract_model.md file. Then '\n",
      "                         'instead of\\n'\n",
      "                         'docker pull tensorflow/serving\\n'\n",
      "                         'use\\n'\n",
      "                         'docker pull emacski/tensorflow-serving\\n'\n",
      "                         'Then\\n'\n",
      "                         'docker run -p 8500:8500 -p 8501:8501 --mount '\n",
      "                         'type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model '\n",
      "                         '-e MODEL_NAME=tip_model -t '\n",
      "                         'emacski/tensorflow-serving\\n'\n",
      "                         'Then run the curl command as written, and you should '\n",
      "                         'get a prediction.'},\n",
      "                {'question': 'VMs - What do I do if my VM runs out of space?',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Try deleting data you’ve saved to your VM locally '\n",
      "                         'during ETLs\\n'\n",
      "                         'Kill processes related to deleted files\\n'\n",
      "                         'Download ncdu and look for large files (pay '\n",
      "                         'particular attention to files related to Prefect)\\n'\n",
      "                         'If you delete any files related to Prefect, '\n",
      "                         'eliminate caching from your flow code'},\n",
      "                {'question': 'Homework - What does it mean “Stop with loading '\n",
      "                             \"the files into a bucket.' Stop with loading the \"\n",
      "                             'files into a bucket?”',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': \"Ans: What they mean is that they don't want you to \"\n",
      "                         'do anything more than that. You should load the '\n",
      "                         'files into the bucket and create an external table '\n",
      "                         'based on those files (but nothing like cleaning the '\n",
      "                         'data and putting it in parquet format)'},\n",
      "                {'question': 'Homework - Reading parquets from nyc.gov '\n",
      "                             'directly into pandas returns Out of bounds error',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'If for whatever reason you try to read parquets '\n",
      "                         'directly from nyc.gov’s cloudfront into pandas, you '\n",
      "                         'might run into this error:\\n'\n",
      "                         'pyarrow.lib.ArrowInvalid: Casting from timestamp[us] '\n",
      "                         'to timestamp[ns] would result in out of bounds\\n'\n",
      "                         'Cause:\\n'\n",
      "                         'there is one errant data record where the '\n",
      "                         'dropOff_datetime was set to year 3019 instead of '\n",
      "                         '2019.\\n'\n",
      "                         'pandas uses “timestamp[ns]” (as noted above), and '\n",
      "                         'int64 only allows a ~580 year range, centered on '\n",
      "                         '2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\\n'\n",
      "                         'This becomes out of bounds when pandas tries to read '\n",
      "                         'it because 3019 > 2300 (approx value of '\n",
      "                         'pd.Timestamp.Max\\n'\n",
      "                         'Fix:\\n'\n",
      "                         'Use pyarrow to read it:\\n'\n",
      "                         'import pyarrow.parquet as pq df = '\n",
      "                         \"pq.read_table('fhv_tripdata_2019-02.parquet').to_pandas(safe=False)\\n\"\n",
      "                         'However this results in weird timestamps for the '\n",
      "                         'offending record\\n'\n",
      "                         'Read the datetime columns separately using '\n",
      "                         'pq.read_table\\n'\n",
      "                         '\\n'\n",
      "                         'table = pq.read_table(‘taxi.parquet’)\\n'\n",
      "                         'datetimes = [‘list of datetime column names’]\\n'\n",
      "                         'df_dts = pd.DataFrame()\\n'\n",
      "                         'for col in datetimes:\\n'\n",
      "                         'df_dts[col] = pd.to_datetime(table .column(col), '\n",
      "                         \"errors='coerce')\\n\"\n",
      "                         '\\n'\n",
      "                         'The `errors=’coerce’` parameter will convert the out '\n",
      "                         'of bounds timestamps into either the max or the min\\n'\n",
      "                         'Use parquet.compute.filter to remove the offending '\n",
      "                         'rows\\n'\n",
      "                         '\\n'\n",
      "                         'import pyarrow.compute as pc\\n'\n",
      "                         'table = pq.read_table(\"‘taxi.parquet\")\\n'\n",
      "                         'df = table.filter(\\n'\n",
      "                         'pc.less_equal(table[\"dropOff_datetime\"], '\n",
      "                         'pa.scalar(pd.Timestamp.max))\\n'\n",
      "                         ').to_pandas()'},\n",
      "                {'question': 'Question: for homework 3 , we need all 12 '\n",
      "                             'parquet files for green taxi 2022 right ?',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Answer: The 2022 NYC taxi data parquet files are '\n",
      "                         'available for each month separately. Therefore, you '\n",
      "                         'need to add all 12 files to your GCS bucket and then '\n",
      "                         'refer to them using the URIs option when creating an '\n",
      "                         'external table in BigQuery. You can use the wildcard '\n",
      "                         '\"*\" to refer to all 12 files using a single string.'},\n",
      "                {'question': 'Homework - Uploading files to GCS via GUI',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'This can help avoid schema issues in the homework. \\n'\n",
      "                         'Download files locally and use the ‘upload files’ '\n",
      "                         'button in GCS at the desired path. You can upload '\n",
      "                         'many files at once. You can also choose to upload a '\n",
      "                         'folder.'},\n",
      "                {'question': 'Homework - Qn 5: The partitioned/clustered table '\n",
      "                             'isn’t giving me the prediction I expected',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Ans: Take a careful look at the format of the dates '\n",
      "                         'in the question.'},\n",
      "                {'question': 'Homework - Qn 6: Did anyone get an exact match '\n",
      "                             'for one of the options given in Module 3 '\n",
      "                             'homework Q6?',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Many people aren’t getting an exact match, but are '\n",
      "                         'very close to one of the options. As per Alexey said '\n",
      "                         'to choose the closest option.'},\n",
      "                {'question': 'Python - invalid start byte Error Message',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': \"UnicodeDecodeError: 'utf-8' codec can't decode byte \"\n",
      "                         '0xa0 in position 41721: invalid start byte\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Step 1: When reading the data from the web into the '\n",
      "                         'pandas dataframe mention the encoding as follows:\\n'\n",
      "                         'pd.read_csv(dataset_url, low_memory=False, '\n",
      "                         \"encoding='latin1')\\n\"\n",
      "                         'Step 2: When writing the dataframe from the local '\n",
      "                         'system to GCS as a csv mention the encoding as '\n",
      "                         'follows:\\n'\n",
      "                         'df.to_csv(path_on_gsc, compression=\"gzip\", '\n",
      "                         \"encoding='utf-8')\\n\"\n",
      "                         'Alternative: use pd.read_parquet(url)'},\n",
      "                {'question': 'Python - Generators in python',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'A generator is a function in python that returns an '\n",
      "                         'iterator using the yield keyword.\\n'\n",
      "                         'A generator is a special type of iterable, similar '\n",
      "                         'to a list or a tuple, but with a crucial difference. '\n",
      "                         'Instead of creating and storing all the values in '\n",
      "                         'memory at once, a generator generates values '\n",
      "                         'on-the-fly as you iterate over it. This makes '\n",
      "                         'generators memory-efficient, particularly when '\n",
      "                         'dealing with large datasets.'},\n",
      "                {'question': 'Python - Easiest way to read multiple files at '\n",
      "                             'the same time?',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'The read_parquet function supports a list of files '\n",
      "                         'as an argument. The list of files will be merged '\n",
      "                         'into a single result table.'},\n",
      "                {'question': \"Python - These won't work. You need to make sure \"\n",
      "                             'you use Int64:',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'Incorrect:\\n'\n",
      "                         \"df['DOlocationID'] = \"\n",
      "                         \"pd.to_numeric(df['DOlocationID'], downcast=integer) \"\n",
      "                         'or\\n'\n",
      "                         \"df['DOlocationID'] = df['DOlocationID'].astype(int)\\n\"\n",
      "                         'Correct:\\n'\n",
      "                         \"df['DOlocationID'] = \"\n",
      "                         \"df['DOlocationID'].astype('Int64')\"},\n",
      "                {'question': 'Prefect - Error on Running Prefect Flow to Load '\n",
      "                             'data to GCS',\n",
      "                 'section': 'error: Error while reading table: '\n",
      "                            'trips_data_all.external_fhv_tripdata, error '\n",
      "                            \"message: Parquet column 'DOlocationID' has type \"\n",
      "                            'INT64 which does not match the target cpp_type '\n",
      "                            'DOUBLE.',\n",
      "                 'text': 'ValueError: Path '\n",
      "                         '/Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a '\n",
      "                         'does not exist.\\n'\n",
      "                         'Remove ```cache_key_fn=task_input_hash ``` as it’s '\n",
      "                         'in argument in your function & run your flow again.\\n'\n",
      "                         'Note: catche key is beneficial if you happen to run '\n",
      "                         \"the code multiple times, it won't repeat the process \"\n",
      "                         'which you have finished running in the previous '\n",
      "                         'run.  That means, if you have this ```cache_key``` '\n",
      "                         'in your initial run, this might cause the error.'},\n",
      "                {'question': 'Prefect - Tip: Downloading csv.gz from a url in '\n",
      "                             'a prefect environment (sample snippet).',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': '@task\\n'\n",
      "                         'def download_file(url: str, file_path: str):\\n'\n",
      "                         'response = requests.get(url)\\n'\n",
      "                         'open(file_path, \"wb\").write(response.content)\\n'\n",
      "                         'return file_path\\n'\n",
      "                         '@flow\\n'\n",
      "                         'def extract_from_web() -> None:\\n'\n",
      "                         'file_path = '\n",
      "                         \"download_file(url=f'{url-filename}.csv.gz',file_path=f'{filename}.csv.gz')\"},\n",
      "                {'question': 'If you are getting not found in location us '\n",
      "                             'error.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Update the seed column types in the dbt_project.yaml '\n",
      "                         'file\\n'\n",
      "                         'for using double : float\\n'\n",
      "                         'for using int : numeric\\n'\n",
      "                         'DBT Cloud production error: prod dataset not '\n",
      "                         'available in location EU\\n'\n",
      "                         'Problem: I am trying to deploy my DBT  models to '\n",
      "                         'production, using DBT Cloud. The data should live in '\n",
      "                         'BigQuery.  The dataset location is EU.  However, '\n",
      "                         'when I am running the model in production, a prod '\n",
      "                         'dataset is being create in BigQuery with a location '\n",
      "                         'US and the dbt invoke build is failing giving me '\n",
      "                         '\"ERROR 404: porject.dataset:prod not available in '\n",
      "                         'location EU\". I tried different ways to fix this. I '\n",
      "                         'am not sure if there is a more simple solution then '\n",
      "                         'creating my project or buckets in location US. Hope '\n",
      "                         'anyone can help here.\\n'\n",
      "                         'Note: Everything is working fine in development '\n",
      "                         'mode, the issue is just happening when scheduling '\n",
      "                         'and running job in production\\n'\n",
      "                         'Solution: I created the prod dataset manually in BQ '\n",
      "                         'and specified EU, then I ran the job.'},\n",
      "                {'question': 'Setup - No development environment',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Error: This project does not have a development '\n",
      "                         'environment configured. Please create a development '\n",
      "                         'environment and configure your development '\n",
      "                         'credentials to use the dbt IDE.\\n'\n",
      "                         'The error itself tells us how to solve this issue, '\n",
      "                         'the guide is here. And from videos @1:42 and also '\n",
      "                         'slack chat'},\n",
      "                {'question': 'Setup - Connecting dbt Cloud with BigQuery Error',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Runtime Error\\n'\n",
      "                         'dbt was unable to connect to the specified '\n",
      "                         'database.\\n'\n",
      "                         'The database returned the following error:\\n'\n",
      "                         '>Database Error\\n'\n",
      "                         'Access Denied: Project <project_name>: User does not '\n",
      "                         'have bigquery.jobs.create permission in project '\n",
      "                         '<project_name>.\\n'\n",
      "                         'Check your database credentials and try again. For '\n",
      "                         'more information, visit:\\n'\n",
      "                         'https://docs.getdbt.com/docs/configure-your-profile\\n'\n",
      "                         'Steps to resolve error in Google Cloud:\\n'\n",
      "                         '1. Navigate to IAM & Admin and select IAM\\n'\n",
      "                         '2. Click Grant Access if your newly created dbt '\n",
      "                         \"service account isn't listed\\n\"\n",
      "                         '3. In New principals field, add your service '\n",
      "                         'account\\n'\n",
      "                         '4. Select a Role and search for BigQuery Job User to '\n",
      "                         'add\\n'\n",
      "                         '5. Go back to dbt cloud project setup and Test your '\n",
      "                         'connection\\n'\n",
      "                         '6. Note: Also add BigQuery Data Owner, Storage '\n",
      "                         'Object Admin, & Storage Admin to prevent permission '\n",
      "                         'issues later in the course'},\n",
      "                {'question': 'Dbt build error',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'error: This dbt Cloud run was cancelled because a '\n",
      "                         'valid dbt project was not found. Please check that '\n",
      "                         'the repository contains a proper dbt_project.yml '\n",
      "                         'config file. If your dbt project is located in a '\n",
      "                         'subdirectory of the connected repository, be sure to '\n",
      "                         'specify its location on the Project settings page in '\n",
      "                         'dbt Cloud'},\n",
      "                {'question': 'Setup - Failed to clone repository.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Error: Failed to clone repository.\\n'\n",
      "                         'git clone '\n",
      "                         'git@github.com:DataTalksClub/data-engineering-zoomcamp.git '\n",
      "                         '/usr/src/develop/…\\n'\n",
      "                         \"Cloning into '/usr/src/develop/...\\n\"\n",
      "                         \"Warning: Permanently added 'github.com,140.82.114.4' \"\n",
      "                         '(ECDSA) to the list of known hosts.\\n'\n",
      "                         'git@github.com: Permission denied (publickey).\\n'\n",
      "                         'fatal: Could not read from remote repository.\\n'\n",
      "                         'Issue: You don’t have permissions to write to '\n",
      "                         'DataTalksClub/data-engineering-zoomcamp.git\\n'\n",
      "                         'Solution 1: Clone the repository and use this forked '\n",
      "                         'repo, which contains your github username. Then, '\n",
      "                         'proceed to specify the path, as in:\\n'\n",
      "                         '[your github '\n",
      "                         'username]/data-engineering-zoomcamp.git\\n'\n",
      "                         'Solution 2: create a fresh repo for dbt-lessons. '\n",
      "                         'We’d need to do branching and PRs in this lesson, so '\n",
      "                         'it might be a good idea to also not mess up your '\n",
      "                         'whole other repo. Then you don’t have to create a '\n",
      "                         'subfolder for the dbt project files\\n'\n",
      "                         'Solution 3: Use https link'},\n",
      "                {'question': 'dbt job - Triggered by pull requests is disabled '\n",
      "                             'when I try to create a new Continuous '\n",
      "                             'Integration job in dbt cloud.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Solution:\\n'\n",
      "                         'Check if you’re on the Developer Plan. As per the '\n",
      "                         \"prerequisites, you'll need to be enrolled in the \"\n",
      "                         'Team Plan or Enterprise Plan to set up a CI Job in '\n",
      "                         'dbt Cloud.\\n'\n",
      "                         \"So If you're on the Developer Plan, you'll need to \"\n",
      "                         'upgrade to utilise CI Jobs.\\n'\n",
      "                         'Note from another user: I’m in the Team Plan (trial '\n",
      "                         'period) but the option is still disabled. What '\n",
      "                         'worked for me instead was this. It works for the '\n",
      "                         'Developer (free) plan.'},\n",
      "                {'question': 'Setup - Your IDE session was unable to start. '\n",
      "                             'Please contact support.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Issue: If the DBT cloud IDE loading indefinitely '\n",
      "                         'then giving you this error\\n'\n",
      "                         'Solution: check the dbt_cloud_setup.md  file and '\n",
      "                         'make a SSH Key and use gitclone to import repo into '\n",
      "                         'dbt project, copy and paste deploy key back in your '\n",
      "                         'repo setting.'},\n",
      "                {'question': 'DBT - I am having problems with columns datatype '\n",
      "                             'while running DBT/BigQuery',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Issue: If you don’t define the column format while '\n",
      "                         'converting from csv to parquet Python will “choose” '\n",
      "                         'based on the first rows.\\n'\n",
      "                         '✅Solution: Defined the schema while running '\n",
      "                         'web_to_gcp.py pipeline.\\n'\n",
      "                         'Sebastian adapted the script:\\n'\n",
      "                         'https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\\n'\n",
      "                         'Need a quick change to make the file work with gz '\n",
      "                         'files, added the following lines (and don’t forget '\n",
      "                         'to delete the file at the end of each iteration of '\n",
      "                         'the loop to avoid any problem of disk space)\\n'\n",
      "                         'file_name_gz = '\n",
      "                         'f\"{service}_tripdata_{year}-{month}.csv.gz\"\\n'\n",
      "                         \"open(file_name_gz, 'wb').write(r.content)\\n\"\n",
      "                         'os.system(f\"gzip -d {file_name_gz}\")\\n'\n",
      "                         'os.system(f\"rm {file_name_init}.*\")\\n'\n",
      "                         'Same ERROR - When running dbt run for '\n",
      "                         'fact_trips.sql, the task failed with error:\\n'\n",
      "                         \"“Parquet column 'ehail_fee' has type DOUBLE which \"\n",
      "                         'does not match the target cpp_type INT64”\\n'\n",
      "                         '开启屏幕阅读器支持\\n'\n",
      "                         '要启用屏幕阅读器支持，请按Ctrl+Alt+Z。要了解键盘快捷键，请按Ctrl+斜杠。\\n'\n",
      "                         '查找和替换\\n'\n",
      "                         'Reason: Parquet files have their own schema. Some '\n",
      "                         'parquet files for green data have records with '\n",
      "                         'decimals in ehail_fee column.\\n'\n",
      "                         'There are some possible fixes:\\n'\n",
      "                         'Drop ehail_feel column since it is not really used. '\n",
      "                         'For instance when creating a partitioned table from '\n",
      "                         'the external table in BigQuery\\n'\n",
      "                         'SELECT * EXCEPT (ehail_fee) FROM…\\n'\n",
      "                         'Modify stg_green_tripdata.sql model using this line '\n",
      "                         'cast(0 as numeric) as ehail_fee.\\n'\n",
      "                         'Modify Airflow dag to make the conversion and avoid '\n",
      "                         'the error.\\n'\n",
      "                         'pv.read_csv(src_file, '\n",
      "                         'convert_options=pv.ConvertOptions(column_types = '\n",
      "                         \"{'ehail_fee': 'float64'}))\\n\"\n",
      "                         'Same type of ERROR - parquet files with different '\n",
      "                         'data types - Fix it with pandas\\n'\n",
      "                         'Here is another possibility that could be '\n",
      "                         'interesting:\\n'\n",
      "                         'You can specify the dtypes when importing the file '\n",
      "                         'from csv to a dataframe with pandas\\n'\n",
      "                         'pd.from_csv(..., dtype=type_dict)\\n'\n",
      "                         'One obstacle is that the regular int64 pandas use (I '\n",
      "                         'think this is from the numpy library) does not '\n",
      "                         'accept null values (NaN, not a number). But you can '\n",
      "                         'use the pandas Int64 instead, notice capital ‘I’. '\n",
      "                         'The type_dict is a python dictionary mapping the '\n",
      "                         'column names to the dtypes.\\n'\n",
      "                         'Sources:\\n'\n",
      "                         'https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\\n'\n",
      "                         'Nullable integer data type — pandas 1.5.3 '\n",
      "                         'documentation'},\n",
      "                {'question': 'Ingestion: When attempting to use the provided '\n",
      "                             'quick script to load trip data into GCS, you '\n",
      "                             'receive error Access Denied from the S3 bucket',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'If the provided URL isn’t working for you '\n",
      "                         '(https://nyc-tlc.s3.amazonaws.com/trip+data/):\\n'\n",
      "                         'We can use the GitHub CLI to easily download the '\n",
      "                         'needed trip data from '\n",
      "                         'https://github.com/DataTalksClub/nyc-tlc-data, and '\n",
      "                         'manually upload to a GCS bucket.\\n'\n",
      "                         'Instructions on how to download the CLI here: '\n",
      "                         'https://github.com/cli/cli\\n'\n",
      "                         'Commands to use:\\n'\n",
      "                         'gh auth login\\n'\n",
      "                         'gh release list -R DataTalksClub/nyc-tlc-data\\n'\n",
      "                         'gh release download yellow -R '\n",
      "                         'DataTalksClub/nyc-tlc-data\\n'\n",
      "                         'gh release download green -R '\n",
      "                         'DataTalksClub/nyc-tlc-data\\n'\n",
      "                         'etc.\\n'\n",
      "                         'Now you can upload the files to a GCS bucket using '\n",
      "                         'the GUI.'},\n",
      "                {'question': 'Ingestion - Error thrown by '\n",
      "                             'format_to_parquet_task when converting '\n",
      "                             'fhv_tripdata_2020-01.csv using Airflow',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'R: This conversion is needed for the question 3 of '\n",
      "                         'homework, in order to process files for fhv data. '\n",
      "                         'The error is:\\n'\n",
      "                         'pyarrow.lib.ArrowInvalid: CSV parse error: Expected '\n",
      "                         '7 columns, got 1: B02765\\n'\n",
      "                         'Cause: Some random line breaks in this particular '\n",
      "                         'file.\\n'\n",
      "                         'Fixed by opening a bash in the container executing '\n",
      "                         'the dag and manually running the following command '\n",
      "                         'that deletes all \\\\n not preceded by \\\\r.\\n'\n",
      "                         \"perl -i -pe 's/(?<!\\\\r)\\\\n/\\\\1/g' \"\n",
      "                         'fhv_tripdata_2020-01.csv\\n'\n",
      "                         'After that, clear the failed task in Airflow to '\n",
      "                         'force re-execution.'},\n",
      "                {'question': 'Hack to load yellow and green trip data for 2019 '\n",
      "                             'and 2020',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'I initially followed '\n",
      "                         'data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py '\n",
      "                         'at main · DataTalksClub/data-engineering-bootcamp '\n",
      "                         '(github.com)\\n'\n",
      "                         'But it was taking forever for the yellow trip data '\n",
      "                         'and when I tried to download and upload the parquet '\n",
      "                         'files directly to GCS, that works fine but when '\n",
      "                         'creating the Bigquery table, there was a schema '\n",
      "                         'inconsistency issue\\n'\n",
      "                         'Then I found another hack shared in the slack which '\n",
      "                         'was suggested by Victoria.\\n'\n",
      "                         '[Optional] Hack for loading data to BigQuery for '\n",
      "                         'Week 4 - YouTube\\n'\n",
      "                         'Please watch until the end as there is few schema '\n",
      "                         'changes required to be done'},\n",
      "                {'question': 'Move many files (more than one) from Google '\n",
      "                             'cloud storage bucket to Big query',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': '“gs\\\\storage_link\\\\*.parquet” need to be added in '\n",
      "                         'destination folder'},\n",
      "                {'question': 'GCP VM - All of sudden ssh stopped working for '\n",
      "                             'my VM after my last restart',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'One common cause experienced is lack of space after '\n",
      "                         'running prefect several times. When running prefect, '\n",
      "                         'check the folder ‘.prefect/storage’ and delete the '\n",
      "                         'logs now and then to avoid the problem.'},\n",
      "                {'question': 'GCP VM - If you have lost SSH access to your '\n",
      "                             'machine due to lack of space. Permission denied '\n",
      "                             '(publickey)',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'You can try to do this steps:'},\n",
      "                {'question': '404 Not found: Dataset '\n",
      "                             'eighth-zenith-372015:trip_data_all was not found '\n",
      "                             'in location us-west1',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'R: Go to BigQuery, and check the location of BOTH\\n'\n",
      "                         'The source dataset (trips_data_all), and\\n'\n",
      "                         'The schema you’re trying to write to (name should '\n",
      "                         'be \\tdbt_<first initial><last name> (if you didn’t '\n",
      "                         'change the default settings at the end when setting '\n",
      "                         'up your project))\\n'\n",
      "                         'Likely, your source data will be in your region, but '\n",
      "                         'the write location will be a multi-regional location '\n",
      "                         '(US in this example). Delete these datasets, and '\n",
      "                         'recreate them with your specified region and the '\n",
      "                         'correct naming format.\\n'\n",
      "                         'Alternatively, instead of removing datasets, you can '\n",
      "                         'specify the single-region location you are using. '\n",
      "                         'E.g. instead of ‘location: US’, specify the region, '\n",
      "                         'so ‘location: US-east1’. See this Github comment for '\n",
      "                         'more detail. Additionally please see this post of '\n",
      "                         'Sandy\\n'\n",
      "                         'In DBT cloud you can actually specify the location '\n",
      "                         'using the following steps:\\n'\n",
      "                         'GPo to your profile page (top right drop-down --> '\n",
      "                         'profile)\\n'\n",
      "                         'Then go to under Credentials --> Analytics (you may '\n",
      "                         'have customised this name)\\n'\n",
      "                         'Click on Bigquery >\\n'\n",
      "                         'Hit Edit\\n'\n",
      "                         'Update your location, you may need to re-upload your '\n",
      "                         'service account JSON to re-fetch your private key, '\n",
      "                         'and save. (NOTE: be sure to exactly copy the region '\n",
      "                         'BigQuery specifies your dataset is in.)'},\n",
      "                {'question': 'When executing dbt run after installing '\n",
      "                             'dbt-utils latest version i.e., 1.0.0 warning has '\n",
      "                             'generated',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Error: `dbt_utils.surrogate_key` has been replaced '\n",
      "                         'by `dbt_utils.generate_surrogate_key`\\n'\n",
      "                         'Fix:\\n'\n",
      "                         'Replace dbt_utils.surrogate_key  with '\n",
      "                         'dbt_utils.generate_surrogate_key in '\n",
      "                         'stg_green_tripdata.sql\\n'\n",
      "                         'When executing dbt run after fact_trips.sql has been '\n",
      "                         'created, the task failed with error:\\n'\n",
      "                         'R: “Access Denied: BigQuery BigQuery: Permission '\n",
      "                         'denied while globbing file pattern.”\\n'\n",
      "                         '1. Fixed by adding the Storage Object Viewer role to '\n",
      "                         'the service account in use in BigQuery.\\n'\n",
      "                         '2. Add the related roles to the service account in '\n",
      "                         'use in GCS.'},\n",
      "                {'question': 'When You are getting error dbt_utils not found',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'You need to create packages.yml file in main project '\n",
      "                         'directory and add packages’ meta data:\\n'\n",
      "                         'packages:\\n'\n",
      "                         '- package: dbt-labs/dbt_utils\\n'\n",
      "                         'version: 0.8.0\\n'\n",
      "                         'After creating file run:\\n'\n",
      "                         'And hit enter.'},\n",
      "                {'question': 'Lineage is currently unavailable. Check that '\n",
      "                             'your project does not contain compilation errors '\n",
      "                             'or contact support if this error persists.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Ensure you properly format your yml file. Check the '\n",
      "                         'build logs if the run was completed successfully. '\n",
      "                         'You can expand the command history console (where '\n",
      "                         \"you type the --vars '{'is_test_run': 'false'}')  and \"\n",
      "                         'click on any stage’s logs to expand and read errors '\n",
      "                         'messages or warnings.'},\n",
      "                {'question': 'Build - Why do my Fact_trips only contain a few '\n",
      "                             'days of data?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Make sure you use:\\n'\n",
      "                         'dbt run --var ‘is_test_run: false’ or\\n'\n",
      "                         'dbt build --var ‘is_test_run: false’\\n'\n",
      "                         '(watch out for formatted text from this document: '\n",
      "                         're-type the single quotes). If that does not work, '\n",
      "                         \"use --vars '{'is_test_run': 'false'}' with each \"\n",
      "                         'phrase separately quoted.'},\n",
      "                {'question': 'Build - Why do my fact_trips only contain one '\n",
      "                             'month of data?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Check if you specified if_exists argument correctly '\n",
      "                         'when writing data from GCS to BigQuery. When I wrote '\n",
      "                         'my automated flow for each month of the years 2019 '\n",
      "                         'and 2020 for green and yellow data I had specified '\n",
      "                         'if_exists=\"replace\" while I was experimenting with '\n",
      "                         'the flow setup. Once you want to run the flow for '\n",
      "                         'all months in 2019 and 2020 make sure to set '\n",
      "                         'if_exists=\"append\"\\n'\n",
      "                         'if_exists=\"replace\" will replace the whole table '\n",
      "                         'with only the month data that you are writing into '\n",
      "                         'BigQuery in that one iteration -> you end up with '\n",
      "                         'only one month in BigQuery (the last one you '\n",
      "                         'inserted)\\n'\n",
      "                         'if_exists=\"append\" will append the new monthly data '\n",
      "                         '-> you end up with data from all months'},\n",
      "                {'question': 'BigQuery returns an error when I try to run the '\n",
      "                             'dm_monthly_zone_revenue.sql model.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'R: After the second SELECT, change this line:\\n'\n",
      "                         \"date_trunc('month', pickup_datetime) as \"\n",
      "                         'revenue_month,\\n'\n",
      "                         'To this line:\\n'\n",
      "                         'date_trunc(pickup_datetime, month) as '\n",
      "                         'revenue_month,\\n'\n",
      "                         'Make sure that “month” isn’t surrounded by quotes!'},\n",
      "                {'question': 'Replace: \\n'\n",
      "                             '{{ dbt_utils.surrogate_key([ \\n'\n",
      "                             '     field_a, \\n'\n",
      "                             '     field_b, \\n'\n",
      "                             '     field_c,\\n'\n",
      "                             '     …,\\n'\n",
      "                             '     field_z     \\n'\n",
      "                             ']) }}',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'For this instead:\\n'\n",
      "                         '{{ dbt_utils.generate_surrogate_key([ \\n'\n",
      "                         '     field_a, \\n'\n",
      "                         '     field_b, \\n'\n",
      "                         '     field_c,\\n'\n",
      "                         '     …,\\n'\n",
      "                         '     field_z\\n'\n",
      "                         ']) }}'},\n",
      "                {'question': 'I changed location in dbt, but dbt run still '\n",
      "                             'gives me an error',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Remove the dataset from BigQuery which was created '\n",
      "                         'by dbt and run dbt run again so that it will '\n",
      "                         'recreate the dataset in BigQuery with the correct '\n",
      "                         'location'},\n",
      "                {'question': 'I ran dbt run without specifying variable which '\n",
      "                             'gave me a table of 100 rows. I ran again with '\n",
      "                             'the variable value specified but my table still '\n",
      "                             'has 100 rows in BQ.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Remove the dataset from BigQuery created by dbt and '\n",
      "                         'run again (with test disabled) to ensure the dataset '\n",
      "                         'created has all the rows.\\n'\n",
      "                         'DBT - Why am I getting a new dataset after running '\n",
      "                         'my CI/CD Job? / What is this new dbt dataset in '\n",
      "                         'BigQuery?\\n'\n",
      "                         'Answer: when you create the CI/CD job, under '\n",
      "                         '‘Compare Changes against an environment (Deferral) '\n",
      "                         'make sure that you select ‘ No; do not defer to '\n",
      "                         'another environment’ - otherwise dbt won’t merge '\n",
      "                         'your dev models into production models; it will '\n",
      "                         'create a new environment called ‘dbt_cloud_pr_number '\n",
      "                         'of pull request’'},\n",
      "                {'question': 'Why do we need the Staging dataset?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Vic created three different datasets in the videos.. '\n",
      "                         'dbt_<name> was used for development and you used a '\n",
      "                         'production dataset for the production environment. '\n",
      "                         'What was the use for the staging dataset?\\n'\n",
      "                         'R: Staging, as the name suggests, is like an '\n",
      "                         'intermediate between the raw datasets and the fact '\n",
      "                         'and dim tables, which are the finished product, so '\n",
      "                         \"to speak. You'll notice that the datasets in staging \"\n",
      "                         'are materialised as views and not tables.\\n'\n",
      "                         \"Vic didn't use it for the project, you just need to \"\n",
      "                         'create production and dbt_name + trips_data_all that '\n",
      "                         'you had already.'},\n",
      "                {'question': 'DBT Docs Served but Not Accessible via Browser',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Try removing the “network: host” line in '\n",
      "                         'docker-compose.'},\n",
      "                {'question': 'BigQuery adapter: 404 Not found: Dataset was not '\n",
      "                             'found in location europe-west6',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Go to Account settings >> Project >> Analytics >> '\n",
      "                         'Click on your connection >> go all the way down to '\n",
      "                         'Location and type in the GCP location just as '\n",
      "                         'displayed in GCP (e.g. europe-west6). You might need '\n",
      "                         'to reupload your GCP key.\\n'\n",
      "                         'Delete your dataset in GBQ\\n'\n",
      "                         'Rebuild project: dbt build\\n'\n",
      "                         'Newly built dataset should be in the correct '\n",
      "                         'location'},\n",
      "                {'question': 'Dbt+git - Main branch is “read-only”',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Create a new branch to edit. More on this can be '\n",
      "                         'found here in the dbt docs.'},\n",
      "                {'question': \"Dbt+git - It appears that I can't edit the files \"\n",
      "                             \"because I'm in read-only mode. Does anyone know \"\n",
      "                             'how I can change that?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Create a new branch for development, then you can '\n",
      "                         'merge it to the main branch\\n'\n",
      "                         'Create a new branch and switch to this branch. It '\n",
      "                         'allows you to make changes. Then you can commit and '\n",
      "                         'push the changes to the “main” branch.'},\n",
      "                {'question': 'Dbt deploy + Git CI - cannot create CI checks '\n",
      "                             'job for deployment to Production. See more '\n",
      "                             'discussion in slack chat',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Error:\\n'\n",
      "                         'Triggered by pull requests\\n'\n",
      "                         'This feature is only available for dbt repositories '\n",
      "                         \"connected through dbt Cloud's native integration \"\n",
      "                         'with Github, Gitlab, or Azure DevOps\\n'\n",
      "                         'Solution: Contrary to the guide on DTC repo, don’t '\n",
      "                         'use the Git Clone option. Use the Github one '\n",
      "                         'instead. Step-by-step guide to UN-LINK Git Clone and '\n",
      "                         'RE-LINK with Github in the next entry below'},\n",
      "                {'question': 'Dbt deploy + Git CI - Unable to configure '\n",
      "                             'Continuous Integration (CI) with Github',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'If you’re trying to configure CI with Github and on '\n",
      "                         'the job’s options you can’t see Run on Pull '\n",
      "                         'Requests? on triggers, you have to reconnect with '\n",
      "                         'Github using native connection instead clone by SSH. '\n",
      "                         'Follow these steps:\\n'\n",
      "                         'On Profile Settings > Linked Accounts connect your '\n",
      "                         'Github account with dbt project allowing the '\n",
      "                         'permissions asked. More info at '\n",
      "                         'https://docs.getdbt.com/docs/collaborate/git/connect-gith\\n'\n",
      "                         'Disconnect your current Github’s configuration from '\n",
      "                         'Account Settings > Projects (analytics) > Github '\n",
      "                         'connection. At the bottom left appears the button '\n",
      "                         'Disconnect, press it.\\n'\n",
      "                         'Once we have confirmed the change, we can configure '\n",
      "                         'it again. This time, choose Github and it will '\n",
      "                         'appear in all repositories which you have allowed to '\n",
      "                         'work with dbt. Select your repository and it’s '\n",
      "                         'ready.\\n'\n",
      "                         'Go to the Deploy > job configuration’s page and go '\n",
      "                         'down until Triggers and now you can see the option '\n",
      "                         'Run on Pull Requests:'},\n",
      "                {'question': 'Compilation Error (Model '\n",
      "                             \"'model.my_new_project.stg_green_tripdata' \"\n",
      "                             '(models/staging/stg_green_tripdata.sql) depends '\n",
      "                             \"on a source named 'staging.green_trip_external' \"\n",
      "                             'which was not found)',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': \"If you're following video DE Zoomcamp 4.3.1 - \"\n",
      "                         'Building the First DBT Models, you may have '\n",
      "                         'encountered an issue at 14:25 where the Lineage '\n",
      "                         \"graph isn't displayed and a Compilation Error \"\n",
      "                         \"occurs, as shown in the attached image. Don't worry \"\n",
      "                         '- a quick fix for this is to simply save your '\n",
      "                         \"schema.yml file. Once you've done this, you should \"\n",
      "                         'be able to view your Lineage graph without any '\n",
      "                         'further issues.'},\n",
      "                {'question': \"'NoneType' object is not iterable\",\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': '> in macro test_accepted_values '\n",
      "                         '(tests/generic/builtin.sql)\\n'\n",
      "                         '> called by test '\n",
      "                         'accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ '\n",
      "                         '(models/staging/schema.yml)\\n'\n",
      "                         'Remember that you have to add to dbt_project.yml the '\n",
      "                         'vars:\\n'\n",
      "                         'vars:\\n'\n",
      "                         'payment_type_values: [1, 2, 3, 4, 5, 6]'},\n",
      "                {'question': 'dbt macro errors with '\n",
      "                             'get_payment_type_description(payment_type)',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'You will face this issue if you copied and pasted '\n",
      "                         'the exact macro directly from '\n",
      "                         'data-engineering-zoomcamp repo.\\n'\n",
      "                         'BigQuery adapter: Retry attempt 1 of 1 after error: '\n",
      "                         \"BadRequest('No matching signature for operator CASE \"\n",
      "                         'for argument types: STRING, INT64, STRING, INT64, '\n",
      "                         'STRING, INT64, STRING, INT64, STRING, INT64, STRING, '\n",
      "                         'INT64, STRING, NULL at [35:5]; reason: invalidQuery, '\n",
      "                         'location: query, message: No matching signature for '\n",
      "                         'operator CASE for argument types: STRING, INT64, '\n",
      "                         'STRING, INT64, STRING, INT64, STRING, INT64, STRING, '\n",
      "                         \"INT64, STRING, INT64, STRING, NULL at [35:5]')\\n\"\n",
      "                         'What you’d have to do is to change the data type of '\n",
      "                         'the numbers (1, 2, 3 etc.) to text by inserting ‘’, '\n",
      "                         'as the initial ‘payment_type’ data type should be '\n",
      "                         'string (Note: I extracted and loaded the green trips '\n",
      "                         'data using Google BQ Marketplace)\\n'\n",
      "                         '{#\\n'\n",
      "                         'This macro returns the description of the '\n",
      "                         'payment_type\\n'\n",
      "                         '#}\\n'\n",
      "                         '{% macro get_payment_type_description(payment_type) '\n",
      "                         '-%}\\n'\n",
      "                         'case {{ payment_type }}\\n'\n",
      "                         \"when '1' then 'Credit card'\\n\"\n",
      "                         \"when '2' then 'Cash'\\n\"\n",
      "                         \"when '3' then 'No charge'\\n\"\n",
      "                         \"when '4' then 'Dispute'\\n\"\n",
      "                         \"when '5' then 'Unknown'\\n\"\n",
      "                         \"when '6' then 'Voided trip'\\n\"\n",
      "                         'end\\n'\n",
      "                         '{%- endmacro %}'},\n",
      "                {'question': 'Troubleshooting in dbt:',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'The dbt error  log contains a link to BigQuery. When '\n",
      "                         'you follow it you will see your query and the '\n",
      "                         'problematic line will be highlighted.'},\n",
      "                {'question': 'Why changing the target schema to “marts” '\n",
      "                             'actually creates a schema named “dbt_marts” '\n",
      "                             'instead?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'It is a default behaviour of dbt to append custom '\n",
      "                         'schema to initial schema. To override this behaviour '\n",
      "                         'simply create a macro named '\n",
      "                         '“generate_schema_name.sql”:\\n'\n",
      "                         '{% macro generate_schema_name(custom_schema_name, '\n",
      "                         'node) -%}\\n'\n",
      "                         '{%- set default_schema = target.schema -%}\\n'\n",
      "                         '{%- if custom_schema_name is none -%}\\n'\n",
      "                         '{{ default_schema }}\\n'\n",
      "                         '{%- else -%}\\n'\n",
      "                         '{{ custom_schema_name | trim }}\\n'\n",
      "                         '{%- endif -%}\\n'\n",
      "                         '{%- endmacro %}\\n'\n",
      "                         'Now you can override default custom schema in '\n",
      "                         '“dbt_project.yml”:'},\n",
      "                {'question': 'How to set subdirectory of the github repository '\n",
      "                             'as the dbt project root',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'There is a project setting which allows you to set '\n",
      "                         '`Project subdirectory` in dbt cloud:'},\n",
      "                {'question': \"Compilation Error : Model 'model.XXX' \"\n",
      "                             '(models/<model_path>/XXX.sql) depends on a '\n",
      "                             \"source named '<a table name>' which was not \"\n",
      "                             'found',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Remember that you should modify accordingly your '\n",
      "                         '.sql models, to read from existing table names in '\n",
      "                         'BigQuery/postgres db\\n'\n",
      "                         \"Example: select * from {{ source('staging',<your \"\n",
      "                         \"table name in the database>') }}\"},\n",
      "                {'question': \"Compilation Error : Model '<model_name>' \"\n",
      "                             '(<model_path>) depends on a node named '\n",
      "                             \"'<seed_name>' which was not found   (Production \"\n",
      "                             'Environment)',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Make sure that you create a pull request from your '\n",
      "                         'Development branch to the Production branch (main by '\n",
      "                         'default). After that, check in your ‘seeds’ folder '\n",
      "                         'if the seed file is inside it.\\n'\n",
      "                         'Another thing to check is your .gitignore file. Make '\n",
      "                         'sure that the .csv extension is not included.'},\n",
      "                {'question': 'When executing dbt run after using fhv_tripdata '\n",
      "                             'as an external table: you get “Access Denied: '\n",
      "                             'BigQuery BigQuery: Permission denied”',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': '1. Go to your dbt cloud service account\\n'\n",
      "                         '1. Adding the  [Storage Object Admin,Storage Admin] '\n",
      "                         'role in addition tco BigQuery Admin.'},\n",
      "                {'question': 'How to automatically infer the column data type '\n",
      "                             '(pandas missing value issues)?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Problem: when injecting data to bigquery, you may '\n",
      "                         'face the type error. This is because pandas by '\n",
      "                         'default will parse integer columns with missing '\n",
      "                         'value as float type.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'One way to solve this problem is to specify/ cast '\n",
      "                         'data type Int64 during the data transformation '\n",
      "                         'stage.\\n'\n",
      "                         'However, you may be lazy to type all the int '\n",
      "                         'columns. If that is the case, you can simply use '\n",
      "                         'convert_dtypes to infer the data type\\n'\n",
      "                         '# Make pandas to infer correct data type (as pandas '\n",
      "                         'parse int with missing as float)\\n'\n",
      "                         'df.fillna(-999999, inplace=True)\\n'\n",
      "                         'df = df.convert_dtypes()\\n'\n",
      "                         'df = df.replace(-999999, None)'},\n",
      "                {'question': 'When loading github repo raise exception that '\n",
      "                             '‘taxi_zone_lookup’ not found',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Seed files loaded from directory with name ‘seed’, '\n",
      "                         'that’s why you should rename dir with name ‘data’ to '\n",
      "                         '‘seed’'},\n",
      "                {'question': '‘taxi_zone_lookup’ not found',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Check the .gitignore file and make sure you don’t '\n",
      "                         'have *.csv in it\\n'\n",
      "                         '\\n'\n",
      "                         'Dbt error 404 was not found in location\\n'\n",
      "                         'My specific error:\\n'\n",
      "                         'Runtime Error in rpc request (from remote '\n",
      "                         'system.sql) 404 Not found: Table '\n",
      "                         'dtc-de-0315:trips_data_all.green_tripdata_partitioned '\n",
      "                         'was not found in location europe-west6 Location: '\n",
      "                         'europe-west6 Job ID: '\n",
      "                         '168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\\n'\n",
      "                         'Make sure all of your datasets have the correct '\n",
      "                         'region and not a generalised region:\\n'\n",
      "                         'Europe-west6 as opposed to EU\\n'\n",
      "                         '\\n'\n",
      "                         'Match this in dbt settings:\\n'\n",
      "                         'dbt -> projects -> optional settings -> manually set '\n",
      "                         'location to match'},\n",
      "                {'question': 'Data type errors when ingesting with parquet '\n",
      "                             'files',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'The easiest way to avoid these errors is by '\n",
      "                         'ingesting the relevant data in a .csv.gz file type. '\n",
      "                         'Then, do:\\n'\n",
      "                         'CREATE OR REPLACE EXTERNAL TABLE '\n",
      "                         '`dtc-de.trips_data_all.fhv_tripdata`\\n'\n",
      "                         'OPTIONS (\\n'\n",
      "                         \"format = 'CSV',\\n\"\n",
      "                         'uris = '\n",
      "                         \"['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\\n\"\n",
      "                         ');\\n'\n",
      "                         'As an example. You should no longer have any data '\n",
      "                         'type issues for week 4.'},\n",
      "                {'question': 'Inconsistent number of rows when re-running '\n",
      "                             'fact_trips model',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'This is due to the way the deduplication is done in '\n",
      "                         'the two staging files.\\n'\n",
      "                         'Solution: add order by in the partition by part of '\n",
      "                         'both staging files. Keep adding columns to order by '\n",
      "                         'until the number of rows in the fact_trips table is '\n",
      "                         'consistent when re-running the fact_trips model.\\n'\n",
      "                         'Explanation (a bit convoluted, feel free to clarify, '\n",
      "                         'correct etc.)\\n'\n",
      "                         'We partition by vendor id and pickup_datetime and '\n",
      "                         'choose the first row (rn=1) from all these '\n",
      "                         'partitions. These partitions are not ordered, so '\n",
      "                         'every time we run this, the first row might be a '\n",
      "                         'different one. Since the first row is different '\n",
      "                         'between runs, it might or might not contain an '\n",
      "                         'unknown borough. Then, in the fact_trips model we '\n",
      "                         'will discard a different number of rows when we '\n",
      "                         'discard all values with an unknown borough.'},\n",
      "                {'question': 'Data Type Error when running fact table',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'If you encounter data type error on trip_type '\n",
      "                         'column, it may due to some nan values that isn’t '\n",
      "                         'null in bigquery.\\n'\n",
      "                         'Solution: try casting it to FLOAT datatype instead '\n",
      "                         'of NUMERIC'},\n",
      "                {'question': 'CREATE TABLE has columns with duplicate name '\n",
      "                             'locationid.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'This error could result if you are using some select '\n",
      "                         '* query without mentioning the name of table for '\n",
      "                         'ex:\\n'\n",
      "                         'with dim_zones as (\\n'\n",
      "                         'select * from '\n",
      "                         '`engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\\n'\n",
      "                         \"where borough != 'Unknown'\\n\"\n",
      "                         '),\\n'\n",
      "                         'fhv as (\\n'\n",
      "                         'select * from '\n",
      "                         '`engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\\n'\n",
      "                         ')\\n'\n",
      "                         'select * from fhv\\n'\n",
      "                         'inner join dim_zones as pickup_zone\\n'\n",
      "                         'on fhv.PUlocationID = pickup_zone.locationid\\n'\n",
      "                         'inner join dim_zones as dropoff_zone\\n'\n",
      "                         'on fhv.DOlocationID = dropoff_zone.locationid\\n'\n",
      "                         ');\\n'\n",
      "                         'To resolve just replace use : select fhv.* from fhv'},\n",
      "                {'question': 'Bad int64 value: 0.0 error',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Some ehail fees are null and casting them to integer '\n",
      "                         'gives Bad int64 value: 0.0 error,\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Using safe_cast returns NULL instead of throwing an '\n",
      "                         'error. So use safe_cast from dbt_utils function in '\n",
      "                         'the jinja code for casting into integer as follows:\\n'\n",
      "                         \"{{ dbt_utils.safe_cast('ehail_fee',  \"\n",
      "                         'api.Column.translate_type(\"integer\"))}} as '\n",
      "                         'ehail_fee,\\n'\n",
      "                         'Can also just use safe_cast(ehail_fee as integer) '\n",
      "                         'without relying on dbt_utils.'},\n",
      "                {'question': 'Bad int64 value: 2.0/1.0 error',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'You might encounter this when building the '\n",
      "                         'fact_trips.sql model. The issue may be with the '\n",
      "                         'payment_type_description field.\\n'\n",
      "                         'Using safe_cast as above, would cause the entire '\n",
      "                         'field to become null. A better approach is to drop '\n",
      "                         'the offending decimal place, then cast to integer.\\n'\n",
      "                         \"cast(replace({{ payment_type }},'.0','') as \"\n",
      "                         'integer)\\n'\n",
      "                         'Bad int64 value: 1.0 error (again)\\n'\n",
      "                         '\\n'\n",
      "                         'I found that there are more columns causing the bad '\n",
      "                         'INT64: ratecodeid and trip_type on Green_tripdata '\n",
      "                         'table.\\n'\n",
      "                         'You can use the queries below to address them:\\n'\n",
      "                         'CAST(\\n'\n",
      "                         \"REGEXP_REPLACE(CAST(rate_code AS STRING), r'\\\\.0', \"\n",
      "                         \"'') AS INT64\\n\"\n",
      "                         ') AS ratecodeid,\\n'\n",
      "                         'CAST(\\n'\n",
      "                         'CASE\\n'\n",
      "                         'WHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), '\n",
      "                         \"r'\\\\.\\\\d+') THEN NULL\\n\"\n",
      "                         'ELSE CAST(trip_type AS INT64)\\n'\n",
      "                         'END AS INT64\\n'\n",
      "                         ') AS trip_type,'},\n",
      "                {'question': 'DBT - Error on building fact_trips.sql: Parquet '\n",
      "                             \"column 'ehail_fee' has type DOUBLE which does \"\n",
      "                             'not match the target cpp_type INT64. File: '\n",
      "                             'gs://<gcs '\n",
      "                             'bucket>/<table>/green_taxi_2019-01.parquet\")',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'The two solution above don’t work for me - I used '\n",
      "                         'the line below in `stg_green_trips.sql` to replace '\n",
      "                         'the original ehail_fee line:\\n'\n",
      "                         \"`{{ dbt.safe_cast('ehail_fee',  \"\n",
      "                         'api.Column.translate_type(\"numeric\"))}} as '\n",
      "                         'ehail_fee,`'},\n",
      "                {'question': 'The - vars argument must be a YAML dictionary, '\n",
      "                             'but was of type str',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Remember to add a space between the variable and the '\n",
      "                         \"value. Otherwise, it won't be interpreted as a \"\n",
      "                         'dictionary.\\n'\n",
      "                         'It should be:\\n'\n",
      "                         \"dbt run --var 'is_test_run: false'\"},\n",
      "                {'question': 'Not able to change Environment Type as it is '\n",
      "                             'greyed out and inaccessible',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': \"You don't need to change the environment type. If \"\n",
      "                         'you are following the videos, you are creating a '\n",
      "                         'Production Deployment, so the only available option '\n",
      "                         \"is the correct one.'\"},\n",
      "                {'question': 'Access Denied: Table '\n",
      "                             'taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: '\n",
      "                             'User does not have permission to query table '\n",
      "                             'taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, '\n",
      "                             'or perhaps it does not exist in location US.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Database Error in model stg_yellow_tripdata '\n",
      "                         '(models/staging/stg_yellow_tripdata.sql)\\n'\n",
      "                         'Access Denied: Table '\n",
      "                         'taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: '\n",
      "                         'User does not have permission to query table '\n",
      "                         'taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, '\n",
      "                         'or perhaps it does not exist in location US.\\n'\n",
      "                         'compiled Code at '\n",
      "                         'target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\\n'\n",
      "                         'In my case, I was set up in a different branch, so '\n",
      "                         'always check the branch you are working on. Change '\n",
      "                         'the '\n",
      "                         '04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml '\n",
      "                         'file in the\\n'\n",
      "                         'sources:\\n'\n",
      "                         '- name: staging\\n'\n",
      "                         'database: your_database_name\\n'\n",
      "                         'If this error will continue when running dbt job, As '\n",
      "                         'for changing the branch for your job, you can use '\n",
      "                         'the ‘Custom Branch’ settings in your dbt Cloud '\n",
      "                         'environment. This allows you to run your job on a '\n",
      "                         'different branch than the default one (usually '\n",
      "                         'main). To do this, you need to:\\n'\n",
      "                         'Go to an environment and select Settings to edit it\\n'\n",
      "                         'Select Only run on a custom branch in General '\n",
      "                         'settings\\n'\n",
      "                         'Enter the name of your custom branch (e.g. HW)\\n'\n",
      "                         'Click Save\\n'\n",
      "                         'Could not parse the dbt project. please check that '\n",
      "                         'the repository contains a valid dbt project\\n'\n",
      "                         'Running the Environment on the master branch causes '\n",
      "                         'this error, you must activate “Only run on a custom '\n",
      "                         'branch” checkbox and specify the branch you are  '\n",
      "                         'working when Environment is setup.'},\n",
      "                {'question': 'Made change to your modelling files and commit '\n",
      "                             'the your development branch, but Job still runs '\n",
      "                             'on old file?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Change to main branch, make a pull request from the '\n",
      "                         'development branch.\\n'\n",
      "                         'Note: this will take you to github.\\n'\n",
      "                         'Approve the merging and rerun you job, it would work '\n",
      "                         'as planned now'},\n",
      "                {'question': 'Setup - I’ve set Github and Bigquery to dbt '\n",
      "                             'successfully. Why nothing showed in my Develop '\n",
      "                             'tab?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Before you can develop some data model on dbt, you '\n",
      "                         'should create development environment and set some '\n",
      "                         'parameter on it. After the model being developed, we '\n",
      "                         'should also create deployment environment to create '\n",
      "                         'and run some jobs.'},\n",
      "                {'question': 'Prefect Agent retrieving runs from queue '\n",
      "                             'sometimes fails with httpx.LocalProtocolError',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Error Message:\\n'\n",
      "                         'Investigate Sentry error: ProtocolError \"Invalid '\n",
      "                         'input ConnectionInputs.SEND_HEADERS in state '\n",
      "                         'ConnectionState.CLOSED\"\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'reference\\n'\n",
      "                         'Run it again because it happens sometimes. Or wait a '\n",
      "                         'few minutes, it will continue.'},\n",
      "                {'question': 'BigQuery returns an error when i try to run ‘dbt '\n",
      "                             'run’:',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'My taxi data was loaded into gcs with '\n",
      "                         'etl_web_to_gcs.py script that converts csv data into '\n",
      "                         'parquet. Then I placed raw data trips into external '\n",
      "                         'tables and when I executed dbt run I got an error '\n",
      "                         \"message: Parquet column 'passenger_count' has type \"\n",
      "                         'INT64 which does not match the target cpp_type '\n",
      "                         'DOUBLE. It is because several columns in files have '\n",
      "                         'different formats of data.\\n'\n",
      "                         \"When I added df[col] = df[col].astype('Int64') \"\n",
      "                         'transformation to the columns: passenger_count, '\n",
      "                         'payment_type, RatecodeID, VendorID, trip_type it '\n",
      "                         'went ok. Several people also faced this error and '\n",
      "                         'more about it you can read on the slack channel.'},\n",
      "                {'question': 'Running dbt run --models stg_green_tripdata '\n",
      "                             \"--var 'is_test_run: false' is not returning \"\n",
      "                             'anything:',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Use the syntax below instead if the code in the '\n",
      "                         'tutorial is not working.\\n'\n",
      "                         'dbt run --select stg_green_tripdata --vars '\n",
      "                         '\\'{\"is_test_run\": false}\\''},\n",
      "                {'question': \"DBT - Error: No module named 'pytz' while \"\n",
      "                             'setting up dbt with docker',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Following dbt with BigQuery on Docker readme.md, '\n",
      "                         'after `docker-compose build` and `docker-compose run '\n",
      "                         'dbt-bq-dtc init`, encountered error '\n",
      "                         \"`ModuleNotFoundError: No module named 'pytz'`\\n\"\n",
      "                         'Solution:\\n'\n",
      "                         'Add `RUN python -m pip install --no-cache pytz` in '\n",
      "                         'the Dockerfile under `FROM --platform=$build_for '\n",
      "                         'python:3.9.9-slim-bullseye as base`'},\n",
      "                {'question': '\\u200b\\u200bVS Code: NoPermissions '\n",
      "                             '(FileSystemError): Error: EACCES: permission '\n",
      "                             'denied (linux)',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'If you have problems editing dbt_project.yml when '\n",
      "                         'using Docker after ‘docker-compose run dbt-bq-dtc '\n",
      "                         'init’, to change profile ‘taxi_rides_ny’ to '\n",
      "                         \"'bq-dbt-workshop’, just run:\\n\"\n",
      "                         'sudo chown -R username path\\n'\n",
      "                         'DBT - Internal Error: Profile should not be None if '\n",
      "                         'loading is completed\\n'\n",
      "                         'When  running dbt debug, change the directory to the '\n",
      "                         'newly created subdirectory (e.g: the newly created '\n",
      "                         '`taxi_rides_ny` directory, which contains the dbt '\n",
      "                         'project).'},\n",
      "                {'question': 'Google Cloud BigQuery Location Problems',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'When running a query on BigQuery sometimes could '\n",
      "                         'appear a this table is not on the specified location '\n",
      "                         'error.\\n'\n",
      "                         'For this problem there is not a straightforward '\n",
      "                         'solution, you need to dig a little, but the problem '\n",
      "                         'could be one of these:\\n'\n",
      "                         'Check the locations of your bucket, datasets and '\n",
      "                         'tables. Make sure they are all on the same one.\\n'\n",
      "                         'Change the query settings to the location you are '\n",
      "                         'in: on the query window select more -> query '\n",
      "                         'settings -> select the location\\n'\n",
      "                         'Check if all the paths you are using in your query '\n",
      "                         'to your tables are correct: you can click on the '\n",
      "                         'table -> details -> and copy the path.'},\n",
      "                {'question': 'DBT Deploy - This dbt Cloud run was cancelled '\n",
      "                             'because a valid dbt project was not found.',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'This happens because we have moved the dbt project '\n",
      "                         'to another directory on our repo.\\n'\n",
      "                         'Or might be that you’re on a different branch than '\n",
      "                         'is expected to be merged from / to.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Go to the projects window on dbt cloud -> settings '\n",
      "                         '-> edit -> and add directory (the extra path to the '\n",
      "                         'dbt project)\\n'\n",
      "                         'For example:\\n'\n",
      "                         '/week5/taxi_rides_ny\\n'\n",
      "                         'Make sure your file explorer path and this Project '\n",
      "                         'settings path matches and there’s no files waiting '\n",
      "                         'to be committed to github if you’re running the job '\n",
      "                         'to deploy to PROD.\\n'\n",
      "                         'And that you had setup the PROD environment to check '\n",
      "                         'in the main branch, or whichever you specified.\\n'\n",
      "                         'In the picture below, I had set it to ella2024 to be '\n",
      "                         'checked as “production-ready” by the “freshness” '\n",
      "                         'check mark at the PROD environment settings. So each '\n",
      "                         'time I merge a branch from something else into '\n",
      "                         'ella2024 and then trigger the PR, the CI check job '\n",
      "                         'would kick-in. But we still do need to Merge and '\n",
      "                         'close the PR manually, I believe, that part is not '\n",
      "                         'automated.\\n'\n",
      "                         'You set up the PROD custom branch (if not default '\n",
      "                         'main) in the Environment setup screen.'},\n",
      "                {'question': 'DBT Deploy + CI - Location Problems on BigQuery',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'When you are creating the pull request and running '\n",
      "                         'the CI, dbt is creating a new schema on BIgQuery. By '\n",
      "                         'default that new schema will be created on ‘US’ '\n",
      "                         'location, if you have your dataset, schemas and '\n",
      "                         'tables on ‘EU’ that will generate an error and the '\n",
      "                         'pull request will not be accepted. To change that '\n",
      "                         'location to ‘EU’ on the connection to BigQuery from '\n",
      "                         'dbt we need to add the location ‘EU’ on the '\n",
      "                         'connection optional settings:\\n'\n",
      "                         'Dbt -> project -> settings -> connection BIgQuery -> '\n",
      "                         'OPtional Settings -> Location -> EU'},\n",
      "                {'question': 'DBT Deploy - Error When trying to run the dbt '\n",
      "                             'project on Prod',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'When running trying to run the dbt project on prod '\n",
      "                         'there is some things you need to do and check on '\n",
      "                         'your own:\\n'\n",
      "                         'First Make the pull request and Merge the branch '\n",
      "                         'into the main.\\n'\n",
      "                         'Make sure you have the latest version, if you made '\n",
      "                         'changes to the repo in another place.\\n'\n",
      "                         'Check if the dbt_project.yml file is accessible to '\n",
      "                         'the project, if not check this solution (Dbt: This '\n",
      "                         'dbt Cloud run was cancelled because a valid dbt '\n",
      "                         'project was not found.).\\n'\n",
      "                         'Check if the name you gave to the dataset on '\n",
      "                         'BigQuery is the same you put on the dataset spot on '\n",
      "                         'the production environment created on dbt cloud.'},\n",
      "                {'question': 'DBT - Error: “404 Not found: Dataset '\n",
      "                             '<dataset_name>:<dbt_schema_name> was not found '\n",
      "                             'in location EU” after building from '\n",
      "                             'stg_green_tripdata.sql',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'In the step in this video (DE Zoomcamp 4.3.1 - Build '\n",
      "                         'the First dbt Models), after creating '\n",
      "                         '`stg_green_tripdata.sql` and clicking `build`, I '\n",
      "                         'encountered an error saying dataset not found in '\n",
      "                         'location EU. The default location for dbt Bigquery '\n",
      "                         'is the US, so when generating the new Bigquery '\n",
      "                         'schema for dbt, unless specified, the schema locates '\n",
      "                         'in the US.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Turns out I forgot to specify Location to be `EU` '\n",
      "                         'when adding connection details.\\n'\n",
      "                         'Develop -> Configure Cloud CLI -> Projects -> '\n",
      "                         'taxi_rides_ny -> (connection) Bigquery -> Edit -> '\n",
      "                         'Location (Optional) -> type `EU` -> Save'},\n",
      "                {'question': 'Homework - Ingesting FHV_20?? data',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Issue: If you’re having problems loading the '\n",
      "                         'FHV_20?? data from the github repo into GCS and then '\n",
      "                         'into BQ (input file not of type parquet), you need '\n",
      "                         'to do two things. First, append the URL Template '\n",
      "                         'link with ‘?raw=true’ like so:\\n'\n",
      "                         'URL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ '\n",
      "                         \"execution_date.strftime(\\\\'%Y-%m\\\\') \"\n",
      "                         '}}.parquet?raw=true\"\\n'\n",
      "                         'Second, update make sure the URL_PREFIX is set to '\n",
      "                         'the following value:\\n'\n",
      "                         '\\n'\n",
      "                         'URL_PREFIX = '\n",
      "                         '\"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\\n'\n",
      "                         'It is critical that you use this link with the '\n",
      "                         'keyword blob. If your link has ‘tree’ here, replace '\n",
      "                         'it. Everything else can stay the same, including the '\n",
      "                         'curl -sSLf command. ‘'},\n",
      "                {'question': 'Homework - Ingesting NYC TLC Data',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'I found out that the easies way to upload datasets '\n",
      "                         'form github for the homework is utilising this '\n",
      "                         'script git_csv_to_gcs.py. Thank you Lidia!!\\n'\n",
      "                         'It is similar to a script that Alexey provided us in '\n",
      "                         '03-data-warehouse/extras/web_to_gcs.py'},\n",
      "                {'question': 'How to set environment variable easily for any '\n",
      "                             'credentials',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'If you have to securely put your credentials for a '\n",
      "                         'project and, probably, push it to a git repository '\n",
      "                         'then the best option is to use an environment '\n",
      "                         'variable\\n'\n",
      "                         'For example for web_to_gcs.py or git_csv_to_gcs.py '\n",
      "                         'we have to set these variables:\\n'\n",
      "                         'GOOGLE_APPLICATION_CREDENTIALS\\n'\n",
      "                         'GCP_GCS_BUCKET\\n'\n",
      "                         'The easises option to do it  is to use .env  '\n",
      "                         '(dotenv).\\n'\n",
      "                         'Install it and add a few lines of code that inject '\n",
      "                         'these variables for your project\\n'\n",
      "                         'pip install python-dotenv\\n'\n",
      "                         'from dotenv import load_dotenv\\n'\n",
      "                         'import os\\n'\n",
      "                         '# Load environment variables from .env file\\n'\n",
      "                         'load_dotenv()\\n'\n",
      "                         '# Now you can access environment variables like '\n",
      "                         'GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\\n'\n",
      "                         'credentials_path = '\n",
      "                         'os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\\n'\n",
      "                         'BUCKET = os.environ.get(\"GCP_GCS_BUCKET\")'},\n",
      "                {'question': 'Invalid date types after Ingesting FHV data '\n",
      "                             'through CSV files: Could not parse '\n",
      "                             \"'pickup_datetime' as a timestamp\",\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'If you uploaded manually the fvh 2019 csv files, you '\n",
      "                         'may face errors regarding date types. Try to create '\n",
      "                         'an the external table in bigquery but define the '\n",
      "                         'pickup_datetime and dropoff_datetime to be strings\\n'\n",
      "                         'CREATE OR REPLACE EXTERNAL TABLE '\n",
      "                         '`gcp_project.trips_data_all.fhv_tripdata`  (\\n'\n",
      "                         'dispatching_base_num STRING,\\n'\n",
      "                         'pickup_datetime STRING,\\n'\n",
      "                         'dropoff_datetime STRING,\\n'\n",
      "                         'PUlocationID STRING,\\n'\n",
      "                         'DOlocationID STRING,\\n'\n",
      "                         'SR_Flag STRING,\\n'\n",
      "                         'Affiliated_base_number STRING\\n'\n",
      "                         ')\\n'\n",
      "                         'OPTIONS (\\n'\n",
      "                         \"format = 'csv',\\n\"\n",
      "                         \"uris = ['gs://bucket/*.csv']\\n\"\n",
      "                         ');\\n'\n",
      "                         'Then when creating the fhv core model in dbt, use '\n",
      "                         'TIMESTAMP(CAST(()) to ensure it first parses as a '\n",
      "                         'string and then convert it to timestamp.\\n'\n",
      "                         'with fhv_tripdata as (\\n'\n",
      "                         \"select * from {{ ref('stg_fhv_tripdata') }}\\n\"\n",
      "                         '),\\n'\n",
      "                         'dim_zones as (\\n'\n",
      "                         \"select * from {{ ref('dim_zones') }}\\n\"\n",
      "                         \"where borough != 'Unknown'\\n\"\n",
      "                         ')\\n'\n",
      "                         'select fhv_tripdata.dispatching_base_num,\\n'\n",
      "                         'TIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS '\n",
      "                         'STRING)) AS pickup_datetime,\\n'\n",
      "                         'TIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS '\n",
      "                         'STRING)) AS dropoff_datetime,'},\n",
      "                {'question': 'Invalid data types after Ingesting FHV data '\n",
      "                             'through parquet files: Could not parse SR_Flag '\n",
      "                             'as Float64,Couldn’t parse datetime column as '\n",
      "                             'timestamp,couldn’t handle NULL values in '\n",
      "                             'PULocationID,DOLocationID',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'If you uploaded manually the fvh 2019 parquet files '\n",
      "                         'manually after downloading from '\n",
      "                         'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet '\n",
      "                         'you may face errors regarding date types while '\n",
      "                         'loading the data in a landing table (say '\n",
      "                         'fhv_tripdata). Try to create an the external table '\n",
      "                         'with the schema defines as following and load each '\n",
      "                         'month in a loop.\\n'\n",
      "                         '-----Correct load with schema defination----will not '\n",
      "                         'throw error----------------------\\n'\n",
      "                         'CREATE OR REPLACE EXTERNAL TABLE '\n",
      "                         '`dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` '\n",
      "                         '(\\n'\n",
      "                         'dispatching_base_num STRING,\\n'\n",
      "                         'pickup_datetime TIMESTAMP,\\n'\n",
      "                         'dropoff_datetime TIMESTAMP,\\n'\n",
      "                         'PUlocationID FLOAT64,\\n'\n",
      "                         'DOlocationID FLOAT64,\\n'\n",
      "                         'SR_Flag FLOAT64,\\n'\n",
      "                         'Affiliated_base_number STRING\\n'\n",
      "                         ')\\n'\n",
      "                         'OPTIONS (\\n'\n",
      "                         \"format = 'PARQUET',\\n\"\n",
      "                         \"uris = ['gs://project id/fhv_2019_8.parquet']\\n\"\n",
      "                         ');\\n'\n",
      "                         \"Can Also USE  uris = ['gs://project \"\n",
      "                         \"id/fhv_2019_*.parquet'] (THIS WILL remove the need \"\n",
      "                         'for the loop and can be done for all month in single '\n",
      "                         'RUN )\\n'\n",
      "                         '– THANKYOU FOR THIS –'},\n",
      "                {'question': 'Google Looker Studio - you have used up your '\n",
      "                             '30-day trial',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'When accessing Looker Studio through the Google '\n",
      "                         'Cloud Project console, you may be prompted to '\n",
      "                         'subscribe to the Pro version and receive the '\n",
      "                         'following errors:\\n'\n",
      "                         'Instead, navigate to '\n",
      "                         'https://lookerstudio.google.com/navigation/reporting '\n",
      "                         'which will take you to the free version.'},\n",
      "                {'question': 'How does dbt handle dependencies between models?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Ans: Dbt provides a mechanism called \"ref\" to manage '\n",
      "                         'dependencies between models. By referencing other '\n",
      "                         'models using the \"ref\" keyword in SQL, dbt '\n",
      "                         'automatically understands the dependencies and '\n",
      "                         'ensures the correct execution order.\\n'\n",
      "                         'Loading FHV Data goes into slumber using Mage?\\n'\n",
      "                         'Try loading the data using jupyter notebooks in a '\n",
      "                         'local environment. There might be bandwidth issues '\n",
      "                         'with Mage.\\n'\n",
      "                         'Load the data into a pandas dataframe using the '\n",
      "                         'urls, make necessary transformations, upload the gcp '\n",
      "                         'bucket / alternatively download the parquet/csv '\n",
      "                         'files locally and then upload to GCP manually.\\n'\n",
      "                         'Region Mismatch in DBT and BigQuery\\n'\n",
      "                         'If you are using the datasets copied into BigQuery '\n",
      "                         'from BigQuery public datasets, the region will be '\n",
      "                         'set as US by default and hence it is much easier to '\n",
      "                         'set your dbt profile location as US while '\n",
      "                         'transforming the tables and views. \\n'\n",
      "                         'You can change the location as follows:'},\n",
      "                {'question': 'What is the fastest way to upload taxi data to '\n",
      "                             'dbt-postgres?',\n",
      "                 'section': 'Module 4: analytics engineering with dbt',\n",
      "                 'text': 'Use the PostgreSQL COPY FROM feature that is '\n",
      "                         'compatible with csv files\\n'\n",
      "                         'COPY table_name [ ( column_name [, ...] ) ]\\n'\n",
      "                         \"FROM { 'filename' | PROGRAM 'command' | STDIN }\\n\"\n",
      "                         '[ [ WITH ] ( option [, ...] ) ]\\n'\n",
      "                         '[ WHERE condition ]'},\n",
      "                {'question': 'When configuring the profiles.yml file for '\n",
      "                             'dbt-postgres with jinja templates with '\n",
      "                             'environment variables, I\\'m getting \"Credentials '\n",
      "                             'in profile \"PROFILE_NAME\", target: \\'dev\\', '\n",
      "                             \"invalid: '5432'is not of type 'integer'\",\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Update the line:\\nWith:'},\n",
      "                {'question': 'Setting up Java and Spark (with PySpark) on '\n",
      "                             'Linux (Alternative option using SDKMAN)',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Install SDKMAN:\\n'\n",
      "                         'curl -s \"https://get.sdkman.io\" | bash\\n'\n",
      "                         'source \"$HOME/.sdkman/bin/sdkman-init.sh\"\\n'\n",
      "                         'Using SDKMAN, install Java 11 and Spark 3.3.2:\\n'\n",
      "                         'sdk install java 11.0.22-tem\\n'\n",
      "                         'sdk install spark 3.3.2\\n'\n",
      "                         'Open a new terminal or run the following in the same '\n",
      "                         'shell:\\n'\n",
      "                         'source \"$HOME/.sdkman/bin/sdkman-init.sh\"\\n'\n",
      "                         'Verify the locations and versions of Java and Spark '\n",
      "                         'that were installed:\\n'\n",
      "                         'echo $JAVA_HOME\\n'\n",
      "                         'java -version\\n'\n",
      "                         'echo $SPARK_HOME\\n'\n",
      "                         'spark-submit --version'},\n",
      "                {'question': 'PySpark - Setting Spark up in Google Colab',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'If you’re seriously struggling to set things up '\n",
      "                         '\"locally\" (here locally meaning non/partly-managed '\n",
      "                         'environment like own laptop, a VM or Codespaces) you '\n",
      "                         'can use the following guide to use Spark in Google '\n",
      "                         'Colab:\\n'\n",
      "                         'https://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\\n'\n",
      "                         'Starter notebook:\\n'\n",
      "                         'https://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\\n'\n",
      "                         'It’s advisable to spend some time setting things up '\n",
      "                         'locally rather than jumping right into this '\n",
      "                         'solution.'},\n",
      "                {'question': 'Spark-shell: unable to load native-hadoop '\n",
      "                             'library for platform - Windows',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'If after installing Java (either jdk or openjdk), '\n",
      "                         'Hadoop and Spark, and setting the corresponding '\n",
      "                         'environment variables you find the following error '\n",
      "                         'when spark-shell is run at CMD:\\n'\n",
      "                         'java.lang.IllegalAccessError: class '\n",
      "                         'org.apache.spark.storage.StorageUtils$ (in unnamed '\n",
      "                         'module @0x3c947bc5) cannot access class '\n",
      "                         'sun.nio.ch.DirectBuffer (in module java.base) '\n",
      "                         'because module java.base does not export sun.nio.ch '\n",
      "                         'to unnamed\\n'\n",
      "                         'module @0x3c947bc5\\n'\n",
      "                         'Solution: Java 17 or 19 is not supported by Spark. '\n",
      "                         'Spark 3.x: requires Java 8/11/16. Install Java 11 '\n",
      "                         'from the website provided in the windows.md setup '\n",
      "                         'file.'},\n",
      "                {'question': 'PySpark - Python was not found; run without '\n",
      "                             'arguments to install from the Microsoft Store, '\n",
      "                             'or disable this shortcut from Settings > Manage '\n",
      "                             'App Execution Aliases.',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'I found this error while executing the user defined '\n",
      "                         'function in Spark (crazy_stuff_udf). I am working on '\n",
      "                         'Windows and using conda. After following the setup '\n",
      "                         'instructions, I found that the PYSPARK_PYTHON '\n",
      "                         'environment variable was not set correctly, given '\n",
      "                         'that conda has different python paths for each '\n",
      "                         'environment.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'pip install findspark on the command line inside '\n",
      "                         'proper environment\\n'\n",
      "                         'Add to the top of the script\\n'\n",
      "                         'import findspark\\n'\n",
      "                         'findspark.init()'},\n",
      "                {'question': 'PySpark - TypeError: code() argument 13 must be '\n",
      "                             'str, not int  , while executing `import '\n",
      "                             'pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'This is because Python 3.11 has some inconsistencies '\n",
      "                         'with such an old version of Spark. The solution is a '\n",
      "                         'downgrade in the Python version. Python 3.9 using a '\n",
      "                         'conda environment takes care of it. Or install newer '\n",
      "                         'PySpark >= 3.5.1 works for me (Ella) [source].'},\n",
      "                {'question': 'Java+Spark - Easy setup with miniconda env '\n",
      "                             '(worked on MacOS)',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'If anyone is a Pythonista or becoming one (which you '\n",
      "                         'will essentially be one along this journey), and '\n",
      "                         'desires to have all python dependencies under same '\n",
      "                         'virtual environment (e.g. conda) as done with '\n",
      "                         'prefect and previous exercises, simply follow these '\n",
      "                         'steps\\n'\n",
      "                         'Install OpenJDK 11,\\n'\n",
      "                         'on MacOS: $ brew install java11\\n'\n",
      "                         'Add export '\n",
      "                         'PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\\n'\n",
      "                         'to ~/.bashrc or ~/zshrc\\n'\n",
      "                         'Activate working environment (by pipenv / poetry / '\n",
      "                         'conda)\\n'\n",
      "                         'Run $ pip install pyspark\\n'\n",
      "                         'Work with exercises as normal\\n'\n",
      "                         'All default commands of spark will be also available '\n",
      "                         'at shell session under activated enviroment.\\n'\n",
      "                         'Hope this can help!\\n'\n",
      "                         'P.s. you won’t need findspark to firstly '\n",
      "                         'initialize.\\n'\n",
      "                         'Py4J - Py4JJavaError: An error occurred while '\n",
      "                         'calling (...)  java.net.ConnectException: Connection '\n",
      "                         'refused: no further information;\\n'\n",
      "                         \"If you're getting `Py4JavaError` with a generic root \"\n",
      "                         'cause, such as the described above (Connection '\n",
      "                         \"refused: no further information). You're most likely \"\n",
      "                         'using incompatible versions of the JDK or Python '\n",
      "                         'with Spark.\\n'\n",
      "                         'As of the current latest Spark version (3.5.0), it '\n",
      "                         'supports JDK 8 / 11 / 17. All of which can be easily '\n",
      "                         'installed with SDKMan! on macOS or Linux '\n",
      "                         'environments\\n'\n",
      "                         '\\n'\n",
      "                         '$ sdk install java 17.0.10-librca\\n'\n",
      "                         '$ sdk install spark 3.5.0\\n'\n",
      "                         '$ sdk install hadoop 3.3.5\\n'\n",
      "                         'As PySpark 3.5.0 supports Python 3.8+ make sure '\n",
      "                         \"you're setting up your virtualenv with either 3.8 / \"\n",
      "                         '3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 '\n",
      "                         'for now as not all libs in the '\n",
      "                         'data-science/engineering ecosystem are fully package '\n",
      "                         'for that)\\n'\n",
      "                         '\\n'\n",
      "                         '\\n'\n",
      "                         '$ conda create -n ENV_NAME python=3.11\\n'\n",
      "                         '$ conda activate ENV_NAME\\n'\n",
      "                         '$ pip install pyspark==3.5.0\\n'\n",
      "                         'This setup makes installing `findspark` and the '\n",
      "                         'likes of it unnecessary. Happy coding.\\n'\n",
      "                         'Py4J - Py4JJavaError: An error occurred while '\n",
      "                         'calling o54.parquet. Or any kind of Py4JJavaError '\n",
      "                         \"that show up after run df.write.parquet('zones')(On \"\n",
      "                         'window)\\n'\n",
      "                         'This assume you already correctly set up the PATH in '\n",
      "                         'the nano ~/.bashrc\\n'\n",
      "                         'Here my\\n'\n",
      "                         'export JAVA_HOME=\"/c/tools/jdk-11.0.21\"\\n'\n",
      "                         'export PATH=\"${JAVA_HOME}/bin:${PATH}\"\\n'\n",
      "                         'export HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\\n'\n",
      "                         'export PATH=\"${HADOOP_HOME}/bin:${PATH}\"\\n'\n",
      "                         'export '\n",
      "                         'SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\\n'\n",
      "                         'export PATH=\"${SPARK_HOME}/bin:${PATH}\"\\n'\n",
      "                         'export '\n",
      "                         'PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\n'\n",
      "                         'export '\n",
      "                         'PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\n'\n",
      "                         'You also need to add environment variables correctly '\n",
      "                         'which paths to java jdk, spark and hadoop through\\n'\n",
      "                         'Go to Stephenlaye2/winutils3.3.0: winutils.exe '\n",
      "                         'hadoop.dll and hdfs.dll binaries for hadoop windows '\n",
      "                         '(github.com), download the right winutils for '\n",
      "                         'hadoop-3.2.0. Then create a new folder,bin and put '\n",
      "                         'every thing in side to make a '\n",
      "                         '/c/tools/hadoop-3.2.0/bin(You might not need to do '\n",
      "                         'this, but after testing it without the /bin I could '\n",
      "                         'not make it to work)\\n'\n",
      "                         'Then follow the solution in this video: How To '\n",
      "                         'Resolve Issue with Writing DataFrame to Local File | '\n",
      "                         'winutils | msvcp100.dll (youtube.com)\\n'\n",
      "                         'Remember to restart IDE and computer, After the '\n",
      "                         'error An error occurred while calling o54.parquet.  '\n",
      "                         'is fixed but new errors like o31.parquet. Or '\n",
      "                         'o35.parquet. appear.'},\n",
      "                {'question': 'lsRuntimeError: Java gateway process exited '\n",
      "                             'before sending its port number',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'After installing all including pyspark (and it is '\n",
      "                         'successfully imported), but then running this script '\n",
      "                         'on the jupyter notebook\\n'\n",
      "                         'import pyspark\\n'\n",
      "                         'from pyspark.sql import SparkSession\\n'\n",
      "                         'spark = SparkSession.builder \\\\\\n'\n",
      "                         '.master(\"local[*]\") \\\\\\n'\n",
      "                         \".appName('test') \\\\\\n\"\n",
      "                         '.getOrCreate()\\n'\n",
      "                         'df = spark.read \\\\\\n'\n",
      "                         '.option(\"header\", \"true\") \\\\\\n'\n",
      "                         \".csv('taxi+_zone_lookup.csv')\\n\"\n",
      "                         'df.show()\\n'\n",
      "                         'it gives the error:\\n'\n",
      "                         'RuntimeError: Java gateway process exited before '\n",
      "                         'sending its port number\\n'\n",
      "                         '✅The solution (for me) was:\\n'\n",
      "                         'pip install findspark on the command line and then\\n'\n",
      "                         'Add\\n'\n",
      "                         'import findspark\\n'\n",
      "                         'findspark.init()\\n'\n",
      "                         'to the top of the script.\\n'\n",
      "                         'Another possible solution is:\\n'\n",
      "                         'Check that pyspark is pointing to the correct '\n",
      "                         'location.\\n'\n",
      "                         'Run pyspark.__file__. It should be list /home/<your '\n",
      "                         'user '\n",
      "                         'name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py '\n",
      "                         'if you followed the videos.\\n'\n",
      "                         'If it is pointing to your python site-packages '\n",
      "                         'remove the pyspark directory there and check that '\n",
      "                         'you have added the correct exports to you .bashrc '\n",
      "                         'file and that there are not any other exports which '\n",
      "                         'might supersede the ones provided in the course '\n",
      "                         'content.\\n'\n",
      "                         'To add to the solution above, if the errors persist '\n",
      "                         'in regards to setting the correct path for spark,  '\n",
      "                         'an alternative solution for permanent path setting '\n",
      "                         'solve the error is  to set environment variables on '\n",
      "                         'system and user environment variables following this '\n",
      "                         'tutorial: Install Apache PySpark on Windows PC | '\n",
      "                         'Apache Spark Installation Guide\\n'\n",
      "                         'Once everything is installed, skip to 7:14 to set up '\n",
      "                         'environment variables. This allows for the '\n",
      "                         'environment variables to be set permanently.'},\n",
      "                {'question': 'Module Not Found Error in Jupyter Notebook .',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Even after installing pyspark correctly on linux '\n",
      "                         'machine (VM ) as per course instructions, faced a '\n",
      "                         'module not found error in jupyter notebook .\\n'\n",
      "                         'The solution which worked for me(use following in '\n",
      "                         'jupyter notebook) :\\n'\n",
      "                         '!pip install findspark\\n'\n",
      "                         'import findspark\\n'\n",
      "                         'findspark.init()\\n'\n",
      "                         'Thereafter , import pyspark and create spark '\n",
      "                         'contex<<t as usual\\n'\n",
      "                         'None of the solutions above worked for me till I ran '\n",
      "                         '!pip3 install pyspark instead !pip install pyspark.\\n'\n",
      "                         'Filter based on conditions based on multiple '\n",
      "                         'columns\\n'\n",
      "                         'from pyspark.sql.functions import col\\n'\n",
      "                         'new_final.filter((new_final.a_zone==\"Murray Hill\") & '\n",
      "                         '(new_final.b_zone==\"Midwood\")).show()\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'Py4JJavaError - ModuleNotFoundError: No module '\n",
      "                             \"named 'py4j'` while executing `import pyspark`\",\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'You need to look for the Py4J file and note the '\n",
      "                         'version of the filename. Once you know the version, '\n",
      "                         'you can update the export command accordingly, this '\n",
      "                         'is how you check yours:\\n'\n",
      "                         '` ls ${SPARK_HOME}/python/lib/ ` and then you add it '\n",
      "                         'in the export command, mine was:\\n'\n",
      "                         'export '\n",
      "                         'PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\n'\n",
      "                         'Make sure that the version under '\n",
      "                         '`${SPARK_HOME}/python/lib/` matches the filename of '\n",
      "                         'py4j or you will encounter `ModuleNotFoundError: No '\n",
      "                         \"module named 'py4j'` while executing `import \"\n",
      "                         'pyspark`.\\n'\n",
      "                         'For instance, if the file under '\n",
      "                         '`${SPARK_HOME}/python/lib/` was '\n",
      "                         '`py4j-0.10.9.3-src.zip`.\\n'\n",
      "                         'Then the export PYTHONPATH statement above should be '\n",
      "                         'changed to `export '\n",
      "                         'PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` '\n",
      "                         'appropriately.\\n'\n",
      "                         'Additionally, you can check for the version of '\n",
      "                         '‘py4j’ of the spark you’re using from here and '\n",
      "                         'update as mentioned above.\\n'\n",
      "                         '~ Abhijit Chakraborty: Sometimes, even with adding '\n",
      "                         'the correct version of py4j might not solve the '\n",
      "                         'problem. Simply run pip install py4j and problem '\n",
      "                         'should be resolved.'},\n",
      "                {'question': 'Py4J Error - ModuleNotFoundError: No module '\n",
      "                             \"named 'py4j' (Solve with latest version)\",\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'If below does not work, then download the latest '\n",
      "                         'available py4j version with\\n'\n",
      "                         'conda install -c conda-forge py4j\\n'\n",
      "                         'Take care of the latest version number in the '\n",
      "                         'website to replace appropriately.\\n'\n",
      "                         'Now add\\n'\n",
      "                         'export '\n",
      "                         'PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\n'\n",
      "                         'export '\n",
      "                         'PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\\n'\n",
      "                         'in your  .bashrc file.'},\n",
      "                {'question': 'Exception: Jupyter command `jupyter-notebook` '\n",
      "                             'not found.',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Even after we have exported our paths correctly you '\n",
      "                         'may find that  even though Jupyter is installed you '\n",
      "                         'might not have Jupyter Noteboopgak for one reason or '\n",
      "                         'another. Full instructions are found here (for my '\n",
      "                         'walkthrough) or here (where I got the original '\n",
      "                         'instructions from) but are included below. These '\n",
      "                         'instructions include setting up a virtual '\n",
      "                         'environment (handy if you are on your own machine '\n",
      "                         'doing this and not a VM):\\n'\n",
      "                         'Full steps:\\n'\n",
      "                         'Update and upgrade packages:\\n'\n",
      "                         'sudo apt update && sudo apt -y upgrade\\n'\n",
      "                         'Install Python:\\n'\n",
      "                         'sudo apt install python3-pip python3-dev\\n'\n",
      "                         'Install Python virtualenv:\\n'\n",
      "                         'sudo -H pip3 install --upgrade pip\\n'\n",
      "                         'sudo -H pip3 install virtualenv\\n'\n",
      "                         'Create a Python Virtual Environment:\\n'\n",
      "                         'mkdir notebook\\n'\n",
      "                         'cd notebook\\n'\n",
      "                         'virtualenv jupyterenv\\n'\n",
      "                         'source jupyterenv/bin/activate\\n'\n",
      "                         'Install Jupyter Notebook:\\n'\n",
      "                         'pip install jupyter\\n'\n",
      "                         'Run Jupyter Notebook:\\n'\n",
      "                         'jupyter notebook'},\n",
      "                {'question': 'Error java.io.FileNotFoundException',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Code executed:\\n'\n",
      "                         'df = spark.read.parquet(pq_path)\\n'\n",
      "                         '… some operations on df …\\n'\n",
      "                         'df.write.parquet(pq_path, mode=\"overwrite\")\\n'\n",
      "                         'java.io.FileNotFoundException: File '\n",
      "                         'file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet '\n",
      "                         'does not exist\\n'\n",
      "                         'The problem is that Sparks performs lazy '\n",
      "                         'transformations, so the actual action that trigger '\n",
      "                         'the job is df.write, which does delete the parquet '\n",
      "                         'files that is trying to read (mode=”overwrite”)\\n'\n",
      "                         '✅Solution: Write to a different directorydf\\n'\n",
      "                         'df.write.parquet(pq_path_temp, mode=\"overwrite\")'},\n",
      "                {'question': 'Hadoop - FileNotFoundException: Hadoop bin '\n",
      "                             'directory does not exist , when trying to write '\n",
      "                             '(Windows)',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'You need to create the Hadoop /bin directory '\n",
      "                         'manually and add the downloaded files in there, '\n",
      "                         'since the shell script provided for Windows '\n",
      "                         'installation just puts them in '\n",
      "                         '/c/tools/hadoop-3.2.0/ .'},\n",
      "                {'question': 'Which type of SQL is used in Spark? Postgres? '\n",
      "                             'MySQL? SQL Server?',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Actually Spark SQL is one independent “type” of SQL '\n",
      "                         '- Spark SQL.\\n'\n",
      "                         'The several SQL providers are very similar:\\n'\n",
      "                         'SELECT [attributes]\\n'\n",
      "                         'FROM [table]\\n'\n",
      "                         'WHERE [filter]\\n'\n",
      "                         'GROUP BY [grouping attributes]\\n'\n",
      "                         'HAVING [filtering the groups]\\n'\n",
      "                         'ORDER BY [attribute to order]\\n'\n",
      "                         '(INNER/FULL/LEFT/RIGHT) JOIN [table2]\\n'\n",
      "                         'ON [attributes table joining table2] (...)\\n'\n",
      "                         'What differs the most between several SQL providers '\n",
      "                         'are built-in functions.\\n'\n",
      "                         'For Built-in Spark SQL function check this link: '\n",
      "                         'https://spark.apache.org/docs/latest/api/sql/index.html\\n'\n",
      "                         'Extra information on SPARK SQL :\\n'\n",
      "                         'https://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.'},\n",
      "                {'question': 'The spark viewer on localhost:4040 was not '\n",
      "                             'showing the current run',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': '✅Solution: I had two notebooks running, and the one '\n",
      "                         'I wanted to look at had opened a port on '\n",
      "                         'localhost:4041.\\n'\n",
      "                         'If a port is in use, then Spark uses the next '\n",
      "                         'available port number. It can be even 4044. Clean up '\n",
      "                         'after yourself when a port does not work or a '\n",
      "                         'container does not run.\\n'\n",
      "                         'You can run spark.sparkContext.uiWebUrl\\n'\n",
      "                         'and result will be some like\\n'\n",
      "                         \"'http://172.19.10.61:4041'\"},\n",
      "                {'question': 'Java - java.lang.NoSuchMethodError: '\n",
      "                             'sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner '\n",
      "                             'Error during repartition call (conda pyspark '\n",
      "                             'installation)',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': '✅Solution: replace Java Developer Kit 11 with Java '\n",
      "                         'Developer Kit 8.\\n'\n",
      "                         'Java - RuntimeError: Java gateway process exited '\n",
      "                         'before sending its port number\\n'\n",
      "                         'Shows java_home is not set on the notebook log\\n'\n",
      "                         'https://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\\n'\n",
      "                         'https://twitter.com/drkrishnaanand/status/1765423415878463839'},\n",
      "                {'question': 'Spark fails when reading from BigQuery and using '\n",
      "                             '`.show()` on `SELECT` queries',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': '✅I got it working using '\n",
      "                         '`gcs-connector-hadoop-2.2.5-shaded.jar` and Spark '\n",
      "                         '3.1\\n'\n",
      "                         'I also added the google_credentials.json and .p12 to '\n",
      "                         'auth with gcs. These files are downloadable from GCP '\n",
      "                         'Service account.\\n'\n",
      "                         'To create the SparkSession:\\n'\n",
      "                         \"spark = SparkSession.builder.master('local[*]') \\\\\\n\"\n",
      "                         \".appName('spark-read-from-bigquery') \\\\\\n\"\n",
      "                         \".config('BigQueryProjectId','razor-project-xxxxxxx) \"\n",
      "                         '\\\\\\n'\n",
      "                         \".config('BigQueryDatasetLocation','de_final_data') \"\n",
      "                         '\\\\\\n'\n",
      "                         \".config('parentProject','razor-project-xxxxxxx) \\\\\\n\"\n",
      "                         '.config(\"google.cloud.auth.service.account.enable\", '\n",
      "                         '\"true\") \\\\\\n'\n",
      "                         '.config(\"credentialsFile\", '\n",
      "                         '\"google_credentials.json\") \\\\\\n'\n",
      "                         '.config(\"GcpJsonKeyFile\", \"google_credentials.json\") '\n",
      "                         '\\\\\\n'\n",
      "                         '.config(\"spark.driver.memory\", \"4g\") \\\\\\n'\n",
      "                         '.config(\"spark.executor.memory\", \"2g\") \\\\\\n'\n",
      "                         '.config(\"spark.memory.offHeap.enabled\",True) \\\\\\n'\n",
      "                         '.config(\"spark.memory.offHeap.size\",\"5g\") \\\\\\n'\n",
      "                         \".config('google.cloud.auth.service.account.json.keyfile', \"\n",
      "                         '\"google_credentials.json\") \\\\\\n'\n",
      "                         '.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") '\n",
      "                         '\\\\\\n'\n",
      "                         '.config(\"fs.gs.impl\", '\n",
      "                         '\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") '\n",
      "                         '\\\\\\n'\n",
      "                         '.config(\"fs.AbstractFileSystem.gs.impl\", '\n",
      "                         '\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\\n'\n",
      "                         '.getOrCreate()'},\n",
      "                {'question': 'Spark BigQuery connector Automatic configuration',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'While creating a SparkSession using the config '\n",
      "                         'spark.jars.packages as '\n",
      "                         'com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\\n'\n",
      "                         'spark = '\n",
      "                         'SparkSession.builder.master(\\'local\\').appName(\\'bq\\').config(\"spark.jars.packages\", '\n",
      "                         '\"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\\n'\n",
      "                         'automatically downloads the required dependency jars '\n",
      "                         'and configures the connector, removing the need to '\n",
      "                         'manage this dependency. More details available here'},\n",
      "                {'question': 'Spark Cloud Storage connector',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Link to Slack Thread : has anyone figured out how to '\n",
      "                         'read from GCP data lake instead of downloading all '\n",
      "                         'the taxi data again?\\n'\n",
      "                         'There’s a few extra steps to go into reading from '\n",
      "                         'GCS with PySpark\\n'\n",
      "                         '1.)  IMPORTANT: Download the Cloud Storage connector '\n",
      "                         'for Hadoop here: '\n",
      "                         'https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\\n'\n",
      "                         'As the name implies, this .jar file is what '\n",
      "                         'essentially connects PySpark with your GCS\\n'\n",
      "                         '2.) Move the .jar file to your Spark file directory. '\n",
      "                         'I installed Spark using homebrew on my MacOS machine '\n",
      "                         'and I had to create a /jars directory under '\n",
      "                         '\"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my '\n",
      "                         'spark dir is located)\\n'\n",
      "                         '3.) In your Python script, there are a few extra '\n",
      "                         'classes you’ll have to import:\\n'\n",
      "                         'import pyspark\\n'\n",
      "                         'from pyspark.sql import SparkSession\\n'\n",
      "                         'from pyspark.conf import SparkConf\\n'\n",
      "                         'from pyspark.context import SparkContext\\n'\n",
      "                         '4.) You must set up your configurations before '\n",
      "                         'building your SparkSession. Here’s my code snippet:\\n'\n",
      "                         'conf = SparkConf() \\\\\\n'\n",
      "                         \".setMaster('local[*]') \\\\\\n\"\n",
      "                         \".setAppName('test') \\\\\\n\"\n",
      "                         '.set(\"spark.jars\", '\n",
      "                         '\"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") '\n",
      "                         '\\\\\\n'\n",
      "                         '.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", '\n",
      "                         '\"true\") \\\\\\n'\n",
      "                         '.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", '\n",
      "                         '\"path/to/google_credentials.json\")\\n'\n",
      "                         'sc = SparkContext(conf=conf)\\n'\n",
      "                         'sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  '\n",
      "                         '\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\n'\n",
      "                         'sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", '\n",
      "                         '\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\n'\n",
      "                         'sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", '\n",
      "                         '\"path/to/google_credentials.json\")\\n'\n",
      "                         'sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", '\n",
      "                         '\"true\")\\n'\n",
      "                         '5.) Once you run that, build your SparkSession with '\n",
      "                         'the new parameters we’d just instantiated in the '\n",
      "                         'previous step:\\n'\n",
      "                         'spark = SparkSession.builder \\\\\\n'\n",
      "                         '.config(conf=sc.getConf()) \\\\\\n'\n",
      "                         '.getOrCreate()\\n'\n",
      "                         '6.) Finally, you’re able to read your files straight '\n",
      "                         'from GCS!\\n'\n",
      "                         'df_green = '\n",
      "                         'spark.read.parquet(\"gs://{BUCKET}/green/202*/\")'},\n",
      "                {'question': 'How can I read a small number of rows from the '\n",
      "                             'parquet file directly?',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'from pyarrow.parquet import ParquetFile\\n'\n",
      "                         \"pf = ParquetFile('fhvhv_tripdata_2021-01.parquet')\\n\"\n",
      "                         '#pyarrow builds tables, not dataframes\\n'\n",
      "                         'tbl_small = next(pf.iter_batches(batch_size = '\n",
      "                         '1000))\\n'\n",
      "                         '#this function converts the table to a dataframe of '\n",
      "                         'manageable size\\n'\n",
      "                         'df = tbl_small.to_pandas()\\n'\n",
      "                         'Alternatively without PyArrow:\\n'\n",
      "                         'df = '\n",
      "                         \"spark.read.parquet('fhvhv_tripdata_2021-01.parquet')\\n\"\n",
      "                         \"df1 = df.sort('DOLocationID').limit(1000)\\n\"\n",
      "                         'pdf = df1.select(\"*\").toPandas()\\n'\n",
      "                         'gcsu'},\n",
      "                {'question': 'DataType error when creating Spark DataFrame '\n",
      "                             'with a specified schema?',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Probably you’ll encounter this if you followed the '\n",
      "                         'video ‘5.3.1 - First Look at Spark/PySpark’ and used '\n",
      "                         'the parquet file from the TLC website (csv was used '\n",
      "                         'in the video).\\n'\n",
      "                         'When defining the schema, the PULocation and '\n",
      "                         'DOLocationID are defined as IntegerType. This will '\n",
      "                         'cause an error because the Parquet file is INT64 and '\n",
      "                         'you’ll get an error like:\\n'\n",
      "                         'Parquet column cannot be converted in file [...] '\n",
      "                         'Column [...] Expected: int, Found: INT64\\n'\n",
      "                         'Change the schema definition from IntegerType to '\n",
      "                         'LongType and it should work'},\n",
      "                {'question': 'Remove white spaces from column names in Pyspark',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'df_finalx=df_finalw.select([col(x).alias(x.replace(\" '\n",
      "                         '\",\"\")) for x in df_finalw.columns])\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': \"AttributeError: 'DataFrame' object has no \"\n",
      "                             \"attribute 'iteritems'\",\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'This error comes up on the Spark video 5.3.1 - First '\n",
      "                         'Look at Spark/PySpark,\\n'\n",
      "                         'because as at the creation of the video, 2021 data '\n",
      "                         'was the most recent which utilised csv files but as '\n",
      "                         'at now its parquet.\\n'\n",
      "                         'So when you run the command '\n",
      "                         'spark.createDataFrame(df1_pandas).show(),\\n'\n",
      "                         'You get the Attribute error. This is caused by the '\n",
      "                         'pandas version 2.0.0 which seems incompatible with '\n",
      "                         'Spark 3.3.2, so to fix it you have to downgrade '\n",
      "                         'pandas to 1.5.3 using the command pip install -U '\n",
      "                         'pandas==1.5.3\\n'\n",
      "                         'Another option is adding the following after '\n",
      "                         'importing pandas, if one does not want to downgrade '\n",
      "                         'pandas version (source) :\\n'\n",
      "                         'pd.DataFrame.iteritems = pd.DataFrame.items\\n'\n",
      "                         'Note that this problem is solved with Spark versions '\n",
      "                         'from 3.4.1'},\n",
      "                {'question': \"AttributeError: 'DataFrame' object has no \"\n",
      "                             \"attribute 'iteritems'\",\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Another alternative is to install pandas 2.0.1 (it '\n",
      "                         'worked well as at the time of writing this), and it '\n",
      "                         'is compatible with Pyspark 3.5.1. Make sure to add '\n",
      "                         'or edit your environment variable like this:\\n'\n",
      "                         'export '\n",
      "                         'SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\n'\n",
      "                         'export PATH=\"${SPARK_HOME}/bin:${PATH}\"'},\n",
      "                {'question': 'Spark Standalone Mode on Windows',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Open a CMD terminal in administrator mode\\n'\n",
      "                         'cd %SPARK_HOME%\\n'\n",
      "                         'Start a master node: bin\\\\spark-class '\n",
      "                         'org.apache.spark.deploy.master.Master\\n'\n",
      "                         'Start a worker node: bin\\\\spark-class '\n",
      "                         'org.apache.spark.deploy.worker.Worker '\n",
      "                         'spark://<master_ip>:<port> --host <IP_ADDR>\\n'\n",
      "                         'bin/spark-class '\n",
      "                         'org.apache.spark.deploy.worker.Worker '\n",
      "                         'spark://localhost:7077 --host <IP_ADDR>\\n'\n",
      "                         'spark://<master_ip>:<port>: copy the address from '\n",
      "                         'the previous command, in my case it was '\n",
      "                         'spark://localhost:7077\\n'\n",
      "                         'Use --host <IP_ADDR> if you want to run the worker '\n",
      "                         'on a different machine. For now leave it empty.\\n'\n",
      "                         'Now you can access Spark UI through localhost:8080\\n'\n",
      "                         'Homework for Module 5:\\n'\n",
      "                         'Do not refer to the homework file located under '\n",
      "                         '/05-batch/code/. The correct file is located under\\n'\n",
      "                         'https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md'},\n",
      "                {'question': 'Export PYTHONPATH command in linux is temporary',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'You can either type the export command every time '\n",
      "                         'you run a new session, add it to the .bashrc/ which '\n",
      "                         'you can find in /home or run this command at the '\n",
      "                         'beginning of your homebook:\\n'\n",
      "                         'import findspark\\n'\n",
      "                         'findspark.init()'},\n",
      "                {'question': 'Compressed file ended before the end-of-stream '\n",
      "                             'marker was reached',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'I solved this issue: unzip the file with:\\n'\n",
      "                         'f\\n'\n",
      "                         'before creating head.csv'},\n",
      "                {'question': 'Compression Error: zcat output is gibberish, '\n",
      "                             'seems like still compressed',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'In the code along from Video 5.3.3 Alexey downloads '\n",
      "                         'the CSV files from the NYT website and gzips them in '\n",
      "                         'their bash script. If we now (2023) follow along but '\n",
      "                         'download the data from the GH course Repo, it will '\n",
      "                         'already be zippes as csv.gz files. Therefore we zip '\n",
      "                         'it again if we follow the code from the video '\n",
      "                         'exactly. This then leads to gibberish outcome when '\n",
      "                         'we then try to cat the contents or count the lines '\n",
      "                         'with zcat, because the file is zipped twitch and '\n",
      "                         'zcat only unzips it once.\\n'\n",
      "                         '✅solution: do not gzip the files downloaded from the '\n",
      "                         'course repo. Just wget them and save them as they '\n",
      "                         'are as csv.gz files. Then the zcat command and the '\n",
      "                         'showSchema command will also work\\n'\n",
      "                         'URL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\\n'\n",
      "                         'LOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\\n'\n",
      "                         'LOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\\n'\n",
      "                         'LOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\\n'\n",
      "                         'echo \"downloading ${URL} to ${LOCAL_PATH}\"\\n'\n",
      "                         'mkdir -p ${LOCAL_PREFIX}\\n'\n",
      "                         'wget ${URL} -O ${LOCAL_PATH}\\n'\n",
      "                         'echo \"compressing ${LOCAL_PATH}\"\\n'\n",
      "                         '# gzip ${LOCAL_PATH} <- uncomment this line'},\n",
      "                {'question': 'PicklingError: Could not serialise object: '\n",
      "                             'IndexError: tuple index out of range.',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Occurred while running : '\n",
      "                         'spark.createDataFrame(df_pandas).show()\\n'\n",
      "                         'This error is usually due to the python version, '\n",
      "                         'since spark till date of 2 march 2023 doesn’t '\n",
      "                         'support python 3.11, try creating a new env with '\n",
      "                         'python version 3.8 and then run this command.\\n'\n",
      "                         'On the virtual machine, you can create a conda '\n",
      "                         'environment (here called myenv) with python 3.10 '\n",
      "                         'installed:\\n'\n",
      "                         'conda create -n myenv python=3.10 anaconda\\n'\n",
      "                         'Then you must run conda activate myenv to run python '\n",
      "                         '3.10. Otherwise you’ll still be running version '\n",
      "                         '3.11. You can deactivate by typing conda '\n",
      "                         'deactivate.'},\n",
      "                {'question': 'Connecting from local Spark to GCS - Spark does '\n",
      "                             'not find my google credentials as shown in the '\n",
      "                             'video?',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Make sure you have your credentials of your GCP in '\n",
      "                         'your VM under the location defined in the script.'},\n",
      "                {'question': 'Spark docker-compose setup',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'To run spark in docker setup\\n'\n",
      "                         '1. Build bitnami spark docker\\n'\n",
      "                         'a. clone bitnami repo using command\\n'\n",
      "                         'git clone https://github.com/bitnami/containers.git\\n'\n",
      "                         '(tested on commit '\n",
      "                         '9cef8b892d29c04f8a271a644341c8222790c992)\\n'\n",
      "                         'b. edit file '\n",
      "                         '`bitnami/spark/3.3/debian-11/Dockerfile` and update '\n",
      "                         'java and spark version as following\\n'\n",
      "                         '\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n'\n",
      "                         '\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\n'\n",
      "                         'reference: '\n",
      "                         'https://github.com/bitnami/containers/issues/13409\\n'\n",
      "                         'c. build docker image by navigating to above '\n",
      "                         'directory and running docker build command\\n'\n",
      "                         'navigate cd bitnami/spark/3.3/debian-11/\\n'\n",
      "                         'build command docker build -t spark:3.3-java-17 .\\n'\n",
      "                         '2. run docker compose\\n'\n",
      "                         'using following file\\n'\n",
      "                         '```yaml docker-compose.yml\\n'\n",
      "                         \"version: '2'\\n\"\n",
      "                         'services:\\n'\n",
      "                         'spark:\\n'\n",
      "                         'image: spark:3.3-java-17\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- SPARK_MODE=master\\n'\n",
      "                         '- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n'\n",
      "                         '- SPARK_RPC_ENCRYPTION_ENABLED=no\\n'\n",
      "                         '- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n'\n",
      "                         '- SPARK_SSL_ENABLED=no\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- \"./:/home/jovyan/work:rw\"\\n'\n",
      "                         'ports:\\n'\n",
      "                         \"- '8080:8080'\\n\"\n",
      "                         \"- '7077:7077'\\n\"\n",
      "                         'spark-worker:\\n'\n",
      "                         'image: spark:3.3-java-17\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- SPARK_MODE=worker\\n'\n",
      "                         '- SPARK_MASTER_URL=spark://spark:7077\\n'\n",
      "                         '- SPARK_WORKER_MEMORY=1G\\n'\n",
      "                         '- SPARK_WORKER_CORES=1\\n'\n",
      "                         '- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n'\n",
      "                         '- SPARK_RPC_ENCRYPTION_ENABLED=no\\n'\n",
      "                         '- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n'\n",
      "                         '- SPARK_SSL_ENABLED=no\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- \"./:/home/jovyan/work:rw\"\\n'\n",
      "                         'ports:\\n'\n",
      "                         \"- '8081:8081'\\n\"\n",
      "                         'spark-nb:\\n'\n",
      "                         'image: jupyter/pyspark-notebook:java-17.0.5\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- SPARK_MASTER_URL=spark://spark:7077\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- \"./:/home/jovyan/work:rw\"\\n'\n",
      "                         'ports:\\n'\n",
      "                         \"- '8888:8888'\\n\"\n",
      "                         \"- '4040:4040'\\n\"\n",
      "                         '```\\n'\n",
      "                         'run command to deploy docker compose\\n'\n",
      "                         'docker-compose up\\n'\n",
      "                         'Access jupyter notebook using link logged in docker '\n",
      "                         'compose logs\\n'\n",
      "                         'Spark master url is spark://spark:7077'},\n",
      "                {'question': 'How do you read data stored in gcs on pandas '\n",
      "                             'with your local computer?',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'To do this\\n'\n",
      "                         'pip install gcsfs,\\n'\n",
      "                         'Thereafter copy the uri path to the file and use \\n'\n",
      "                         'df = pandas.read_csc(gs://path)'},\n",
      "                {'question': 'TypeError when using spark.createDataFrame '\n",
      "                             'function on a pandas df',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Error:\\n'\n",
      "                         'spark.createDataFrame(df_pandas).schema\\n'\n",
      "                         'TypeError: field Affiliated_base_number: Can not '\n",
      "                         \"merge type <class 'pyspark.sql.types.StringType'> \"\n",
      "                         \"and <class 'pyspark.sql.types.DoubleType'>\\n\"\n",
      "                         'Solution:\\n'\n",
      "                         'Affiliated_base_number is a mix of letters and '\n",
      "                         'numbers (you can check this with a preview of the '\n",
      "                         'table), so it cannot be set to DoubleType (only for '\n",
      "                         'double-precision numbers). The suitable type would '\n",
      "                         'be StringType. Spark  inferSchema is more accurate '\n",
      "                         'than Pandas infer type method in this case. You can '\n",
      "                         'set it to  true  while reading the csv, so you don’t '\n",
      "                         'have to take out any data from your dataset. '\n",
      "                         'Something like this can help:\\n'\n",
      "                         'df = spark.read \\\\\\n'\n",
      "                         '.options(\\n'\n",
      "                         'header = \"true\", \\\\\\n'\n",
      "                         'inferSchema = \"true\", \\\\\\n'\n",
      "                         ') \\\\\\n'\n",
      "                         \".csv('path/to/your/csv/file/')\\n\"\n",
      "                         'Solution B:\\n'\n",
      "                         \"It's because some rows in the affiliated_base_number \"\n",
      "                         'are null and therefore it is assigned the datatype '\n",
      "                         'String and this cannot be converted to type Double. '\n",
      "                         'So if you really want to convert this pandas df to a '\n",
      "                         'pyspark df only take the  rows from the pandas df '\n",
      "                         \"that are not null in the 'Affiliated_base_number' \"\n",
      "                         'column. Then you will be able to apply the pyspark '\n",
      "                         'function createDataFrame.\\n'\n",
      "                         '# Only take rows that have no null values\\n'\n",
      "                         'pandas_df= pandas_df[pandas_df.notnull().all(1)]'},\n",
      "                {'question': 'MemoryManager: Total allocation exceeds 95.00% '\n",
      "                             '(1,020,054,720 bytes) of heap memory',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Default executor memory is 1gb. This error appeared '\n",
      "                         'when working with the homework dataset.\\n'\n",
      "                         'Error: MemoryManager: Total allocation exceeds '\n",
      "                         '95.00% (1,020,054,720 bytes) of heap memory\\n'\n",
      "                         'Scaling row group sizes to 95.00% for 8 writers\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Increase the memory of the executor when creating '\n",
      "                         'the Spark session like this:\\n'\n",
      "                         'Remember to restart the Jupyter session (ie. close '\n",
      "                         'the Spark session) or the config won’t take effect.'},\n",
      "                {'question': 'How to spark standalone cluster is run on '\n",
      "                             'windows OS',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Change the working directory to the spark '\n",
      "                         'directory:\\n'\n",
      "                         'if you have setup up your SPARK_HOME variable, use '\n",
      "                         'the following;\\n'\n",
      "                         'cd %SPARK_HOME%\\n'\n",
      "                         'if not, use the following;\\n'\n",
      "                         'cd <path to spark installation>\\n'\n",
      "                         'Creating a Local Spark Cluster\\n'\n",
      "                         'To start Spark Master:\\n'\n",
      "                         'bin\\\\spark-class '\n",
      "                         'org.apache.spark.deploy.master.Master --host '\n",
      "                         'localhost\\n'\n",
      "                         'Starting up a cluster:\\n'\n",
      "                         'bin\\\\spark-class '\n",
      "                         'org.apache.spark.deploy.worker.Worker '\n",
      "                         'spark://localhost:7077 --host localhost'},\n",
      "                {'question': 'Env variables set in ~/.bashrc are not loaded to '\n",
      "                             'Jupyter in VS Code',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'I added PYTHONPATH, JAVA_HOME and SPARK_HOME to '\n",
      "                         '~/.bashrc, import pyspark worked ok in iPython in '\n",
      "                         'terminal, but couldn’t be found in .ipynb opened in '\n",
      "                         'VS Code\\n'\n",
      "                         'After adding new lines to ~/.bashrc, need to restart '\n",
      "                         'the shell to activate the new lines, do either\\n'\n",
      "                         'source ~/.bashrc\\n'\n",
      "                         'exec bash\\n'\n",
      "                         'Instead of configuring paths in ~/.bashrc, I created '\n",
      "                         '.env file in the root of my workspace:'},\n",
      "                {'question': 'How to port forward outside VS Code',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'I don’t use visual studio, so I did it the old '\n",
      "                         'fashioned way: ssh -L 8888:localhost:8888 <my '\n",
      "                         'user>@<VM IP> (replace user and IP with the ones '\n",
      "                         'used by the GCP VM, e.g. : ssh -L '\n",
      "                         '8888:localhost:8888 myuser@34.140.188.1'},\n",
      "                {'question': '“wc -l” is giving a different result then shown '\n",
      "                             'in the video',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'If you are doing wc -l '\n",
      "                         'fhvhv_tripdata_2021-01.csv.gz  with the gzip file as '\n",
      "                         'the file argument, you will get a different result, '\n",
      "                         'obviously! Since the file is compressed.\\n'\n",
      "                         'Unzip the file and then do wc -l '\n",
      "                         'fhvhv_tripdata_2021-01.csv to get the right '\n",
      "                         'results.'},\n",
      "                {'question': '`spark-submit` errors',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'when trying to:\\n'\n",
      "                         'URL=\"spark://$HOSTNAME:7077\"\\n'\n",
      "                         'spark-submit \\\\\\n'\n",
      "                         '--master=\"{$URL}\" \\\\\\n'\n",
      "                         '06_spark_sql.py \\\\\\n'\n",
      "                         '--input_green=data/pq/green/2021/*/ \\\\\\n'\n",
      "                         '--input_yellow=data/pq/yellow/2021/*/ \\\\\\n'\n",
      "                         '--output=data/report-2021\\n'\n",
      "                         'and you get errors like the following (SUMMARIZED):\\n'\n",
      "                         'WARN Utils: Your hostname, <HOSTNAME> resolves to a '\n",
      "                         'loopback address..\\n'\n",
      "                         'WARN Utils: Set SPARK_LOCAL_IP if you need to bind '\n",
      "                         'to another address Setting default log level to '\n",
      "                         '\"WARN\".\\n'\n",
      "                         'Exception in thread \"main\" '\n",
      "                         'org.apache.spark.SparkException: Master must either '\n",
      "                         'be yarn or start with spark, mesos, k8s, or local at '\n",
      "                         '…\\n'\n",
      "                         'Try replacing --master=\"{$URL}\"\\n'\n",
      "                         'with --master=$URL (edited)\\n'\n",
      "                         'Extra edit for spark version 3.4.2 - if '\n",
      "                         'encountering:\\n'\n",
      "                         '`Error: Unrecognized option: --master=`\\n'\n",
      "                         '→ Replace `--master=\"{$URL}\"` with  `--master '\n",
      "                         '\"${URL}\"`'},\n",
      "                {'question': 'Hadoop - Exception in thread \"main\" '\n",
      "                             'java.lang.UnsatisfiedLinkError: '\n",
      "                             'org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'If you are seeing this (or similar) error when '\n",
      "                         'attempting to write to parquet, it is likely an '\n",
      "                         'issue with your path variables.\\n'\n",
      "                         'For Windows, create a new User Variable '\n",
      "                         '“HADOOP_HOME” that points to your Hadoop directory. '\n",
      "                         'Then add “%HADOOP_HOME%\\\\bin” to the PATH variable.\\n'\n",
      "                         'Additional tips can be found here: '\n",
      "                         'https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io'},\n",
      "                {'question': 'Java.io.IOException. Cannot run program '\n",
      "                             '“C:\\\\hadoop\\\\bin\\\\winutils.exe”. CreateProcess '\n",
      "                             'error=216, This version of 1% is not compatible '\n",
      "                             'with the version of Windows you are using.',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Change the hadoop version to 3.0.1.Replace all the '\n",
      "                         'files in the local hadoop bin folder with the files '\n",
      "                         'in this repo:  winutils/hadoop-3.0.1/bin at master · '\n",
      "                         'cdarlint/winutils (github.com)\\n'\n",
      "                         'If this does not work try to change other versions '\n",
      "                         'found in this repository.\\n'\n",
      "                         'For more information please see this link: This '\n",
      "                         'version of %1 is not compatible with the version of '\n",
      "                         \"Windows you're running · Issue #20 · \"\n",
      "                         'cdarlint/winutils (github.com)'},\n",
      "                {'question': 'Dataproc - ERROR: '\n",
      "                             '(gcloud.dataproc.jobs.submit.pyspark) The '\n",
      "                             'required property [project] is not currently '\n",
      "                             'set. It can be set on a per-command basis by '\n",
      "                             're-running your command with the [--project] '\n",
      "                             'flag.',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Fix is to set the flag like the error states. Get '\n",
      "                         'your project ID from your dashboard and set it like '\n",
      "                         'so:\\n'\n",
      "                         'gcloud dataproc jobs submit pyspark \\\\\\n'\n",
      "                         '--cluster=my_cluster \\\\\\n'\n",
      "                         '--region=us-central1 \\\\\\n'\n",
      "                         '--project=my-dtc-project-1010101 \\\\\\n'\n",
      "                         'gs://my-dtc-bucket-id/code/06_spark_sql.py\\n'\n",
      "                         '-- \\\\\\n'\n",
      "                         '…'},\n",
      "                {'question': 'Run Local Cluster Spark in Windows 10 with CMD',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Go to %SPARK_HOME%\\\\bin\\n'\n",
      "                         'Run spark-class '\n",
      "                         'org.apache.spark.deploy.master.Master to run the '\n",
      "                         'master. This will give you a URL of the form '\n",
      "                         'spark://ip:port\\n'\n",
      "                         'Run spark-class '\n",
      "                         'org.apache.spark.deploy.worker.Worker '\n",
      "                         'spark://ip:port to run the worker. Make sure you use '\n",
      "                         'the URL you obtained in step 2.\\n'\n",
      "                         'Create a new Jupyter notebook:\\n'\n",
      "                         'spark = SparkSession.builder \\\\\\n'\n",
      "                         '.master(\"spark://{ip}:7077\") \\\\\\n'\n",
      "                         \".appName('test') \\\\\\n\"\n",
      "                         '.getOrCreate()\\n'\n",
      "                         'Check on Spark UI the master, worker and app.'},\n",
      "                {'question': 'lServiceException: 401 Anonymous caller does not '\n",
      "                             'have storage.objects.list access to the Google '\n",
      "                             'Cloud Storage bucket. Permission '\n",
      "                             \"'storage.objects.list' denied on resource (or it \"\n",
      "                             'may not exist).',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'This occurs because you are not logged in “gcloud '\n",
      "                         'auth login” and maybe the project id is not settled. '\n",
      "                         'Then type in a terminal:\\n'\n",
      "                         'gcloud auth login\\n'\n",
      "                         'This will open a tab in the browser, accept the '\n",
      "                         'terms, after that close the tab if you want. Then '\n",
      "                         'set the project is like:\\n'\n",
      "                         'gcloud config set project <YOUR PROJECT_ID>\\n'\n",
      "                         'Then you can run the command to upload the pq dir to '\n",
      "                         'a GCS Bucket:\\n'\n",
      "                         'gsutil -m cp -r pq/ <YOUR URI from gsutil>/pq'},\n",
      "                {'question': 'py4j.protocol.Py4JJavaError  GCP',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'When submit a job, it might throw an error about '\n",
      "                         'Java in log panel within Dataproc. I changed the '\n",
      "                         'Versioning Control when I created a cluster, so it '\n",
      "                         'means that I delete the cluster and created a new '\n",
      "                         'one, and instead of choosing Debian-Hadoop-Spark, I '\n",
      "                         'switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for '\n",
      "                         'Versioning Control feature, the main reason to '\n",
      "                         'choose this is because I have the same Ubuntu '\n",
      "                         'version in mi laptop, I tried to find documentation '\n",
      "                         \"to sustent this but unfortunately I couldn't \"\n",
      "                         'nevertheless it works for me.'},\n",
      "                {'question': 'Repartition the Dataframe to 6 partitions using '\n",
      "                             'df.repartition(6) - got 8 partitions instead',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Use both repartition and coalesce, like so:\\n'\n",
      "                         'df = df.repartition(6)\\n'\n",
      "                         'df = df.coalesce(6)\\n'\n",
      "                         \"df.write.parquet('fhv/2019/10', mode='overwrite')\"},\n",
      "                {'question': 'Jupyter Notebook or SparkUI not loading properly '\n",
      "                             'at localhost after port forwarding from VS code?',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Possible solution - Try to forward the port using '\n",
      "                         'ssh cli instead of vs code.\\n'\n",
      "                         'Run > “ssh -L <local port>:<VM host/ip>:<VM port> '\n",
      "                         '<ssh hostname>”\\n'\n",
      "                         'ssh hostname is the name you specified in the '\n",
      "                         '~/.ssh/config file\\n'\n",
      "                         'In case of Jupyter Notebook run\\n'\n",
      "                         '“ssh -L 8888:localhost:8888 gcp-vm”\\n'\n",
      "                         'from your local machine’s cli.\\n'\n",
      "                         'NOTE: If you logout from the session, the connection '\n",
      "                         'would break. Also while creating the spark session '\n",
      "                         \"notice the block's log because sometimes it fails to \"\n",
      "                         'run at 4040 and then switches to 4041.\\n'\n",
      "                         '~Abhijit Chakrborty: If you are having trouble '\n",
      "                         'accessing localhost ports from GCP VM consider '\n",
      "                         'adding the forwarding instructions to .ssh/config '\n",
      "                         'file as following:\\n'\n",
      "                         '```\\n'\n",
      "                         'Host <hostname>\\n'\n",
      "                         'Hostname <external-gcp-ip>\\n'\n",
      "                         'User xxxx\\n'\n",
      "                         'IdentityFile yyyy\\n'\n",
      "                         'LocalForward 8888 localhost:8888\\n'\n",
      "                         'LocalForward 8080 localhost:8080\\n'\n",
      "                         'LocalForward 5432 localhost:5432\\n'\n",
      "                         'LocalForward 4040 localhost:4040\\n'\n",
      "                         '```\\n'\n",
      "                         'This should automatically forward all ports and will '\n",
      "                         'enable accessing localhost ports.'},\n",
      "                {'question': 'Installing Java 11 on codespaces',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': '~ Abhijit Chakraborty\\n'\n",
      "                         '`sdk list java`  to check for available java sdk '\n",
      "                         'versions.\\n'\n",
      "                         '`sdk install java 11.0.22-amzn`  as  '\n",
      "                         'java-11.0.22-amzn was available for my codespace.\\n'\n",
      "                         'click on Y if prompted to change the default java '\n",
      "                         'version.\\n'\n",
      "                         'Check for java version using `java -version `.\\n'\n",
      "                         'If working fine great, else `sdk default java '\n",
      "                         '11.0.22-amzn` or whatever version you have '\n",
      "                         'installed.'},\n",
      "                {'question': \"Error: Insufficient 'SSD_TOTAL_GB' quota. \"\n",
      "                             'Requested 500.0, available 470.0.',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Sometimes while creating a dataproc cluster on GCP, '\n",
      "                         'the following error is encountered.\\n'\n",
      "                         'Solution: As mentioned here, sometimes there might '\n",
      "                         'not be enough resources in the given region to '\n",
      "                         'allocate the request. Usually, gets freed up in a '\n",
      "                         'bit and one can create a cluster. – abhirup ghosh\\n'\n",
      "                         'Solution 2:  Changing the type of boot-disk from '\n",
      "                         'PD-Balanced to PD-Standard, in terraform, helped '\n",
      "                         'solve the problem.- Sundara Kumar Padmanabhan'},\n",
      "                {'question': 'Homework - how to convert the time difference of '\n",
      "                             'two timestamps to hours',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Pyspark converts the difference of two TimestampType '\n",
      "                         \"values to Python's native datetime.timedelta object. \"\n",
      "                         'The timedelta object only stores the duration in '\n",
      "                         'terms of days, seconds, and microseconds. Each of '\n",
      "                         'the three units of time must be manually converted '\n",
      "                         'into hours in order to express the total duration '\n",
      "                         'between the two timestamps using only hours.\\n'\n",
      "                         'Another way for achieving this is using the datediff '\n",
      "                         '(sql function). It receives this parameters\\n'\n",
      "                         'Upper Date: the closest date you have. For example '\n",
      "                         'dropoff_datetime\\n'\n",
      "                         'Lower Date: the farthest date you have.  For example '\n",
      "                         'pickup_datetime\\n'\n",
      "                         'And the result is returned in terms of days, so you '\n",
      "                         'could multiply the result for 24 in order to get the '\n",
      "                         'hours.'},\n",
      "                {'question': 'PicklingError: Could not serialize object: '\n",
      "                             'IndexError: tuple index out of range',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'This version combination worked for me:\\n'\n",
      "                         'PySpark = 3.3.2\\n'\n",
      "                         'Pandas = 1.5.3\\n'\n",
      "                         '\\n'\n",
      "                         'If it still has an error,'},\n",
      "                {'question': 'Py4JJavaError: An error occurred while calling '\n",
      "                             'o180.showString. : '\n",
      "                             'org.apache.spark.SparkException: Job aborted due '\n",
      "                             'to stage failure: Task 0 in stage 6.0 failed 1 '\n",
      "                             'times, most recent failure: Lost task 0.0 in '\n",
      "                             'stage 6.0 (TID 6) (host.docker.internal executor '\n",
      "                             'driver): org.apache.spark.SparkException: Python '\n",
      "                             'worker failed to connect back.',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Run this before SparkSession\\n'\n",
      "                         'import os\\n'\n",
      "                         'import sys\\n'\n",
      "                         \"os.environ['PYSPARK_PYTHON'] = sys.executable\\n\"\n",
      "                         \"os.environ['PYSPARK_DRIVER_PYTHON'] = \"\n",
      "                         'sys.executable'},\n",
      "                {'question': 'RuntimeError: Python in worker has different '\n",
      "                             'version 3.11 than that in driver 3.10, PySpark '\n",
      "                             'cannot run with different minor versions. Please '\n",
      "                             'check environment variables PYSPARK_PYTHON and '\n",
      "                             'PYSPARK_DRIVER_PYTHON are correctly set.',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'import os\\n'\n",
      "                         'import sys\\n'\n",
      "                         \"os.environ['PYSPARK_PYTHON'] = sys.executable\\n\"\n",
      "                         \"os.environ['PYSPARK_DRIVER_PYTHON'] = \"\n",
      "                         'sys.executable\\n'\n",
      "                         'Dataproc Pricing: '\n",
      "                         'https://cloud.google.com/dataproc/pricing#on_gke_pricing'},\n",
      "                {'question': 'Dataproc Qn: Is it essential to have a VM on GCP '\n",
      "                             'for running Dataproc and submitting jobs ?',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': 'Ans: No, you can submit a job to DataProc from your '\n",
      "                         'local computer by installing gsutil '\n",
      "                         '(https://cloud.google.com/storage/docs/gsutil_install) '\n",
      "                         'and configuring it. Then, you can execute the '\n",
      "                         'following command from your local computer.\\n'\n",
      "                         'gcloud dataproc jobs submit pyspark \\\\\\n'\n",
      "                         '--cluster=de-zoomcamp-cluster \\\\\\n'\n",
      "                         '--region=europe-west6 \\\\\\n'\n",
      "                         'gs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py '\n",
      "                         '\\\\\\n'\n",
      "                         '-- \\\\\\n'\n",
      "                         '--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ '\n",
      "                         '\\\\\\n'\n",
      "                         '--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ '\n",
      "                         '\\\\\\n'\n",
      "                         '--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 '\n",
      "                         '(edited)'},\n",
      "                {'question': 'In module 5.3.1, trying to run '\n",
      "                             'spark.createDataFrame(df_pandas).show() returns '\n",
      "                             'error',\n",
      "                 'section': 'Module 5: pyspark',\n",
      "                 'text': \"AttributeError: 'DataFrame' object has no attribute \"\n",
      "                         \"'iteritems'\\n\"\n",
      "                         'this is because the method inside the pyspark refers '\n",
      "                         'to a package that has been already deprecated\\n'\n",
      "                         '(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\\n'\n",
      "                         'You can do this code below, which is mentioned in '\n",
      "                         'the stackoverflow link above:\\n'\n",
      "                         'Q: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster '\n",
      "                         'I cannot create a cluster and get this message. I '\n",
      "                         \"tried many times as the FAQ said, but it didn't \"\n",
      "                         'work. What can I do?\\n'\n",
      "                         'Error\\n'\n",
      "                         \"Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, \"\n",
      "                         'available 250.0.\\n'\n",
      "                         'Request ID: 17942272465025572271\\n'\n",
      "                         'A: The master and worker nodes are allocated a '\n",
      "                         'maximum of 250 GB of memory combined. In the '\n",
      "                         'configuration section, adhere to the following '\n",
      "                         'specifications:\\n'\n",
      "                         'Master Node:\\n'\n",
      "                         'Machine type: n2-standard-2\\n'\n",
      "                         'Primary disk size: 85 GB\\n'\n",
      "                         'Worker Node:\\n'\n",
      "                         'Number of worker nodes: 2\\n'\n",
      "                         'Machine type: n2-standard-2\\n'\n",
      "                         'Primary disk size: 80 GB\\n'\n",
      "                         'You can allocate up to 82.5 GB memory for worker '\n",
      "                         'nodes, keeping in mind that the total memory '\n",
      "                         'allocated across all nodes cannot exceed 250 GB.'},\n",
      "                {'question': 'Setting JAVA_HOME with Homebrew on Apple Silicon',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'The MacOS setup instruction '\n",
      "                         '(https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) '\n",
      "                         'for setting the JAVA_HOME environment variable is '\n",
      "                         'for Intel-based Macs which have a default install '\n",
      "                         'location at /usr/local/. If you have an Apple '\n",
      "                         'Silicon mac, you will have to set JAVA_HOME to '\n",
      "                         '/opt/homebrew/, specifically in your .bashrc or '\n",
      "                         '.zshrc:\\n'\n",
      "                         'export JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\\n'\n",
      "                         'export PATH=\"$JAVA_HOME:$PATH\"\\n'\n",
      "                         'Confirm that your path was correctly set by running '\n",
      "                         'the command: which java\\n'\n",
      "                         'You should expect to see the output:\\n'\n",
      "                         '/opt/homebrew/opt/openjdk/bin/java\\n'\n",
      "                         'Reference: https://docs.brew.sh/Installation'},\n",
      "                {'question': 'Could not start docker image “control-center” '\n",
      "                             'from the docker-compose.yaml file.',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Check Docker Compose File:\\n'\n",
      "                         'Ensure that your docker-compose.yaml file is '\n",
      "                         'correctly configured with the necessary details for '\n",
      "                         'the \"control-center\" service. Check the service '\n",
      "                         'name, image name, ports, volumes, environment '\n",
      "                         'variables, and any other configurations required for '\n",
      "                         'the container to start.\\n'\n",
      "                         'On Mac OSX 12.2.1 (Monterey) I could not start the '\n",
      "                         'kafka control center. I opened Docker Desktop and '\n",
      "                         'saw docker images still running from week 4, which I '\n",
      "                         'did not see when I typed “docker ps.” I deleted them '\n",
      "                         'in docker desktop and then had no problem starting '\n",
      "                         'up the kafka environment.'},\n",
      "                {'question': 'Module “kafka” not found when trying to run '\n",
      "                             'producer.py',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Solution from Alexey: create a virtual environment '\n",
      "                         'and run requirements.txt and the python files in '\n",
      "                         'that environment.\\n'\n",
      "                         'To create a virtual env and install packages (run '\n",
      "                         'only once)\\n'\n",
      "                         'python -m venv env\\n'\n",
      "                         'source env/bin/activate\\n'\n",
      "                         'pip install -r ../requirements.txt\\n'\n",
      "                         \"To activate it (you'll need to run it every time you \"\n",
      "                         'need the virtual env):\\n'\n",
      "                         'source env/bin/activate\\n'\n",
      "                         'To deactivate it:\\n'\n",
      "                         'deactivate\\n'\n",
      "                         'This works on MacOS, Linux and Windows - but for '\n",
      "                         \"Windows the path is slightly different (it's \"\n",
      "                         'env/Scripts/activate)\\n'\n",
      "                         'Also the virtual environment should be created only '\n",
      "                         'to run the python file. Docker images should first '\n",
      "                         'all be up and running.'},\n",
      "                {'question': 'Error importing cimpl dll when running avro '\n",
      "                             'examples',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'ImportError: DLL load failed while importing cimpl: '\n",
      "                         'The specified module could not be found\\n'\n",
      "                         'Verify Python Version:\\n'\n",
      "                         'Make sure you are using a compatible version of '\n",
      "                         'Python with the Avro library. Check the Python '\n",
      "                         'version and compatibility requirements specified by '\n",
      "                         'the Avro library documentation.\\n'\n",
      "                         '... you may have to load librdkafka-5d2e2910.dll in '\n",
      "                         'the code. Add this before importing avro:\\n'\n",
      "                         'from ctypes import CDLL\\n'\n",
      "                         'CDLL(\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\")\\n'\n",
      "                         'It seems that the error may occur depending on the '\n",
      "                         'OS and python version installed.\\n'\n",
      "                         'ALTERNATIVE:\\n'\n",
      "                         'ImportError: DLL load failed while importing cimpl\\n'\n",
      "                         '✅SOLUTION: '\n",
      "                         '$env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in '\n",
      "                         'Powershell.\\n'\n",
      "                         'You need to set this DLL manually in Conda Env.\\n'\n",
      "                         'Source: '\n",
      "                         'https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2'},\n",
      "                {'question': \"ModuleNotFoundError: No module named 'avro'\",\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': '✅SOLUTION: pip install confluent-kafka[avro].\\n'\n",
      "                         \"For some reason, Conda also doesn't include this \"\n",
      "                         'when installing confluent-kafka via pip.\\n'\n",
      "                         'More sources on Anaconda and confluent-kafka '\n",
      "                         'issues:\\n'\n",
      "                         'https://github.com/confluentinc/confluent-kafka-python/issues/590\\n'\n",
      "                         'https://github.com/confluentinc/confluent-kafka-python/issues/1221\\n'\n",
      "                         'https://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka'},\n",
      "                {'question': 'Error while running python3 stream.py worker',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'If you get an error while running the command '\n",
      "                         'python3 stream.py worker\\n'\n",
      "                         'Run pip uninstall kafka-python\\n'\n",
      "                         'Then run pip install kafka-python==1.4.6\\n'\n",
      "                         'What is the use of  Redpanda ?\\n'\n",
      "                         'Redpanda: Redpanda is built on top of the Raft '\n",
      "                         'consensus algorithm and is designed as a '\n",
      "                         'high-performance, low-latency alternative to Kafka. '\n",
      "                         'It uses a log-centric architecture similar to Kafka '\n",
      "                         'but with different underlying principles.\\n'\n",
      "                         'Redpanda is a powerful, yet simple, and '\n",
      "                         'cost-efficient streaming data platform that is '\n",
      "                         'compatible with Kafka® APIs while eliminating Kafka '\n",
      "                         'complexity.'},\n",
      "                {'question': 'Negsignal:SIGKILL while converting dta files to '\n",
      "                             'parquet format',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Got this error because the docker container memory '\n",
      "                         'was exhausted. The dta file was upto 800MB but my '\n",
      "                         'docker container does not have enough memory to '\n",
      "                         'handle that.\\n'\n",
      "                         'Solution was to load the file in chunks with Pandas, '\n",
      "                         'then create multiple parquet files for each dat file '\n",
      "                         'I was processing. This worked smoothly and the issue '\n",
      "                         'was resolved.'},\n",
      "                {'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv '\n",
      "                             'is missing',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Copy the file found in the Java example: '\n",
      "                         'data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv'},\n",
      "                {'question': 'Kafka- python videos have low audio and hard to '\n",
      "                             'follow up',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'tip:As the videos have low audio so I downloaded '\n",
      "                         'them and used VLC media player with putting the '\n",
      "                         'audio to the max 200% of original audio and the '\n",
      "                         'audio became quite good or try to use auto caption '\n",
      "                         'generated on Youtube directly.\\n'\n",
      "                         'Kafka Python Videos - Rides.csv\\n'\n",
      "                         'There is no clear explanation of the rides.csv data '\n",
      "                         'that the producer.py python programs use. You can '\n",
      "                         'find that here '\n",
      "                         'https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.'},\n",
      "                {'question': 'kafka.errors.NoBrokersAvailable: '\n",
      "                             'NoBrokersAvailable',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'If you have this error, it most likely that your '\n",
      "                         'kafka broker docker container is not working.\\n'\n",
      "                         'Use docker ps to confirm\\n'\n",
      "                         'Then in the docker compose yaml file folder, run '\n",
      "                         'docker compose up -d to start all the instances.'},\n",
      "                {'question': 'Kafka homwork Q3, there are options that support '\n",
      "                             'scaling concept more than the others:',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Ankush said we can focus on horizontal scaling '\n",
      "                         'option.\\n'\n",
      "                         '“think of scaling in terms of scaling from consumer '\n",
      "                         'end. Or consuming message via horizontal scaling”'},\n",
      "                {'question': 'How to fix docker compose error: Error response '\n",
      "                             'from daemon: pull access denied for spark-3.3.1, '\n",
      "                             \"repository does not exist or may require 'docker \"\n",
      "                             \"login': denied: requested access to the resource \"\n",
      "                             'is denied',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'If you get this error, know that you have not built '\n",
      "                         'your sparks and juypter images. This images aren’t '\n",
      "                         'readily available on dockerHub.\\n'\n",
      "                         'In the spark folder, run ./build.sh from a bash cli '\n",
      "                         'to to build all images before running docker '\n",
      "                         'compose'},\n",
      "                {'question': 'Python Kafka: ./build.sh: Permission denied '\n",
      "                             'Error',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Run this command in terminal in the same directory '\n",
      "                         '(/docker/spark):\\n'\n",
      "                         'chmod +x build.sh'},\n",
      "                {'question': 'Python Kafka: ‘KafkaTimeoutError: Failed to '\n",
      "                             'update metadata after 60.0 secs.’ when running '\n",
      "                             'stream-example/producer.py',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Restarting all services worked for me:\\n'\n",
      "                         'docker-compose down\\n'\n",
      "                         'docker-compose up'},\n",
      "                {'question': 'Python Kafka: ./spark-submit.sh streaming.py - '\n",
      "                             'ERROR StandaloneSchedulerBackend: Application '\n",
      "                             'has been killed. Reason: All masters are '\n",
      "                             'unresponsive! Giving up.',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'While following tutorial 13.2 , when running '\n",
      "                         './spark-submit.sh streaming.py, encountered the '\n",
      "                         'following error:\\n'\n",
      "                         '…\\n'\n",
      "                         '24/03/11 09:48:36 INFO '\n",
      "                         'StandaloneAppClient$ClientEndpoint: Connecting to '\n",
      "                         'master spark://localhost:7077...\\n'\n",
      "                         '24/03/11 09:48:36 INFO TransportClientFactory: '\n",
      "                         'Successfully created connection to '\n",
      "                         'localhost/127.0.0.1:7077 after 10 ms (0 ms spent in '\n",
      "                         'bootstraps)\\n'\n",
      "                         '24/03/11 09:48:54 WARN GarbageCollectionMetrics: To '\n",
      "                         'enable non-built-in garbage collector(s) List(G1 '\n",
      "                         'Concurrent GC), users should configure it(them) to '\n",
      "                         'spark.eventLog.gcMetrics.youngGenerationGarbageCollectors '\n",
      "                         'or '\n",
      "                         'spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n'\n",
      "                         '24/03/11 09:48:56 INFO '\n",
      "                         'StandaloneAppClient$ClientEndpoint: Connecting to '\n",
      "                         'master spark://localhost:7077…\\n'\n",
      "                         '24/03/11 09:49:16 INFO '\n",
      "                         'StandaloneAppClient$ClientEndpoint: Connecting to '\n",
      "                         'master spark://localhost:7077...\\n'\n",
      "                         '24/03/11 09:49:36 WARN StandaloneSchedulerBackend: '\n",
      "                         'Application ID is not initialized yet.\\n'\n",
      "                         '24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: '\n",
      "                         'Application has been killed. Reason: All masters are '\n",
      "                         'unresponsive! Giving up.\\n'\n",
      "                         '…\\n'\n",
      "                         'py4j.protocol.Py4JJavaError: An error occurred while '\n",
      "                         'calling None.org.apache.spark.sql.SparkSession.\\n'\n",
      "                         ': java.lang.IllegalStateException: Cannot call '\n",
      "                         'methods on a stopped SparkContext.\\n'\n",
      "                         '…\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Downgrade your local PySpark to 3.3.1 (same as '\n",
      "                         'Dockerfile)\\n'\n",
      "                         'The reason for the failed connection in my case was '\n",
      "                         'the mismatch of PySpark versions. You can see that '\n",
      "                         'from the logs of spark-master in the docker '\n",
      "                         'container.\\n'\n",
      "                         'Solution 2:\\n'\n",
      "                         'Check what Spark version your local machine has\\n'\n",
      "                         'pyspark –version\\n'\n",
      "                         'spark-submit –version\\n'\n",
      "                         'Add your version to SPARK_VERSION in build.sh'},\n",
      "                {'question': 'Python Kafka: ./spark-submit.sh streaming.py - '\n",
      "                             'How to check why Spark master connection fails',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Start a new terminal\\n'\n",
      "                         'Run: docker ps\\n'\n",
      "                         'Copy the CONTAINER ID of the spark-master container\\n'\n",
      "                         'Run: docker exec -it <spark_master_container_id> '\n",
      "                         'bash\\n'\n",
      "                         'Run: cat logs/spark-master.out\\n'\n",
      "                         'Check for the log when the error happened\\n'\n",
      "                         'Google the error message from there'},\n",
      "                {'question': 'Python Kafka: ./spark-submit.sh streaming.py '\n",
      "                             'Error: py4j.protocol.Py4JJavaError: An error '\n",
      "                             'occurred while calling '\n",
      "                             'None.org.apache.spark.api.java.JavaSparkContext.',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Make sure your java version is 11 or 8.\\n'\n",
      "                         'Check your version by:\\n'\n",
      "                         'java --version\\n'\n",
      "                         'Check all your versions by:\\n'\n",
      "                         '/usr/libexec/java_home -V\\n'\n",
      "                         'If you already have got java 11 but just not '\n",
      "                         'selected as default, select the specific version '\n",
      "                         'by:\\n'\n",
      "                         'export JAVA_HOME=$(/usr/libexec/java_home -v '\n",
      "                         '11.0.22)\\n'\n",
      "                         '(or other version of 11)'},\n",
      "                {'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar '\n",
      "                             'errors: package xxx does not exist even after '\n",
      "                             'gradle build',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'In my set up, all of the dependencies listed in '\n",
      "                         'gradle.build were not installed in '\n",
      "                         '<project_name>-1.0-SNAPSHOT.jar.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'In build.gradle file, I added the following at the '\n",
      "                         'end:\\n'\n",
      "                         'shadowJar {\\n'\n",
      "                         'archiveBaseName = \"java-kafka-rides\"\\n'\n",
      "                         \"archiveClassifier = ''\\n\"\n",
      "                         '}\\n'\n",
      "                         'And then in the command line ran ‘gradle shadowjar’, '\n",
      "                         'and run the script from '\n",
      "                         'java-kafka-rides-1.0-SNAPSHOT.jar created by the '\n",
      "                         'shadowjar'},\n",
      "                {'question': 'Python Kafka: Installing dependencies for '\n",
      "                             'python3 '\n",
      "                             '06-streaming/python/avro_example/producer.py',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'confluent-kafka: `pip install confluent-kafka` or '\n",
      "                         '`conda install conda-forge::python-confluent-kafka`\\n'\n",
      "                         'fastavro: pip install fastavro\\n'\n",
      "                         'Abhirup Ghosh\\n'\n",
      "                         'Can install Faust Library for Module 6 Python '\n",
      "                         'Version due to dependency conflicts?\\n'\n",
      "                         'The Faust repository and library is no longer '\n",
      "                         'maintained - https://github.com/robinhood/faust\\n'\n",
      "                         'If you do not know Java, you now have the option to '\n",
      "                         'follow the Python Videos 6.13 & 6.14 here '\n",
      "                         'https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  '\n",
      "                         'and follow the RedPanda Python version here '\n",
      "                         'https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example '\n",
      "                         '- NOTE: I highly recommend watching the Java videos '\n",
      "                         'to understand the concept of streaming but you can '\n",
      "                         'skip the coding parts - all will become clear when '\n",
      "                         'you get to the Python videos and RedPanda files.'},\n",
      "                {'question': 'Java Kafka: How to run '\n",
      "                             'producer/consumer/kstreams/etc in terminal',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'In the project directory, run:\\n'\n",
      "                         'java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out '\n",
      "                         'src/main/java/org/example/JsonProducer.java'},\n",
      "                {'question': 'Java Kafka: When running the '\n",
      "                             'producer/consumer/etc java scripts, no results '\n",
      "                             'retrieved or no message sent',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'For example, when running JsonConsumer.java, got:\\n'\n",
      "                         'Consuming form kafka started\\n'\n",
      "                         'RESULTS:::0\\n'\n",
      "                         'RESULTS:::0\\n'\n",
      "                         'RESULTS:::0\\n'\n",
      "                         'Or when running JsonProducer.java, got:\\n'\n",
      "                         'Exception in thread \"main\" '\n",
      "                         'java.util.concurrent.ExecutionException: '\n",
      "                         'org.apache.kafka.common.errors.SaslAuthenticationException: '\n",
      "                         'Authentication failed\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Make sure in the scripts in '\n",
      "                         'src/main/java/org/example/ that you are running '\n",
      "                         '(e.g. JsonConsumer.java, JsonProducer.java), the '\n",
      "                         'StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the '\n",
      "                         'correct server url (e.g. europe-west3 from example '\n",
      "                         'vs europe-west2)\\n'\n",
      "                         'Make sure cluster key and secrets are updated in '\n",
      "                         'src/main/java/org/example/Secrets.java '\n",
      "                         '(KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)'},\n",
      "                {'question': 'Java Kafka: Tests are not picked up in VSCode',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'Situation: in VS Code, usually there will be a '\n",
      "                         'triangle icon next to each test. I couldn’t see it '\n",
      "                         'at first and had to do some fixes.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         '(Source)\\n'\n",
      "                         'VS Code\\n'\n",
      "                         '→ Explorer (first icon on the left navigation bar)\\n'\n",
      "                         '→ JAVA PROJECTS (bottom collapsable)\\n'\n",
      "                         '→  icon next in the rightmost position to JAVA '\n",
      "                         'PROJECTS\\n'\n",
      "                         '→  clean Workspace\\n'\n",
      "                         '→ Confirm by clicking Reload and Delete\\n'\n",
      "                         'Now you will be able to see the triangle icon next '\n",
      "                         'to each test like what you normally see in python '\n",
      "                         'tests.\\n'\n",
      "                         'E.g.:\\n'\n",
      "                         'You can also add classes and packages in this window '\n",
      "                         'instead of creating files in the project directory'},\n",
      "                {'question': 'Confluent Kafka: Where can I find schema '\n",
      "                             'registry URL?',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'In Confluent Cloud:\\n'\n",
      "                         'Environment → default (or whatever you named your '\n",
      "                         'environment as) → The right navigation bar →  '\n",
      "                         '“Stream Governance API” →  The URL under “Endpoint”\\n'\n",
      "                         'And create credentials from Credentials section '\n",
      "                         'below it'},\n",
      "                {'question': 'How do I check compatibility of local and '\n",
      "                             'container Spark versions?',\n",
      "                 'section': 'Module 6: streaming with kafka',\n",
      "                 'text': 'You can check the version of your local spark using '\n",
      "                         'spark-submit --version. In the build.sh file of the '\n",
      "                         'Python folder, make sure that SPARK_VERSION matches '\n",
      "                         'your local version. Similarly, make sure the pyspark '\n",
      "                         'you pip installed also matches this version.'},\n",
      "                {'question': 'How to fix the error \"ModuleNotFoundError: No '\n",
      "                             'module named \\'kafka.vendor.six.moves\\'\"?',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'According to https://github.com/dpkp/kafka-python/\\n'\n",
      "                         '“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE '\n",
      "                         'https://github.com/wbarnha/kafka-python-ng FOR THE '\n",
      "                         'TIME BEING”\\n'\n",
      "                         'Use pip install kafka-python-ng instead'},\n",
      "                {'question': 'How is my capstone project going to be '\n",
      "                             'evaluated?',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'Each submitted project will be evaluated by 3 '\n",
      "                         '(three) randomly assigned students that have also '\n",
      "                         'submitted the project.\\n'\n",
      "                         'You will also be responsible for grading the '\n",
      "                         'projects from 3 fellow students yourself. Please be '\n",
      "                         'aware that: not complying to this rule also implies '\n",
      "                         'you failing to achieve the Certificate at the end of '\n",
      "                         'the course.\\n'\n",
      "                         'The final grade you get will be the median score of '\n",
      "                         'the grades you get from the peer reviewers.\\n'\n",
      "                         'And of course, the peer review criteria for '\n",
      "                         'evaluating or being evaluated must follow the '\n",
      "                         'guidelines defined here.'},\n",
      "                {'question': 'Project 1 & Project 2',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'There is only ONE project for this Zoomcamp. You do '\n",
      "                         'not need to submit or create two projects. There are '\n",
      "                         'simply TWO chances to pass the course. You can use '\n",
      "                         'the Second Attempt if you a) fail the first attempt '\n",
      "                         'b) do not have the time due to other engagements '\n",
      "                         'such as holiday or sickness etc. to enter your '\n",
      "                         'project into the first attempt.'},\n",
      "                {'question': 'Does anyone know nice and relatively large '\n",
      "                             'datasets?',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'See a list of datasets here: '\n",
      "                         'https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md'},\n",
      "                {'question': 'How to run python as start up script?',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'You need to redefine the python environment variable '\n",
      "                         'to that of your user account'},\n",
      "                {'question': 'Spark Streaming - How do I read from multiple '\n",
      "                             'topics in the same Spark Session',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'Initiate a Spark Session\\n'\n",
      "                         'spark = (SparkSession\\n'\n",
      "                         '.builder\\n'\n",
      "                         '.appName(app_name)\\n'\n",
      "                         '.master(master=master)\\n'\n",
      "                         '.getOrCreate())\\n'\n",
      "                         'spark.streams.resetTerminated()\\n'\n",
      "                         'query1 = spark\\n'\n",
      "                         '.readStream\\n'\n",
      "                         '…\\n'\n",
      "                         '…\\n'\n",
      "                         '.load()\\n'\n",
      "                         'query2 = spark\\n'\n",
      "                         '.readStream\\n'\n",
      "                         '…\\n'\n",
      "                         '…\\n'\n",
      "                         '.load()\\n'\n",
      "                         'query3 = spark\\n'\n",
      "                         '.readStream\\n'\n",
      "                         '…\\n'\n",
      "                         '…\\n'\n",
      "                         '.load()\\n'\n",
      "                         'query1.start()\\n'\n",
      "                         'query2.start()\\n'\n",
      "                         'query3.start()\\n'\n",
      "                         'spark.streams.awaitAnyTermination() #waits for any '\n",
      "                         'one of the query to receive kill signal or error '\n",
      "                         'failure. This is asynchronous\\n'\n",
      "                         '# On the contrary query3.start().awaitTermination() '\n",
      "                         'is a blocking ex call. Works well when we are '\n",
      "                         'reading only from one topic.'},\n",
      "                {'question': 'Data Transformation from Databricks to Azure SQL '\n",
      "                             'DB',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'Transformed data can be moved in to azure blob '\n",
      "                         'storage and then it can be moved in to azure SQL DB, '\n",
      "                         'instead of moving directly from databricks to Azure '\n",
      "                         'SQL DB.'},\n",
      "                {'question': 'Orchestrating dbt with Airflow',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'The trial dbt account provides access to dbt API. '\n",
      "                         'Job will still be needed to be added manually. '\n",
      "                         'Airflow will run the job using a python operator '\n",
      "                         'calling the API. You will need to provide api key, '\n",
      "                         'job id, etc. (be careful not committing it to '\n",
      "                         'Github).\\n'\n",
      "                         'Detailed explanation here: '\n",
      "                         'https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\\n'\n",
      "                         'Source code example here: '\n",
      "                         'https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py'},\n",
      "                {'question': 'Orchestrating DataProc with Airflow',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\\n'\n",
      "                         'https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\\n'\n",
      "                         'Give the following roles to you service account:\\n'\n",
      "                         'DataProc Administrator\\n'\n",
      "                         'Service Account User (explanation here)\\n'\n",
      "                         'Use DataprocSubmitPySparkJobOperator, '\n",
      "                         'DataprocDeleteClusterOperator and  '\n",
      "                         'DataprocCreateClusterOperator.\\n'\n",
      "                         'When using  DataprocSubmitPySparkJobOperator, do not '\n",
      "                         'forget to add:\\n'\n",
      "                         'dataproc_jars = '\n",
      "                         '[\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\\n'\n",
      "                         'Because DataProc does not already have the BigQuery '\n",
      "                         'Connector.'},\n",
      "                {'question': 'Orchestrating dbt cloud with Mage',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'You can trigger your dbt job in Mage pipeline. For '\n",
      "                         'this get your dbt cloud api key under settings/Api '\n",
      "                         'tokens/personal tokens. Add it safely to  your .env\\n'\n",
      "                         'For example\\n'\n",
      "                         'dbt_api_trigger=dbt_**\\n'\n",
      "                         'Navigate to job page and find api trigger  link\\n'\n",
      "                         'Then create a custom mage Python block with a simple '\n",
      "                         'http request like here\\n'\n",
      "                         'from dotenv import load_dotenv\\n'\n",
      "                         'from pathlib import Path\\n'\n",
      "                         \"dotenv_path = Path('/home/src/.env')\\n\"\n",
      "                         'load_dotenv(dotenv_path=dotenv_path)\\n'\n",
      "                         'dbt_api_trigger= os.getenv(dbt_api_trigger)\\n'\n",
      "                         'url = '\n",
      "                         'f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\\n'\n",
      "                         'headers = {\\n'\n",
      "                         '        \"Authorization\": f\"Token '\n",
      "                         '{dbt_api_trigger}\",\\n'\n",
      "                         '        \"Content-Type\": \"application/json\" }\\n'\n",
      "                         'body = {\\n'\n",
      "                         '        \"cause\": \"Triggered via API\"\\n'\n",
      "                         '    }\\n'\n",
      "                         '    response = requests.post(url, headers=headers, '\n",
      "                         'json=body)\\n'\n",
      "                         'voila! You triggered dbt job form your mage '\n",
      "                         'pipeline.'},\n",
      "                {'question': 'Project evaluation - Reproducibility',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'The slack thread : '\n",
      "                         'thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\\n'\n",
      "                         'The question is that sometimes even if you take '\n",
      "                         'plenty of effort to document every single step, and '\n",
      "                         \"we can't even sure if the person doing the peer \"\n",
      "                         'review will be able to follow-up, so how this '\n",
      "                         'criteria will be evaluated?\\n'\n",
      "                         'Alex clarifies: “Ideally yes, you should try to '\n",
      "                         're-run everything. But I understand that not '\n",
      "                         'everyone has time to do it, so if you check the code '\n",
      "                         'by looking at it and try to spot errors, places with '\n",
      "                         \"missing instructions and so on - then it's already \"\n",
      "                         'great”'},\n",
      "                {'question': 'Key Vault in Azure cloud stack',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'The key valut in Azure cloud is used to store '\n",
      "                         'credentials or passwords or secrets of different '\n",
      "                         'tech stack used in Azure. For example if u do not '\n",
      "                         'want to expose the password in SQL database, then we '\n",
      "                         'can save the password under a given name and use '\n",
      "                         'them in other Azure stack.'},\n",
      "                {'question': 'Spark docker - `ModuleNotFoundError: No module '\n",
      "                             \"named 'py4j'` while executing `import pyspark`\",\n",
      "                 'section': 'Project',\n",
      "                 'text': 'You can get the version of py4j from inside docker '\n",
      "                         'using this command\\n'\n",
      "                         'docker exec -it --user airflow '\n",
      "                         'airflow-airflow-scheduler-1 bash -c \"ls '\n",
      "                         '/opt/spark/python/lib\"'},\n",
      "                {'question': 'psycopg2 complains of incompatible environment '\n",
      "                             'e.g x86 instead of amd',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'Either use conda or pip for managing venv, using '\n",
      "                         'both of them together will cause incompatibility.\\n'\n",
      "                         'If you’re using conda, install psycopg2 using the '\n",
      "                         'conda-forge channel, which may handle the '\n",
      "                         'architecture compatibility automatically\\n'\n",
      "                         'conda install -c conda-forge psycopg2\\n'\n",
      "                         'If pip, do the normal install\\n'\n",
      "                         'pip install psycopg2'},\n",
      "                {'question': 'Setting up dbt locally with Docker and Postgres',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'This is not a FAQ but more of an advice if you want '\n",
      "                         'to set up dbt locally, I did it in the following '\n",
      "                         'way:\\n'\n",
      "                         'I had the postgres instance from week 2 (year 2024) '\n",
      "                         'up (the docker-compose)\\n'\n",
      "                         'mkdir dbt\\n'\n",
      "                         'vi dbt/profiles.yml\\n'\n",
      "                         'And here I attached this content (only the required '\n",
      "                         'fields) and replaced them with the proper values '\n",
      "                         '(for instance mine where in the .env file of the '\n",
      "                         'folder of week 2 docker stuff)\\n'\n",
      "                         'cd dbt && git clone '\n",
      "                         'https://github.com/dbt-labs/dbt-starter-project\\n'\n",
      "                         'mkdir project && cd project && mv '\n",
      "                         'dbt-starter-project/* .\\n'\n",
      "                         'Make sure that you align the profile name in '\n",
      "                         'profiles.yml with the dbt_project.yml file\\n'\n",
      "                         'Add this line anywhere on the dbt_project.yml file:\\n'\n",
      "                         'config-version: 2\\n'\n",
      "                         'docker run --network=mage-zoomcamp_default --mount '\n",
      "                         'type=bind,source=/<your-path>/dbt/project,target=/usr/app '\n",
      "                         '--mount '\n",
      "                         'type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml '\n",
      "                         'ghcr.io/dbt-labs/dbt-postgres ls\\n'\n",
      "                         'If you have trouble run\\n'\n",
      "                         'docker run --network=mage-zoomcamp_default --mount '\n",
      "                         'type=bind,source=/<your-path>/dbt/project,target=/usr/app '\n",
      "                         '--mount '\n",
      "                         'type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml '\n",
      "                         'ghcr.io/dbt-labs/dbt-postgres debug'},\n",
      "                {'question': 'How to connect Pyspark with BigQuery?',\n",
      "                 'section': 'Project',\n",
      "                 'text': 'The following line should be included in pyspark '\n",
      "                         'configuration\\n'\n",
      "                         '# Example initialization of SparkSession variable\\n'\n",
      "                         'spark = (SparkSession.builder\\n'\n",
      "                         '.master(...)\\n'\n",
      "                         '.appName(...)\\n'\n",
      "                         '# Add the following configuration\\n'\n",
      "                         '.config(\"spark.jars.packages\", '\n",
      "                         '\"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\\n'\n",
      "                         ')'},\n",
      "                {'question': 'How to run a dbt-core project as an Airflow Task '\n",
      "                             'Group on Google Cloud Composer using a service '\n",
      "                             'account JSON key',\n",
      "                 'section': 'Course Management Form for Homeworks',\n",
      "                 'text': 'Install the astronomer-cosmos package as a '\n",
      "                         'dependency. (see Terraform example).\\n'\n",
      "                         'Make a new folder, dbt/, inside the dags/ folder of '\n",
      "                         'your Composer GCP bucket and copy paste your '\n",
      "                         'dbt-core project there. (see example)\\n'\n",
      "                         'Ensure your profiles.yml is configured to '\n",
      "                         'authenticate with a service account key. (see '\n",
      "                         'BigQuery example)\\n'\n",
      "                         'Create a new DAG using the DbtTaskGroup class and a '\n",
      "                         'ProfileConfig specifying a profiles_yml_filepath '\n",
      "                         'that points to the location of your JSON key file. '\n",
      "                         '(see example)\\n'\n",
      "                         'Your dbt lineage graph should now appear as tasks '\n",
      "                         'inside a task group like this:'},\n",
      "                {'question': 'Edit Course Profile.',\n",
      "                 'section': 'Workshop 1 - dlthub',\n",
      "                 'text': 'The display name listed on the leaderboard is an '\n",
      "                         'auto-generated randomized name. You can edit it to '\n",
      "                         'be a nickname, or your real name, if you prefer. '\n",
      "                         'Your entry on the Leaderboard is the one highlighted '\n",
      "                         'in teal(?) / light green (?).\\n'\n",
      "                         'The Certificate name should be your actual name that '\n",
      "                         'you want to appear on your certificate after '\n",
      "                         'completing the course.\\n'\n",
      "                         'The \"Display on Leaderboard\" option indicates '\n",
      "                         'whether you want your name to be listed on the '\n",
      "                         'course leaderboard.\\n'\n",
      "                         'Question: Is it possible to create external tables '\n",
      "                         'in BigQuery using URLs, such as those from the NY '\n",
      "                         'Taxi data website?\\n'\n",
      "                         'Answer: Not really, only Bigtable, Cloud Storage, '\n",
      "                         'and Google Drive are supported data stores.'},\n",
      "                {'question': 'How do I install the necessary dependencies to '\n",
      "                             'run the code?',\n",
      "                 'section': 'Workshop 1 - dlthub',\n",
      "                 'text': 'Answer: To run the provided code, ensure that the '\n",
      "                         \"'dlt[duckdb]' package is installed. You can do this \"\n",
      "                         'by executing the provided installation command: !pip '\n",
      "                         'install dlt[duckdb]. If you’re doing it locally, be '\n",
      "                         'sure to also have duckdb pip installed (even before '\n",
      "                         'the duckdb package is loaded).'},\n",
      "                {'question': 'Other packages needed but not listed',\n",
      "                 'section': 'Workshop 1 - dlthub',\n",
      "                 'text': 'If you are running Jupyter Notebook on a fresh new '\n",
      "                         'Codespace or in local machine with a new Virtual '\n",
      "                         'Environment, you will need this package to run the '\n",
      "                         'starter Jupyter Notebook offered by the teacher. '\n",
      "                         'Execute this:\\n'\n",
      "                         'pip install jupyter'},\n",
      "                {'question': 'How can I use DuckDB In-Memory database with dlt '\n",
      "                             '?',\n",
      "                 'section': 'Workshop 1 - dlthub',\n",
      "                 'text': 'Alternatively, you can switch to in-file storage '\n",
      "                         'with:'},\n",
      "                {'question': 'Homework - dlt Exercise 3 - Merge a generator '\n",
      "                             'concerns',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'After loading, you should have a total of 8 records, '\n",
      "                         'and ID 3 should have age 33\\n'\n",
      "                         'Question: Calculate the sum of ages of all the '\n",
      "                         'people loaded as described above\\n'\n",
      "                         \"The sum of all eight records' respective ages is too \"\n",
      "                         'big to be in the choices. You need to first filter '\n",
      "                         'out the people whose occupation is equal to None in '\n",
      "                         'order to get an answer that is close to or present '\n",
      "                         'in the given choices. 😃\\n'\n",
      "                         '----------------------------------------------------------------------------------------\\n'\n",
      "                         'FIXED = use a raw string and keep the file:/// at '\n",
      "                         'the start of your file path\\n'\n",
      "                         \"I'm having an issue with the dlt workshop notebook. \"\n",
      "                         \"The 'Load to Parquet file' section specifically. No \"\n",
      "                         \"matter what I change the file path to, it's still \"\n",
      "                         'saving the dlt files directly to my C drive.\\n'\n",
      "                         '# Set the bucket_url. We can also use a local '\n",
      "                         'folder\\n'\n",
      "                         \"os.environ['DESTINATION__FILESYSTEM__BUCKET_URL'] = \"\n",
      "                         \"r'file:///content/.dlt/my_folder'\\n\"\n",
      "                         'url = '\n",
      "                         '\"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\\n'\n",
      "                         '# Define your pipeline\\n'\n",
      "                         'pipeline = dlt.pipeline(\\n'\n",
      "                         \"pipeline_name='my_pipeline',\\n\"\n",
      "                         \"destination='filesystem',\\n\"\n",
      "                         \"dataset_name='mydata'\\n\"\n",
      "                         ')\\n'\n",
      "                         '# Run the pipeline with the generator we created '\n",
      "                         'earlier.\\n'\n",
      "                         'load_info = pipeline.run(stream_download_jsonl(url), '\n",
      "                         'table_name=\"users\", loader_file_format=\"parquet\")\\n'\n",
      "                         'print(load_info)\\n'\n",
      "                         '# Get a list of all Parquet files in the specified '\n",
      "                         'folder\\n'\n",
      "                         'parquet_files = '\n",
      "                         \"glob.glob('/content/.dlt/my_folder/mydata/users/*.parquet')\\n\"\n",
      "                         '# show parquet files\\n'\n",
      "                         'for file in parquet_files:\\n'\n",
      "                         'print(file)'},\n",
      "                {'question': 'command.sh Error - source: no such file or '\n",
      "                             'directory: command.sh',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Check the contents of the repository with ls - the '\n",
      "                         'command.sh file should be in the root folder\\n'\n",
      "                         'If it is not, verify that you had cloned the correct '\n",
      "                         'repository - '\n",
      "                         'https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04'},\n",
      "                {'question': 'psql - command not found: psql (alternative '\n",
      "                             'install)',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'psql is a command line tool that is installed '\n",
      "                         \"alongside PostgreSQL DB, but since we've always been \"\n",
      "                         \"running PostgreSQL in a container, you've only got \"\n",
      "                         '`pgcli`, which lacks the feature to run a sql script '\n",
      "                         'into the DB. Besides, having a command line for each '\n",
      "                         \"database flavor you'll have to deal with as a Data \"\n",
      "                         'Professional is far from ideal.\\n'\n",
      "                         'So, instead, you can use usql. Check the docs for '\n",
      "                         'details on how to install for your OS. On macOS, it '\n",
      "                         'supports `homebrew`, and on Windows, it supports '\n",
      "                         'scoop.\\n'\n",
      "                         'So, to run the taxi_trips.sql script with usql:'},\n",
      "                {'question': 'Setup - source command.sh - error: '\n",
      "                             '“docker-compose” not found',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'If you encounter this error and are certain that you '\n",
      "                         'have docker compose installed, but typically run it '\n",
      "                         'as docker compose without the hyphen, then consider '\n",
      "                         'editing command.sh file by removing the hyphen from '\n",
      "                         '‘docker-compose’. Example:\\n'\n",
      "                         'start-cluster() {\\n'\n",
      "                         'docker compose -f docker/docker-compose.yml up -d\\n'\n",
      "                         '}'},\n",
      "                {'question': 'Setup - start-cluster error: Invalid top-level '\n",
      "                             'property x-image',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'ERROR: The Compose file '\n",
      "                         \"'./docker/docker-compose.yml' is invalid because:\\n\"\n",
      "                         'Invalid top-level property \"x-image\". Valid '\n",
      "                         'top-level sections for this Compose file are: '\n",
      "                         'version, services, networks, volumes, secrets, '\n",
      "                         'configs, and extensions starting with \"x-\".\\n'\n",
      "                         \"You might be seeing this error because you're using \"\n",
      "                         'the wrong Compose file version. Either specify a '\n",
      "                         'supported version (e.g \"2.2\" or \"3.3\") and place '\n",
      "                         'your service definitions under the `services` key, '\n",
      "                         'or omit the `version` key and place your service '\n",
      "                         'definitions at the root of the file to use version '\n",
      "                         '1.\\n'\n",
      "                         'For more on the Compose file format versions, see '\n",
      "                         'https://docs.docker.com/compose/compose-file/\\n'\n",
      "                         'If you encounter the above error and have '\n",
      "                         'docker-compose installed, try updating your version '\n",
      "                         'of docker-compose. At the time of reporting this '\n",
      "                         'issue (March 17 2024), Ubuntu does not seem to '\n",
      "                         'support a docker-compose version high enough to run '\n",
      "                         'the required docker images. If you have this error '\n",
      "                         'and are on a Ubuntu machine, consider starting a VM '\n",
      "                         'with a Debian machine or look for an alternative way '\n",
      "                         'to download docker-compose at the latest version on '\n",
      "                         'your machine.'},\n",
      "                {'question': 'stream-kafka Qn: Is it expected that the records '\n",
      "                             'are being ingested 10 at a time?',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Ans: [source] Yes, it is so that we can observe the '\n",
      "                         'changes as we’re working on the queries in '\n",
      "                         'real-time. The script is changing the date timestamp '\n",
      "                         'to the current time, so our queries with the '\n",
      "                         'now()filter would work. Open another terminal tab to '\n",
      "                         'copy+paste the queries while the stream-kafka script '\n",
      "                         'is running in the background.\\n'\n",
      "                         'Noel: I have recently increased this up to 100 at a '\n",
      "                         'time, you may pull the latest changes from the '\n",
      "                         'repository.'},\n",
      "                {'question': 'Setup - Qn: Is kafka install required for the '\n",
      "                             'RisingWave workshop? [source]',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Ans: No, it is not.'},\n",
      "                {'question': 'Setup - Qn: How much free disk space should we '\n",
      "                             'have? [source]',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Ans: about 7GB free for all the containers to be '\n",
      "                         'provisioned and then the psql still needs to run and '\n",
      "                         'ingest the taxi data, so maybe 10gb in total?'},\n",
      "                {'question': 'Psycopg2 - issues when running stream-kafka '\n",
      "                             'script',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Replace psycopg2==2.9.9 with psycopg2-binary in the '\n",
      "                         'requirements.txt file [source] [another]\\n'\n",
      "                         'When you open another terminal to run the psql, '\n",
      "                         'remember to do the source command.sh step for each '\n",
      "                         'terminal session\\n'\n",
      "                         '---------------------------------------------------------------------------------------------'},\n",
      "                {'question': 'Psycopg2 - `Could not build wheels for psycopg2, '\n",
      "                             'which is required to install '\n",
      "                             'pyproject.toml-based projects`',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'If you’re using an Anaconda installation:\\n'\n",
      "                         'Cd home/\\n'\n",
      "                         'Conda install gcc\\n'\n",
      "                         'Source back to your RisingWave Venv - source '\n",
      "                         '.venv/bin/activate\\n'\n",
      "                         'Pip install psycopg2-binary\\n'\n",
      "                         'Pip install -r requirements.txt\\n'\n",
      "                         'For some reason this worked - the Conda base doesn’t '\n",
      "                         'have the GCC installed - (GNU Compiler Collection) a '\n",
      "                         'compiler system that supports various programming '\n",
      "                         'languages. Without this the it fails to install '\n",
      "                         'pyproject.toml-based projects\\n'\n",
      "                         \"“It's possible that in your specific environment, \"\n",
      "                         'the gcc installation was required at the system '\n",
      "                         'level rather than within the virtual environment. '\n",
      "                         'This can happen if the build process for psycopg2 '\n",
      "                         'tries to access system-level dependencies during '\n",
      "                         'installation.\\n'\n",
      "                         'Installing gcc in your main Python installation '\n",
      "                         '(Conda) would make it available system-wide, '\n",
      "                         'allowing any Python environment to access it when '\n",
      "                         'necessary for building packages.”\\n'\n",
      "                         'gcc stands for GNU Compiler Collection. It is a '\n",
      "                         'compiler system developed by the GNU Project that '\n",
      "                         'supports various programming languages, including C, '\n",
      "                         'C++, Objective-C, and Fortran.\\n'\n",
      "                         'GCC is widely used for compiling source code written '\n",
      "                         'in these languages into executable programs or '\n",
      "                         \"libraries. It's a key tool in the software \"\n",
      "                         'development process, particularly in the compilation '\n",
      "                         'stage where source code is translated into machine '\n",
      "                         \"code that can be executed by a computer's \"\n",
      "                         'processor.\\n'\n",
      "                         'In addition to compiling source code, GCC also '\n",
      "                         'provides various optimization options, debugging '\n",
      "                         'support, and extensive documentation, making it a '\n",
      "                         'powerful and versatile tool for developers across '\n",
      "                         'different platforms and architectures.\\n'\n",
      "                         '—-----------------------------------------------------------------------------------'},\n",
      "                {'question': 'Psycopg2 InternalError: Failed to run the query '\n",
      "                             '- when running the seed-kafka command after '\n",
      "                             'initial setup.',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Below I have listed some steps I took to rectify '\n",
      "                         'this and potentially other minor errors, in '\n",
      "                         'Windows:\\n'\n",
      "                         'Use the git bash terminal in windows.\\n'\n",
      "                         'Activate python venv from git bash: source '\n",
      "                         '.venv/Scripts/activate\\n'\n",
      "                         'Modify the seed_kafka.py file: in the first line, '\n",
      "                         'replace python3 with python.\\n'\n",
      "                         'Now from git bash, run the seed-kafka cmd. It should '\n",
      "                         'work now.\\n'\n",
      "                         'Additional Notes:\\n'\n",
      "                         'You can connect to the RisingWave cluster from '\n",
      "                         'Powershell with the command psql -h localhost -p '\n",
      "                         '4566 -d dev -U root , otherwise it asks for a '\n",
      "                         'password.\\n'\n",
      "                         'The equivalent of source commands.sh  in Powershell '\n",
      "                         'is . .\\\\commands.sh from the workshop directory.\\n'\n",
      "                         'Hope this can save you from some trouble in case '\n",
      "                         \"you're doing this workshop on Windows like I am.\\n\"\n",
      "                         '—--------------------------------------------------------------------------------------'},\n",
      "                {'question': 'Running stream-kafka script gets stuck on a loop '\n",
      "                             'with Connection Refused',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'In case the script gets stuck on\\n'\n",
      "                         '%3|1709652240.100|FAIL|rdkafka#producer-2| '\n",
      "                         '[thrd:localhost:9092/bootstrap]: '\n",
      "                         'localhost:9092/bootstrap: Connect to '\n",
      "                         'ipv4#127.0.0.1:9092 failed: Connection refused '\n",
      "                         '(after 0ms in state CONNECT)gre\\n'\n",
      "                         'after trying to load the trip data, check the logs '\n",
      "                         'of the message_queue container in docker. If it '\n",
      "                         'keeps restarting with Could not initialize seastar: '\n",
      "                         'std::runtime_error (insufficient physical memory: '\n",
      "                         'needed 4294967296 available 4067422208)  as the last '\n",
      "                         'message, then go to the docker-compose file in the '\n",
      "                         'docker folder of the project and change the ‘memory’ '\n",
      "                         'command for the message_queue service for some lower '\n",
      "                         'value.\\n'\n",
      "                         'Solution: lower the memory allocation of the service '\n",
      "                         '“message_queue” in your docker-compose file from '\n",
      "                         '4GB. If you have the “insufficient physical memory” '\n",
      "                         'error message (try 3GB)\\n'\n",
      "                         'Issue: Running psql -f '\n",
      "                         'risingwave-sql/table/trip_data.sql after starting '\n",
      "                         'services with ‘default’ values using docker-compose '\n",
      "                         'up gives the error  '\n",
      "                         '“psql:risingwave-sql/table/trip_data.sql:61: ERROR:  '\n",
      "                         'syntax error at or near \".\" LINE 60:       '\n",
      "                         \"properties.bootstrap.server='message_queue:29092'”\\n\"\n",
      "                         'Solution: Make sure you have run source commands.sh '\n",
      "                         'in each terminal window'},\n",
      "                {'question': 'For the homework questions is there a specific '\n",
      "                             'number of records that have to be processed to '\n",
      "                             'obtain the final answer?',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Use seed-kafka instead of stream-kafka to get a '\n",
      "                         'static set of results.'},\n",
      "                {'question': 'Homework - Materialized view does not guarantee '\n",
      "                             'order by warning',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'It is best to use the order by and limit clause on '\n",
      "                         'the query to the materialized view instead of the '\n",
      "                         'materialized view creation in order to guarantee '\n",
      "                         'consistent results\\n'\n",
      "                         'Homework - The answers in the homework do not match '\n",
      "                         'the provided options: You must follow the following '\n",
      "                         'steps: 1. clean-cluster 2. docker volume prune and '\n",
      "                         'use seed-kafka instead of stream-kafka. Ensure that '\n",
      "                         'the number of records is 100K.'},\n",
      "                {'question': 'How to install postgress on Linux like OS',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'For this workshop, and if you are following the view '\n",
      "                         'from Noel (2024) this requires you to install '\n",
      "                         'postgres to use it on your terminal. Found this '\n",
      "                         'steps (commands) to get it done [source]:\\n'\n",
      "                         'wget --quiet -O - '\n",
      "                         'https://www.postgresql.org/media/keys/ACCC4CF8.asc | '\n",
      "                         'sudo apt-key add -\\n'\n",
      "                         'sudo sh -c \\'echo \"deb '\n",
      "                         'http://apt.postgresql.org/pub/repos/apt/ '\n",
      "                         '$(lsb_release -cs)-pgdg main\" >> '\n",
      "                         \"/etc/apt/sources.list.d/pgdg.list'\\n\"\n",
      "                         'sudo apt update\\n'\n",
      "                         'apt install postgresql postgresql-contrib\\n'\n",
      "                         '(comment): now let’s check the service for '\n",
      "                         'postgresql\\n'\n",
      "                         'service postgresql status\\n'\n",
      "                         '(comment) If down: use the next command\\n'\n",
      "                         'service postgresql start\\n'\n",
      "                         '(comment) And your are done'},\n",
      "                {'question': 'Unable to Open Dashboard as xdg-open doesn’t '\n",
      "                             'open any browser',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Refer to the solution given in the first solution '\n",
      "                         'here:\\n'\n",
      "                         'https://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\\n'\n",
      "                         'Instead of w3m use any other browser of your '\n",
      "                         'choice.\\n'\n",
      "                         'It is just trying to open the index.html file. Which '\n",
      "                         'you can do from your File Explorer/Finder. If you’re '\n",
      "                         'on wsl try using explorer.exe index.html'},\n",
      "                {'question': 'Resolving Python Interpreter Path '\n",
      "                             'Inconsistencies in Unix-like Environments',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Example Error:\\n'\n",
      "                         'When attempting to execute a Python script named '\n",
      "                         'seed-kafka.py or server.py with the following '\n",
      "                         'shebang line specifying Python 3 as the '\n",
      "                         'interpreter:\\n'\n",
      "                         'Users may encounter the following error in a '\n",
      "                         'Unix-like environment:\\n'\n",
      "                         'This error indicates that there is a problem with '\n",
      "                         'the Python interpreter path specified in the shebang '\n",
      "                         'line. The presence of the \\\\r character suggests '\n",
      "                         'that the script was edited or created in a Windows '\n",
      "                         'environment, causing the interpreter path to be '\n",
      "                         'incorrect when executed in Unix-like environments.\\n'\n",
      "                         '2 Solutions:\\n'\n",
      "                         'Either one or the other\\n'\n",
      "                         'Update Shebang Line:\\n'\n",
      "                         'Verify Python Interpreter Path: Use the which '\n",
      "                         'python3 command to determine the path to the Python '\n",
      "                         '3 interpreter available in the current environment.\\n'\n",
      "                         'Update Shebang Line: Open the script file in a text '\n",
      "                         'editor. Modify the shebang line to point to the '\n",
      "                         'correct Python interpreter path found in the '\n",
      "                         'previous step. Ensure that the shebang line is '\n",
      "                         'consistent with the Python interpreter path in the '\n",
      "                         'execution environment.\\n'\n",
      "                         'Example Shebang Line:\\n'\n",
      "                         'Replace /usr/bin/env python3 with the correct Python '\n",
      "                         'interpreter path found using which python3.\\n'\n",
      "                         'Convert Line Endings:\\n'\n",
      "                         'Use the dos2unix command-line tool to convert the '\n",
      "                         'line endings of the script from Windows-style to '\n",
      "                         'Unix-style.\\n'\n",
      "                         'This removes the extraneous carriage return '\n",
      "                         'characters (\\\\r), resolving issues related to '\n",
      "                         'unexpected tokens and ensuring compatibility with '\n",
      "                         'Unix-like environments.\\n'\n",
      "                         'Example Command:'},\n",
      "                {'question': 'How does windowing work in Sql?',\n",
      "                 'section': 'Workshop 2 - RisingWave',\n",
      "                 'text': 'Ans : Windowing in streaming SQL involves defining a '\n",
      "                         'time-based or row-based boundary for data '\n",
      "                         'processing. It allows you to analyze and aggregate '\n",
      "                         'data over specific time intervals or based on the '\n",
      "                         'number of events received, providing a way to manage '\n",
      "                         'and organize streaming data for analysis.'},\n",
      "                {'question': 'Encountering the error \"ModuleNotFoundError: No '\n",
      "                             'module named \\'kafka.vendor.six.moves\\'\" when '\n",
      "                             'running \"from kafka import KafkaProducer\" in '\n",
      "                             'Jupyter Notebook for Module 6 Homework?',\n",
      "                 'section': 'Triggers in Mage via CLI',\n",
      "                 'text': 'Python 3.12.1, is not compatible with '\n",
      "                         'kafka-python-2.0.2. Therefore, instead of running '\n",
      "                         '\"pip install kafka-python\", you can resolve the '\n",
      "                         'issue by using \"pip install '\n",
      "                         'git+https://github.com/dpkp/kafka-python.git\". If '\n",
      "                         'you have already installed kafka-python, you need to '\n",
      "                         'run \"pip uninstall kafka-python\" before executing '\n",
      "                         '\"pip install '\n",
      "                         'git+https://github.com/dpkp/kafka-python.git\" to '\n",
      "                         'resolve the compatibility issue.\\n'\n",
      "                         'Q:In the Mage pipeline, individual blocks run '\n",
      "                         'successfully. However, when executing the pipeline '\n",
      "                         'as a whole, some blocks fail.\\n'\n",
      "                         'A: I have the following key-value pair in '\n",
      "                         'io_config.yaml file configured but still Mage blocks '\n",
      "                         'failed to generate OAuth and authenticate with GCP: '\n",
      "                         'GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ '\n",
      "                         'env_var(\\'GCP_CREDENTIALS\\') }}\". The '\n",
      "                         'GCP_CREDENTIALS variable holds the full path to the '\n",
      "                         \"service account key's JSON file. Adding the \"\n",
      "                         'following line within the failed code block resolved '\n",
      "                         'the issue: '\n",
      "                         \"os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"\n",
      "                         \"os.environ.get('GCP_CREDENTIALS').\\n\"\n",
      "                         'This occurs because the path to profiles.yml is not '\n",
      "                         'correctly specified. You can rectify this by:\\n'\n",
      "                         '“export DBT_PROFILES_DBT=path/to/profiles.yml”\\n'\n",
      "                         'Eg., /home/src/magic-zoomcamp/dbt/project_name/\\n'\n",
      "                         'Do the similar for DBT_PROJECT_DIR if getting '\n",
      "                         'similar issue with dbt_project.yml.\\n'\n",
      "                         'Once DIRs are set,:\\n'\n",
      "                         '“dbt debug –config-dir”\\n'\n",
      "                         'This would update your paths. To maintain same path '\n",
      "                         'across sessions, use the path variables in your .env '\n",
      "                         'file.\\n'\n",
      "                         'To add triggers in mage pipelines via CLI, you can '\n",
      "                         'create a trigger of type API, and copy the API '\n",
      "                         'links.\\n'\n",
      "                         'Eg. link: '\n",
      "                         'http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\n'\n",
      "                         'Then create a trigger.py as such:\\n'\n",
      "                         'import os\\n'\n",
      "                         'import requests\\n'\n",
      "                         'class MageTrigger:\\n'\n",
      "                         'OPTIONS = {\\n'\n",
      "                         '\"<pipeline_name>\": {\\n'\n",
      "                         '\"trigger_id\": 10,\\n'\n",
      "                         '\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\\n'\n",
      "                         '}\\n'\n",
      "                         '}\\n'\n",
      "                         '@staticmethod\\n'\n",
      "                         'def trigger_pipeline(pipeline_name, '\n",
      "                         'variables=None):\\n'\n",
      "                         'trigger_id = '\n",
      "                         'MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\\n'\n",
      "                         'key = MageTrigger.OPTIONS[pipeline_name][\"key\"]\\n'\n",
      "                         'endpoint = '\n",
      "                         'f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\\n'\n",
      "                         \"headers = {'Content-Type': 'application/json'}\\n\"\n",
      "                         'payload = {}\\n'\n",
      "                         'if variables is not None:\\n'\n",
      "                         \"payload['pipeline_run'] = {'variables': variables}\\n\"\n",
      "                         'response = requests.post(endpoint, headers=headers, '\n",
      "                         'json=payload)\\n'\n",
      "                         'return response\\n'\n",
      "                         'MageTrigger.trigger_pipeline(\"<pipeline_name>\")\\n'\n",
      "                         'Finally, after the mage server is up an running, '\n",
      "                         'simply this command:\\n'\n",
      "                         'python trigger.py from mage directory in terminal.\\n'\n",
      "                         'Can I do data partitioning & clustering run by dbt '\n",
      "                         'pipeline, or I would need to do this manually in '\n",
      "                         'BigQuery afterwards?\\n'\n",
      "                         'You can use this configuration in your DBT model:\\n'\n",
      "                         '{\\n'\n",
      "                         '\"field\": \"<field name>\",\\n'\n",
      "                         '\"data_type\": \"<timestamp | date | datetime | '\n",
      "                         'int64>\",\\n'\n",
      "                         '\"granularity\": \"<hour | day | month | year>\"\\n'\n",
      "                         '# Only required if data_type is \"int64\"\\n'\n",
      "                         '\"range\": {\\n'\n",
      "                         '\"start\": <int>,\\n'\n",
      "                         '\"end\": <int>,\\n'\n",
      "                         '\"interval\": <int>\\n'\n",
      "                         '}\\n'\n",
      "                         '}\\n'\n",
      "                         'and for clustering\\n'\n",
      "                         '{{\\n'\n",
      "                         'config(\\n'\n",
      "                         'materialized = \"table\",\\n'\n",
      "                         'cluster_by = \"order_id\",\\n'\n",
      "                         ')\\n'\n",
      "                         '}}\\n'\n",
      "                         'more details in: '\n",
      "                         'https://docs.getdbt.com/reference/resource-configs/bigquery-configs'},\n",
      "                {'question': 'Basic Commands',\n",
      "                 'section': 'Triggers in Mage via CLI',\n",
      "                 'text': 'Docker Commands\\n'\n",
      "                         '# Create a Docker Image from a base image\\n'\n",
      "                         'Docker run -it ubuntu bash\\n'\n",
      "                         '#List docker images\\n'\n",
      "                         'Docker images list\\n'\n",
      "                         '#List  Running containers\\n'\n",
      "                         'Docker ps -a\\n'\n",
      "                         '#List with full container ids\\n'\n",
      "                         'Docker ps -a --no-trunc\\n'\n",
      "                         '#Add onto existing image to create new image\\n'\n",
      "                         'Docker commit -a <User_Name> -m \"Message\" '\n",
      "                         'container_id New_Image_Name\\n'\n",
      "                         '# Create a Docker Image with an entrypoint from a '\n",
      "                         'base image\\n'\n",
      "                         'Docker run -it --entry_point=bash python:3.11\\n'\n",
      "                         '#Attach to a stopped container\\n'\n",
      "                         'Docker start -ai <Container_Name>\\n'\n",
      "                         '#Attach to a running container\\n'\n",
      "                         'docker exec -it <Container_ID> bash\\n'\n",
      "                         '#copying from host to container\\n'\n",
      "                         'Docker cp <SRC_PATH/file> <containerid>:<dest_path>\\n'\n",
      "                         '#copying from container to host\\n'\n",
      "                         'Docker cp <containerid>:<Srct_path> <Dest Path on '\n",
      "                         'host/file>\\n'\n",
      "                         '#Create an image from a docker file\\n'\n",
      "                         'Docker build -t <Image_Name> <Location of '\n",
      "                         'Dockerfile>\\n'\n",
      "                         '#DockerFile Options and best practices\\n'\n",
      "                         'https://devopscube.com/build-docker-image/\\n'\n",
      "                         '#Docker delete all images forcefully\\n'\n",
      "                         'docker rmi -f $(docker images -aq)\\n'\n",
      "                         '#Docker delete all containers forcefully\\n'\n",
      "                         'docker rm -f $(docker ps -qa)\\n'\n",
      "                         '#docker compose creation\\n'\n",
      "                         'https://www.composerize.com/\\n'\n",
      "                         'GCP Commands\\n'\n",
      "                         '1.     Create SSH Keys\\n'\n",
      "                         '2.     Added to the Settings of Compute Engine VM '\n",
      "                         'Instance\\n'\n",
      "                         '3.     SSH-ed into the VM Instance with a config '\n",
      "                         'similar to following\\n'\n",
      "                         'Host my-website.com\\n'\n",
      "                         'HostName my-website.com\\n'\n",
      "                         'User my-user\\n'\n",
      "                         'IdentityFile ~/.ssh/id_rsa\\n'\n",
      "                         '4.     Installed Anaconda by installing the sh file '\n",
      "                         'through bash <Anaconda.sh>\\n'\n",
      "                         '5.     Install Docker after\\n'\n",
      "                         'a.     Sudo apt-get update\\n'\n",
      "                         'b.     Sudo apt-get docker\\n'\n",
      "                         '6.     To run Docker without SUDO permissions\\n'\n",
      "                         'a.     '\n",
      "                         'https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\\n'\n",
      "                         '7.     Google cloud remote copy\\n'\n",
      "                         'a.     gcloud compute scp '\n",
      "                         'LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\\n'\n",
      "                         'Install GCP Cloud SDK on Docker Machine\\n'\n",
      "                         'https://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\\n'\n",
      "                         'sudo apt-get install apt-transport-https '\n",
      "                         'ca-certificates gnupg && echo \"deb '\n",
      "                         '[signed-by=/usr/share/keyrings/cloud.google.gpg] '\n",
      "                         'https://packages.cloud.google.com/apt cloud-sdk '\n",
      "                         'main\"| sudo tee -a '\n",
      "                         '/etc/apt/sources.list.d/google-cloud-sdk.list&& curl '\n",
      "                         'https://packages.cloud.google.com/apt/doc/apt-key.gpg '\n",
      "                         '| sudo apt-key --keyring '\n",
      "                         '/usr/share/keyrings/cloud.google.gpg add - && sudo '\n",
      "                         'apt-get update && sudo apt-get install '\n",
      "                         'google-cloud-sdk && sudo apt-get install '\n",
      "                         'google-cloud-sdk-app-engine-java && sudo apt-get '\n",
      "                         'install google-cloud-sdk-app-engine-python && gcloud '\n",
      "                         'init\\n'\n",
      "                         'Anaconda Commands\\n'\n",
      "                         '#Activate environment\\n'\n",
      "                         'Conda Activate <environment_name>\\n'\n",
      "                         '#DeActivate environment\\n'\n",
      "                         'Conda DeActivate <environment_name>\\n'\n",
      "                         '#Start iterm without conda environment\\n'\n",
      "                         'conda config --set auto_activate_base false\\n'\n",
      "                         '# Using Conda forge as default (Community driven '\n",
      "                         'packaging recipes and solutions)\\n'\n",
      "                         'https://conda-forge.org/docs/user/introduction.html\\n'\n",
      "                         'conda --version\\n'\n",
      "                         'conda update conda\\n'\n",
      "                         'conda config --add channels conda-forge\\n'\n",
      "                         'conda config --set channel_priority strict\\n'\n",
      "                         '#Using Libmamba as Solver\\n'\n",
      "                         'conda install pgcli  --solver=libmamba\\n'\n",
      "                         'Linux/MAC Commands\\n'\n",
      "                         'Starting and Stopping Services on Linux\\n'\n",
      "                         '●  \\tsudo systemctl start postgresql\\n'\n",
      "                         '●  \\tsudo systemctl stop postgresql\\n'\n",
      "                         'Starting and Stopping Services on MAC\\n'\n",
      "                         '●      launchctl start postgresql\\n'\n",
      "                         '●      launchctl stop postgresql\\n'\n",
      "                         'Identifying processes listening to a Port across '\n",
      "                         'MAC/Linux\\n'\n",
      "                         'sudo lsof -i -P -n | grep LISTEN\\n'\n",
      "                         '$ sudo netstat -tulpn | grep LISTEN\\n'\n",
      "                         '$ sudo ss -tulpn | grep LISTEN\\n'\n",
      "                         '$ sudo lsof -i:22 ## see a specific port such as 22 '\n",
      "                         '##\\n'\n",
      "                         '$ sudo nmap -sTU -O IP-address-Here\\n'\n",
      "                         'Installing a package on Debian\\n'\n",
      "                         'sudo apt install <packagename>\\n'\n",
      "                         'Listing all package on Debian\\n'\n",
      "                         'Dpkg -l | grep <packagename>\\n'\n",
      "                         'UnInstalling a package on Debian\\n'\n",
      "                         'Sudo apt remove <packagename>\\n'\n",
      "                         'Sudo apt autoclean  && sudo apt autoremove\\n'\n",
      "                         'List all Processes on Debian/Ubuntu\\n'\n",
      "                         'Ps -aux\\n'\n",
      "                         'apt-get update && apt-get install procps\\n'\n",
      "                         'apt-get install iproute2 for ss -tulpn\\n'\n",
      "                         '#Postgres Install\\n'\n",
      "                         'sudo sh -c \\'echo \"deb '\n",
      "                         'https://apt.postgresql.org/pub/repos/apt '\n",
      "                         '$(lsb_release -cs)-pgdg main\" > '\n",
      "                         \"/etc/apt/sources.list.d/pgdg.list'\\n\"\n",
      "                         'wget --quiet -O - '\n",
      "                         'https://www.postgresql.org/media/keys/ACCC4CF8.asc | '\n",
      "                         'sudo apt-key add -\\n'\n",
      "                         'sudo apt-get update\\n'\n",
      "                         'sudo apt-get -y install postgresql\\n'\n",
      "                         '#Changing Postgresql port to 5432\\n'\n",
      "                         '- sudo service postgresql stop - sed -e '\n",
      "                         \"'s/^port.*/port = 5432/' \"\n",
      "                         '/etc/postgresql/10/main/postgresql.conf > '\n",
      "                         'postgresql.conf\\n'\n",
      "                         '- sudo chown postgres postgresql.conf\\n'\n",
      "                         '- sudo mv postgresql.conf /etc/postgresql/10/main\\n'\n",
      "                         '- sudo systemctl restart postgresql'}]},\n",
      " {'course': 'machine-learning-zoomcamp',\n",
      "  'documents': [{'question': 'How do I sign up?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Machine Learning Zoomcamp FAQ\\n'\n",
      "                         'The purpose of this document is to capture '\n",
      "                         'frequently asked technical questions.\\n'\n",
      "                         'We did this for our data engineering course and it '\n",
      "                         'worked quite well. Check this document for '\n",
      "                         'inspiration on how to structure your questions and '\n",
      "                         'answers:\\n'\n",
      "                         'Data Engineering Zoomcamp FAQ\\n'\n",
      "                         'In the course GitHub repository there’s a link. Here '\n",
      "                         'it is: https://airtable.com/shryxwLd0COOEaqXo\\n'\n",
      "                         'work'},\n",
      "                {'question': 'Is it going to be live? When?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'The course videos are pre-recorded, you can start '\n",
      "                         'watching the course right now.\\n'\n",
      "                         'We will also occasionally have office hours - live '\n",
      "                         'sessions where we will answer your questions. The '\n",
      "                         'office hours sessions are recorded too.\\n'\n",
      "                         'You can see the office hours as well as the '\n",
      "                         'pre-recorded course videos in the course playlist on '\n",
      "                         'YouTube.'},\n",
      "                {'question': 'What if I miss a session?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Everything is recorded, so you won’t miss anything. '\n",
      "                         'You will be able to ask your questions for office '\n",
      "                         'hours in advance and we will cover them during the '\n",
      "                         'live stream. Also, you can always ask questions in '\n",
      "                         'Slack.'},\n",
      "                {'question': 'How much theory will you cover?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'The bare minimum. The focus is more on practice, and '\n",
      "                         \"we'll cover the theory only on the intuitive level.: \"\n",
      "                         'https://mlbookcamp.com/article/python\\n'\n",
      "                         \"For example, we won't derive the gradient update \"\n",
      "                         'rule for logistic regression (there are other great '\n",
      "                         \"courses for that), but we'll cover how to use \"\n",
      "                         'logistic regression and make sense of the results.'},\n",
      "                {'question': \"I don't know math. Can I take the course?\",\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': \"Yes! We'll cover some linear algebra in the course, \"\n",
      "                         'but in general, there will be very few formulas, '\n",
      "                         'mostly code.\\n'\n",
      "                         'Here are some interesting videos covering linear '\n",
      "                         'algebra that you can already watch: ML Zoomcamp 1.8 '\n",
      "                         '- Linear Algebra Refresher from Alexey Grigorev or '\n",
      "                         'the excellent playlist from 3Blue1Brown Vectors | '\n",
      "                         'Chapter 1, Essence of linear algebra. Never hesitate '\n",
      "                         'to ask the community for help if you have any '\n",
      "                         'question.\\n'\n",
      "                         '(Mélanie Fouesnard)'},\n",
      "                {'question': \"I filled the form, but haven't received a \"\n",
      "                             'confirmation email. Is it normal?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'The process is automated now, so you should receive '\n",
      "                         'the email eventually. If you haven’t, check your '\n",
      "                         'promotions tab in Gmail as well as spam.\\n'\n",
      "                         \"If you unsubscribed from our newsletter, you won't \"\n",
      "                         'get course related updates too.\\n'\n",
      "                         \"But don't worry, it’s not a problem. To make sure \"\n",
      "                         'you don’t miss anything, join the '\n",
      "                         '#course-ml-zoomcamp channel in Slack and our '\n",
      "                         'telegram channel with announcements. This is enough '\n",
      "                         'to follow the course.'},\n",
      "                {'question': 'How long is the course?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Approximately 4 months, but may take more if you '\n",
      "                         'want to do some extra activities (an extra project, '\n",
      "                         'an article, etc)'},\n",
      "                {'question': 'How much time do I need for this course?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Around ~10 hours per week. Timur Kamaliev did a '\n",
      "                         'detailed analysis of how much time students of the '\n",
      "                         'previous cohort needed to spend on different modules '\n",
      "                         'and projects. Full article'},\n",
      "                {'question': 'Will I get a certificate?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes, if you finish at least 2 out of 3 projects and '\n",
      "                         'review 3 peers’ Projects by the deadline, you will '\n",
      "                         'get a certificate. This is what it looks like: link. '\n",
      "                         'There’s also a version without a robot: link.'},\n",
      "                {'question': 'Will I get a certificate if I missed the midterm '\n",
      "                             'project?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': \"Yes, it's possible. See the previous answer.\"},\n",
      "                {'question': 'How much Python should I know?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Check this article. If you know everything in this '\n",
      "                         'article, you know enough. If you don’t, read the '\n",
      "                         'article and join the coursIntroduction to Pythone '\n",
      "                         'too :)\\n'\n",
      "                         'Introduction to Python – Machine Learning Bookcamp\\n'\n",
      "                         'You can follow this English course from the '\n",
      "                         'OpenClassrooms e-learning platform, which is free '\n",
      "                         'and covers the python basics for data analysis: '\n",
      "                         'Learn Python Basics for Data Analysis - '\n",
      "                         'OpenClassrooms . It is important to know some basics '\n",
      "                         'such as: how to run a Jupyter notebook, how to '\n",
      "                         'import libraries (and what libraries are), how to '\n",
      "                         'declare a variable (and what variables are) and some '\n",
      "                         'important operations regarding data analysis.\\n'\n",
      "                         '(Mélanie Fouesnard)'},\n",
      "                {'question': 'Any particular hardware requirements for the '\n",
      "                             'course or everything is mostly cloud? TIA! '\n",
      "                             \"Couldn't really find this in the FAQ.\",\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'For the Machine Learning part, all you need is a '\n",
      "                         'working laptop with an internet connection. The Deep '\n",
      "                         'Learning part is more resource intensive, but for '\n",
      "                         'that you can use a cloud (we use Saturn cloud but '\n",
      "                         'can be anything else).\\n'\n",
      "                         '(Rileen Sinha; based on response by Alexey on '\n",
      "                         'Slack)'},\n",
      "                {'question': 'How to setup TensorFlow with GPU support on '\n",
      "                             'Ubuntu?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Here is an article that worked for me: '\n",
      "                         'https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/'},\n",
      "                {'question': 'I’m new to Slack and can’t find the course '\n",
      "                             'channel. Where is it?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Here’s how you join a in Slack: '\n",
      "                         'https://slack.com/help/articles/205239967-Join-a-channel\\n'\n",
      "                         'Click “All channels” at the top of your left '\n",
      "                         \"sidebar. If you don't see this option, click “More” \"\n",
      "                         'to find it.\\n'\n",
      "                         'Browse the list of public channels in your '\n",
      "                         'workspace, or use the search bar to search by '\n",
      "                         'channel name or description.\\n'\n",
      "                         'Select a channel from the list to view it.\\n'\n",
      "                         'Click Join Channel.\\n'\n",
      "                         'Do we need to provide the GitHub link to only our '\n",
      "                         'code corresponding to the homework questions?\\n'\n",
      "                         'Yes. You are required to provide the URL to your '\n",
      "                         'repo in order to receive a grade'},\n",
      "                {'question': 'The course has already started. Can I still join '\n",
      "                             'it?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Yes, you can. You won’t be able to submit some of '\n",
      "                         'the homeworks, but you can still take part in the '\n",
      "                         'course.\\n'\n",
      "                         'In order to get a certificate, you need to submit 2 '\n",
      "                         'out of 3 course projects and review 3 peers’ '\n",
      "                         'Projects by the deadline. It means that if you join '\n",
      "                         'the course at the end of November and manage to work '\n",
      "                         'on two projects, you will still be eligible for a '\n",
      "                         'certificate.'},\n",
      "                {'question': 'When does the next iteration start?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'The course is available in the self-paced mode too, '\n",
      "                         'so you can go through the materials at any time. But '\n",
      "                         'if you want to do it as a cohort with other '\n",
      "                         'students, the next iterations will happen in '\n",
      "                         'September 2023, September 2024 (and potentially '\n",
      "                         'other Septembers as well).'},\n",
      "                {'question': 'Can I submit the homework after the due date?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'No, it’s not possible. The form is closed after the '\n",
      "                         'due date. But don’t worry, homework is not mandatory '\n",
      "                         'for finishing the course.'},\n",
      "                {'question': 'I just joined. What should I do next? How can I '\n",
      "                             'access course materials?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'Welcome to the course! Go to the course page '\n",
      "                         '(http://mlzoomcamp.com/), scroll down and start '\n",
      "                         'going through the course materials. Then read '\n",
      "                         'everything in the cohort folder for your cohort’s '\n",
      "                         'year.\\n'\n",
      "                         'Click on the links and start watching the videos. '\n",
      "                         'Also watch office hours from previous cohorts. Go to '\n",
      "                         'DTC youtube channel and click on Playlists and '\n",
      "                         'search for {course yyyy}. ML Zoomcamp was first '\n",
      "                         'launched in 2021.\\n'\n",
      "                         'Or you can just use this link: '\n",
      "                         'http://mlzoomcamp.com/#syllabus'},\n",
      "                {'question': 'What are the deadlines in this course?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'For the 2023 cohort, you can see the deadlines here '\n",
      "                         '(it’s taken from the 2023 cohort page)'},\n",
      "                {'question': 'What’s the difference between the previous '\n",
      "                             'iteration of the course (2022) and this one '\n",
      "                             '(2023)?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'There’s not much difference. There was one special '\n",
      "                         'module (BentoML) in the previous iteration of the '\n",
      "                         'course, but the rest of the modules are the same as '\n",
      "                         'in 2022. The homework this year is different.'},\n",
      "                {'question': 'The course videos are from the previous '\n",
      "                             'iteration. Will you release new ones or we’ll '\n",
      "                             'use the videos from 2021?',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'We won’t re-record the course videos. The focus of '\n",
      "                         'the course and the skills we want to teach remained '\n",
      "                         'the same, and the videos are still up-to-date.\\n'\n",
      "                         'If you haven’t taken part in the previous iteration, '\n",
      "                         'you can start watching the videos. It’ll be useful '\n",
      "                         'for you and you will learn new things. However, we '\n",
      "                         'recommend using Python 3.10 now instead of Python '\n",
      "                         '3.8.'},\n",
      "                {'question': 'Submitting learning in public links',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'When you post about what you learned from the course '\n",
      "                         'on your social media pages, use the tag #mlzoomcamp. '\n",
      "                         'When you submit your homework, there’s a section in '\n",
      "                         'the form for putting the links there. Separate '\n",
      "                         'multiple links by any whitespace character '\n",
      "                         '(linebreak, space, tab, etc).\\n'\n",
      "                         'For posting the learning in public links, you get '\n",
      "                         'extra scores. But the number of scores is limited to '\n",
      "                         '7 points: if you put more than 7 links in your '\n",
      "                         'homework form, you’ll get only 7 points.\\n'\n",
      "                         'The same content can be posted to 7 different social '\n",
      "                         'sites and still earn you 7 points if you add 7 URLs '\n",
      "                         'per week, see Alexey’s reply. (~ ellacharmed)\\n'\n",
      "                         'For midterms/capstones, the awarded points are '\n",
      "                         'doubled as the duration is longer. So for projects '\n",
      "                         'the points are capped at 14 for 14 URLs.'},\n",
      "                {'question': 'Adding community notes',\n",
      "                 'section': 'General course-related questions',\n",
      "                 'text': 'You can create your own github repository for the '\n",
      "                         'course with your notes, homework, projects, etc.\\n'\n",
      "                         'Then fork the original course repo and add a link '\n",
      "                         \"under the 'Community Notes' section to the notes \"\n",
      "                         'that are in your own repo.\\n'\n",
      "                         \"After that's done, create a pull request to sync \"\n",
      "                         'your fork with the original course repo.\\n'\n",
      "                         '(By Wesley Barreto)'},\n",
      "                {'question': 'Computing the hash for the leaderboard and '\n",
      "                             'project review',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'Leaderboard Links:\\n'\n",
      "                         '2023 - '\n",
      "                         'https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\\n'\n",
      "                         '2022 - '\n",
      "                         'https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\\n'\n",
      "                         'Python Code:\\n'\n",
      "                         'from hashlib import sha1\\n'\n",
      "                         'def compute_hash(email):\\n'\n",
      "                         'return '\n",
      "                         \"sha1(email.lower().encode('utf-8')).hexdigest()\\n\"\n",
      "                         'You need to call the function as follows:\\n'\n",
      "                         \"print(compute_hash('YOUR_EMAIL_HERE'))\\n\"\n",
      "                         'The quotes are required to denote that your email is '\n",
      "                         'a string.\\n'\n",
      "                         '(By Wesley Barreto)\\n'\n",
      "                         'You can also use this website directly by entering '\n",
      "                         'your email: http://www.sha1-online.com. Then, you '\n",
      "                         'just have to copy and paste your hashed email in the '\n",
      "                         '“research” bar of the leaderboard to get your '\n",
      "                         'scores.\\n'\n",
      "                         '(Mélanie Fouesnard)'},\n",
      "                {'question': 'wget is not recognized as an internal or '\n",
      "                             'external command',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'If you get “wget is not recognized as an internal or '\n",
      "                         'external command”, you need to install it.\\n'\n",
      "                         'On Ubuntu, run\\n'\n",
      "                         'sudo apt-get install wget\\n'\n",
      "                         'On Windows, the easiest way to install wget is to '\n",
      "                         'use Chocolatey:\\n'\n",
      "                         'choco install wget\\n'\n",
      "                         'Or you can download a binary from here and put it to '\n",
      "                         'any location in your PATH (e.g. C:/tools/)\\n'\n",
      "                         'On Mac, the easiest way to install wget is to use '\n",
      "                         'brew.\\n'\n",
      "                         'Brew install wget\\n'\n",
      "                         'Alternatively, you can use a Python wget library, '\n",
      "                         'but instead of simply using “wget” you’ll need eeeto '\n",
      "                         'use\\n'\n",
      "                         'python -m wget\\n'\n",
      "                         'You need to install it with pip first:\\n'\n",
      "                         'pip install wget\\n'\n",
      "                         'And then in your python code, for example in your '\n",
      "                         'jupyter notebook, use:\\n'\n",
      "                         'import wget\\n'\n",
      "                         'wget.download(\"URL\")\\n'\n",
      "                         'This should download whatever is at the URL in the '\n",
      "                         'same directory as your code.\\n'\n",
      "                         '(Memoona Tahira)\\n'\n",
      "                         'Alternatively, you can read a CSV file from a URL '\n",
      "                         'directly with pandas:\\n'\n",
      "                         'url = '\n",
      "                         '\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n'\n",
      "                         'df = pd.read_csv(url)\\n'\n",
      "                         'Valid URL schemes include http, ftp, s3, gs, and '\n",
      "                         'file.\\n'\n",
      "                         'In some cases you might need to bypass https '\n",
      "                         'checks:\\n'\n",
      "                         'import ssl\\n'\n",
      "                         'ssl._create_default_https_context = '\n",
      "                         'ssl._create_unverified_context\\n'\n",
      "                         'Or you can use the built-in Python functionality for '\n",
      "                         'downloading the files:\\n'\n",
      "                         'import urllib.request\\n'\n",
      "                         'url = '\n",
      "                         '\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\\n'\n",
      "                         'urllib.request.urlretrieve(url, \"housing.csv\")\\n'\n",
      "                         'Urllib.request.urlretrieve() is a standard Python '\n",
      "                         'library function available on all devices and '\n",
      "                         'platforms. URL requests and URL data retrieval are '\n",
      "                         'done with the urllib.request module.\\n'\n",
      "                         'The urlretrieve() function allows you to download '\n",
      "                         'files from URLs and save them locally. Python '\n",
      "                         'programs use it to download files from the '\n",
      "                         'internet.\\n'\n",
      "                         'On any Python-enabled device or platform, you can '\n",
      "                         'use the urllib.request.urlretrieve() function to '\n",
      "                         'download the file.\\n'\n",
      "                         '(Mohammad Emad Sharifi)'},\n",
      "                {'question': 'Retrieving csv inside notebook',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'You can use\\n'\n",
      "                         '!wget '\n",
      "                         'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n'\n",
      "                         'To download the data too. The exclamation mark !, '\n",
      "                         'lets you execute shell commands inside your '\n",
      "                         'notebooks. This works generally for shell commands '\n",
      "                         'such as ls, cp, mkdir, mv etc . . .\\n'\n",
      "                         'For instance, if you then want to move your data '\n",
      "                         'into a data directory alongside your '\n",
      "                         'notebook-containing directory, you could execute the '\n",
      "                         'following:\\n'\n",
      "                         '!mkdir -p ../data/\\n'\n",
      "                         '!mv housing.csv ../data/'},\n",
      "                {'question': 'Windows WSL and VS Code\\n'\n",
      "                             'If you have a Windows 11 device and would like '\n",
      "                             'to use the built in WSL to access linux you can '\n",
      "                             'use the Microsoft Learn link Set up a WSL '\n",
      "                             'development environment | Microsoft Learn. To '\n",
      "                             'connect this to VS Code download the Microsoft '\n",
      "                             'verified VS Code extension ‘WSL’ this will allow '\n",
      "                             'you to remotely connect to your WSL Ubuntu '\n",
      "                             'instance as if it was a virtual machine.',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': '(Tyler Simpson)'},\n",
      "                {'question': 'Uploading the homework to Github',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'This is my first time using Github to upload a code. '\n",
      "                         'I was getting the below error message when I type\\n'\n",
      "                         'git push -u origin master:\\n'\n",
      "                         'error: src refspec master does not match any\\n'\n",
      "                         'error: failed to push some refs to '\n",
      "                         \"'https://github.com/XXXXXX/1st-Homework.git'\\n\"\n",
      "                         'Solution:\\n'\n",
      "                         'The error message got fixed by running below '\n",
      "                         'commands:\\n'\n",
      "                         'git commit -m \"initial commit\"\\n'\n",
      "                         'git push origin main\\n'\n",
      "                         'If this is your first time to use Github, you will '\n",
      "                         'find a great & straightforward tutorial in this link '\n",
      "                         'https://dennisivy.com/github-quickstart\\n'\n",
      "                         '(Asia Saeed)\\n'\n",
      "                         'You can also use the “upload file” functionality '\n",
      "                         'from GitHub for that\\n'\n",
      "                         'If you write your code on Google colab you can also '\n",
      "                         'directly share it on your Github.\\n'\n",
      "                         '(By Pranab Sarma)'},\n",
      "                {'question': 'Singular Matrix Error',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': \"I'm trying to invert the matrix but I got error that \"\n",
      "                         'the matrix is singular matrix\\n'\n",
      "                         'The singular matrix error is caused by the fact that '\n",
      "                         'not every matrix can be inverted. In particular, in '\n",
      "                         'the homework it happens because you have to pay '\n",
      "                         'close attention when dealing with multiplication '\n",
      "                         '(the method .dot) since multiplication is not '\n",
      "                         'commutative! X.dot(Y) is not necessarily equal to '\n",
      "                         'Y.dot(X), so respect the order otherwise you get the '\n",
      "                         'wrong matrix.'},\n",
      "                {'question': 'Conda is not an internal command',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'I have a problem with my terminal. Command\\n'\n",
      "                         'conda create -n ml-zoomcamp python=3.9\\n'\n",
      "                         'doesn’t work. Any of 3.8/ 3.9 / 3.10 should be all '\n",
      "                         'fine\\n'\n",
      "                         'If you’re on Windows and just installed Anaconda, '\n",
      "                         'you can use Anaconda’s own terminal called “Anaconda '\n",
      "                         'Prompt”.\\n'\n",
      "                         'If you don’t have Anaconda or Miniconda, you should '\n",
      "                         'install it first\\n'\n",
      "                         '(Tatyana Mardvilko)'},\n",
      "                {'question': 'Read-in the File in Windows OS',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'How do I read the dataset with Pandas in Windows?\\n'\n",
      "                         'I used the code below but not working\\n'\n",
      "                         'df = '\n",
      "                         \"pd.read_csv('C:\\\\Users\\\\username\\\\Downloads\\\\data.csv')\\n\"\n",
      "                         'Unlike Linux/Mac OS, Windows uses the backslash (\\\\) '\n",
      "                         'to navigate the files that cause the conflict with '\n",
      "                         'Python. The problem with using the backslash is that '\n",
      "                         \"in Python, the '\\\\' has a purpose known as an escape \"\n",
      "                         'sequence. Escape sequences allow us to include '\n",
      "                         'special characters in strings, for example, \"\\\\n\" to '\n",
      "                         'add a new line or \"\\\\t\" to add spaces, etc. To avoid '\n",
      "                         'the issue we just need to add \"r\" before the file '\n",
      "                         'path and Python will treat it as a literal string '\n",
      "                         '(not an escape sequence).\\n'\n",
      "                         'Here’s how we should be loading the file instead:\\n'\n",
      "                         'df = '\n",
      "                         \"pd.read_csv(r'C:\\\\Users\\\\username\\\\Downloads\\\\data.csv')\\n\"\n",
      "                         '(Muhammad Awon)'},\n",
      "                {'question': \"'403 Forbidden' error message when you try to \"\n",
      "                             'push to a GitHub repository',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'Type the following command:\\n'\n",
      "                         'git config -l | grep url\\n'\n",
      "                         'The output should look like this:\\n'\n",
      "                         'remote.origin.url=https://github.com/github-username/github-repository-name.git\\n'\n",
      "                         'Change this to the following format and make sure '\n",
      "                         'the change is reflected using command in step 1:\\n'\n",
      "                         'git remote set-url origin '\n",
      "                         '\"https://github-username@github.com/github-username/github-repository-name.git\"\\n'\n",
      "                         '(Added by Dheeraj Karra)'},\n",
      "                {'question': 'Fatal: Authentication failed for '\n",
      "                             \"'https://github.com/username\",\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'I had a problem when I tried to push my code from '\n",
      "                         'Git Bash:\\n'\n",
      "                         'remote: Support for password authentication was '\n",
      "                         'removed on August 13, 2021.\\n'\n",
      "                         'remote: Please see '\n",
      "                         'https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls '\n",
      "                         'for information on currently recommended modes of '\n",
      "                         'authentication.\\n'\n",
      "                         'fatal: Authentication failed for '\n",
      "                         \"'https://github.com/username\\n\"\n",
      "                         'Solution:\\n'\n",
      "                         'Create a personal access token from your github '\n",
      "                         'account and use it when you make a push of your last '\n",
      "                         'changes.\\n'\n",
      "                         'https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\\n'\n",
      "                         'Bruno Bedón'},\n",
      "                {'question': 'wget: unable to resolve host address '\n",
      "                             \"'raw.githubusercontent.com'\",\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'In Kaggle, when you are trying to !wget a dataset '\n",
      "                         'from github (or any other public '\n",
      "                         'repository/location), you get the following error:\\n'\n",
      "                         'Getting  this error while trying to import data- '\n",
      "                         '!wget '\n",
      "                         'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n'\n",
      "                         '--2022-09-17 16:55:24--  '\n",
      "                         'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n'\n",
      "                         'Resolving raw.githubusercontent.com '\n",
      "                         '(raw.githubusercontent.com)... failed: Temporary '\n",
      "                         'failure in name resolution.\\n'\n",
      "                         'wget: unable to resolve host address '\n",
      "                         \"'raw.githubusercontent.com'\\n\"\n",
      "                         'Solution:\\n'\n",
      "                         'In your Kaggle notebook settings, turn on the '\n",
      "                         \"Internet for your session. It's on the settings \"\n",
      "                         'panel, on the right hand side of the Kaggle screen. '\n",
      "                         \"You'll be asked to verify your phone number so \"\n",
      "                         'Kaggle knows you are not a bot.'},\n",
      "                {'question': 'Setting up an environment using VS Code',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'I found this video quite helpful: Creating Virtual '\n",
      "                         'Environment for Python from VS Code\\n'\n",
      "                         '[Native Jupiter Notebooks support in VS Code] In VS '\n",
      "                         'Code you can also have a native Jupiter Notebooks '\n",
      "                         'support, i.e. you do not need to open a web browser '\n",
      "                         'to code in a Notebook. If you have port forwarding '\n",
      "                         'enabled + run a ‘jupyter notebook ‘ command from a '\n",
      "                         'remote machine + have a remote connection configured '\n",
      "                         'in .ssh/config (as Alexey’s video suggests) - VS '\n",
      "                         'Code can execute remote Jupyter Notebooks files on a '\n",
      "                         'remote server from your local machine: '\n",
      "                         'https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\\n'\n",
      "                         '[Git support from VS Code] You can work with Github '\n",
      "                         'from VSCode - staging and commits are easy from the '\n",
      "                         'VS Code’s UI:  '\n",
      "                         'https://code.visualstudio.com/docs/sourcecontrol/overview\\n'\n",
      "                         '(Added by Ivan Brigida)'},\n",
      "                {'question': 'Conda Environment Setup',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'With regards to creating an environment for the '\n",
      "                         'project, do we need to run the command \"conda create '\n",
      "                         '-n .......\" and \"conda activate ml-zoomcamp\" '\n",
      "                         'everytime we open vs code to work on the project?\\n'\n",
      "                         'Answer:\\n'\n",
      "                         '\"conda create -n ....\" is just run the first time to '\n",
      "                         'create the environment. Once created, you just need '\n",
      "                         'to run \"conda activate ml-zoomcamp\" whenever you '\n",
      "                         'want to use it.\\n'\n",
      "                         '(Added by Wesley Barreto)\\n'\n",
      "                         'conda env export > environment.yml will also allow '\n",
      "                         'you to reproduce your existing environment in a YAML '\n",
      "                         'file.  You can then recreate it with conda env '\n",
      "                         'create -f environment.yml'},\n",
      "                {'question': 'Floating Point Precision',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'I was doing Question 7 from Week1 Homework and with '\n",
      "                         'step6: Invert XTX, I created the inverse. Now, an '\n",
      "                         'inverse when multiplied by the original matrix '\n",
      "                         'should return in an Identity matrix. But when I '\n",
      "                         'multiplied the inverse with the original matrix, it '\n",
      "                         'gave a matrix like this:\\n'\n",
      "                         'Inverse * Original:\\n'\n",
      "                         '[[ 1.00000000e+00 -1.38777878e-16]\\n'\n",
      "                         '[ 3.16968674e-13  1.00000000e+00]]\\n'\n",
      "                         'Solution:\\n'\n",
      "                         \"It's because floating point math doesn't work well \"\n",
      "                         'on computers as shown here: '\n",
      "                         'https://stackoverflow.com/questions/588004/is-floating-point-math-broken\\n'\n",
      "                         '(Added by Wesley Barreto)'},\n",
      "                {'question': 'What does pandas.DataFrame.info() do?',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'Answer:\\n'\n",
      "                         'It prints the information about the dataset like:\\n'\n",
      "                         'Index datatype\\n'\n",
      "                         'No. of entries\\n'\n",
      "                         'Column information with not-null count and datatype\\n'\n",
      "                         'Memory usage by dataset\\n'\n",
      "                         'We use it as:\\n'\n",
      "                         'df.info()\\n'\n",
      "                         '(Added by Aadarsha Shrestha & Emoghena Itakpe)'},\n",
      "                {'question': \"NameError: name 'np' is not defined\",\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'Pandas and numpy libraries are not being imported\\n'\n",
      "                         \"NameError: name 'np' is not defined\\n\"\n",
      "                         \"NameError: name 'pd' is not defined\\n\"\n",
      "                         \"If you're using numpy or pandas, make sure you use \"\n",
      "                         'the first few lines before anything else.\\n'\n",
      "                         'import pandas as pd\\n'\n",
      "                         'import numpy as np\\n'\n",
      "                         'Added by Manuel Alejandro Aponte'},\n",
      "                {'question': 'How to select column by dtype',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'What if there were hundreds of columns? How do you '\n",
      "                         'get the columns only with numeric or object data in '\n",
      "                         'a more concise way?\\n'\n",
      "                         'df.select_dtypes(include=np.number).columns.tolist()\\n'\n",
      "                         \"df.select_dtypes(include='object').columns.tolist()\\n\"\n",
      "                         'Added by Gregory Morris'},\n",
      "                {'question': 'How to identify the shape of dataset in Pandas',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'There are many ways to identify the shape of '\n",
      "                         'dataset, one of them is using .shape attribute!\\n'\n",
      "                         'df.shape\\n'\n",
      "                         'df.shape[0] # for identify the number of rows\\n'\n",
      "                         'df.shape[1] # for identify the number of columns\\n'\n",
      "                         'Added by Radikal Lukafiardi'},\n",
      "                {'question': 'How to avoid Value errors with array shapes in '\n",
      "                             'homework?',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'First of all use np.dot for matrix multiplication. '\n",
      "                         'When you compute matrix-matrix multiplication you '\n",
      "                         'should understand that order of multiplying is '\n",
      "                         'crucial and affects the result of the '\n",
      "                         'multiplication!\\n'\n",
      "                         'Dimension Mismatch\\n'\n",
      "                         'To perform matrix multiplication, the number of '\n",
      "                         'columns in the 1st matrix should match the number of '\n",
      "                         'rows in the 2nd matrix. You can rearrange the order '\n",
      "                         'to make sure that this satisfies the condition.\\n'\n",
      "                         'Added by Leah Gotladera'},\n",
      "                {'question': 'Question 5: How and why do we replace the NaN '\n",
      "                             'values with average of the column?',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'You would first get the average of the column and '\n",
      "                         'save it to a variable, then replace the NaN values '\n",
      "                         'with the average variable.\\n'\n",
      "                         'This method is called imputing - when you have NaN/ '\n",
      "                         'null values in a column, but you do not want to get '\n",
      "                         'rid of the row because it has valuable information '\n",
      "                         'contributing to other columns.\\n'\n",
      "                         'Added by Anneysha Sarkar'},\n",
      "                {'question': 'Question 7: Mathematical formula for linear '\n",
      "                             'regression',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'In Question 7 we are asked to calculate\\n'\n",
      "                         'The initial problem  can be solved by this, where a '\n",
      "                         'Matrix X is multiplied by some unknown weights w '\n",
      "                         'resulting in the target y.\\n'\n",
      "                         'Additional reading and videos:\\n'\n",
      "                         'Ordinary least squares\\n'\n",
      "                         'Multiple Linear Regression in Matrix Form\\n'\n",
      "                         'Pseudoinverse Solution to OLS\\n'\n",
      "                         'Added by Sylvia Schmitt\\n'\n",
      "                         'with commends from Dmytro Durach'},\n",
      "                {'question': 'Question 7: FINAL MULTIPLICATION not having 5 '\n",
      "                             'column',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'This is most likely that you interchanged the first '\n",
      "                         'step of the multiplication\\n'\n",
      "                         'You used  instead of\\n'\n",
      "                         'Added by Emmanuel Ikpesu'},\n",
      "                {'question': 'Question 7: Multiplication operators.',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'Note, that matrix multiplication (matrix-matrix, '\n",
      "                         'matrix-vector multiplication) can be written as * '\n",
      "                         'operator in some sources, but performed as @ '\n",
      "                         'operator or np.matmul() via numpy. * operator '\n",
      "                         'performs element-wise multiplication (Hadamard '\n",
      "                         'product).\\n'\n",
      "                         'numpy.dot() or ndarray.dot() can be used, but for '\n",
      "                         'matrix-matrix multiplication @ or np.matmul() is '\n",
      "                         'preferred (as per numpy doc).\\n'\n",
      "                         'If multiplying by a scalar numpy.multiply() or * is '\n",
      "                         'preferred.\\n'\n",
      "                         'Added by Andrii Larkin'},\n",
      "                {'question': 'Error launching Jupyter notebook',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'If you face an error kind of ImportError: cannot '\n",
      "                         \"import name 'contextfilter' from 'jinja2' \"\n",
      "                         '(anaconda\\\\lib\\\\site-packages\\\\jinja2\\\\__init__.py) '\n",
      "                         'when launching a new notebook for a brand new '\n",
      "                         'environment.\\n'\n",
      "                         'Switch to the main environment and run \"pip install '\n",
      "                         'nbconvert --upgrade\".\\n'\n",
      "                         'Added by George Chizhmak'},\n",
      "                {'question': 'wget '\n",
      "                             'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv '\n",
      "                             'hangs on MacOS Ventura M1',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': 'If you face this situation and see IPv6 addresses in '\n",
      "                         'the terminal, go to your System Settings > Network > '\n",
      "                         'your network connection > Details > Configure IPv6 > '\n",
      "                         'set to Manually > OK. Then try again'},\n",
      "                {'question': 'In case you are using mac os and having trouble '\n",
      "                             'with WGET',\n",
      "                 'section': '1. Introduction to Machine Learning',\n",
      "                 'text': \"Wget doesn't ship with macOS, so there are other \"\n",
      "                         'alternatives to use.\\n'\n",
      "                         'No worries, we got curl:\\n'\n",
      "                         'example:\\n'\n",
      "                         'curl -o ./housing.csv '\n",
      "                         'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n'\n",
      "                         'Explanations:\\n'\n",
      "                         'curl: a utility for retrieving information from the '\n",
      "                         'internet.\\n'\n",
      "                         '-o: Tell it to store the result as a file.\\n'\n",
      "                         \"filename: You choose the file's name.\\n\"\n",
      "                         'Links: Put the web address (URL) here, and cURL will '\n",
      "                         'extract data from it and save it under the name you '\n",
      "                         'provide.\\n'\n",
      "                         'More about it at:\\n'\n",
      "                         'Curl Documentation\\n'\n",
      "                         'Added by David Espejo'},\n",
      "                {'question': 'How to output only a certain number of decimal '\n",
      "                             'places',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'You can use round() function or f-strings\\n'\n",
      "                         'round(number, 4)  - this will round number up to 4 '\n",
      "                         'decimal places\\n'\n",
      "                         \"print(f'Average mark for the Homework is {avg:.3f}') \"\n",
      "                         '- using F string\\n'\n",
      "                         'Also there is pandas.Series. round idf you need to '\n",
      "                         'round values in the whole Series\\n'\n",
      "                         'Please check the documentation\\n'\n",
      "                         'https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\\n'\n",
      "                         'Added by Olga Rudakova'},\n",
      "                {'question': 'How do I get started with Week 2?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'Here are the crucial links for this Week 2 that '\n",
      "                         'starts September 18, 2023\\n'\n",
      "                         'Ask questions for Live Sessions: '\n",
      "                         'https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\\n'\n",
      "                         'Calendar for weekly meetings: '\n",
      "                         'https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\\n'\n",
      "                         'Week 2 HW: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\\n'\n",
      "                         'Submit HW Week 2: '\n",
      "                         'https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform '\n",
      "                         '(also available at the bottom of the above link)\\n'\n",
      "                         'All HWs: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\n'\n",
      "                         'GitHub for theory: '\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\n'\n",
      "                         'Youtube Link: 2.X --- '\n",
      "                         'https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\\n'\n",
      "                         'FAQs: '\n",
      "                         'https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n'\n",
      "                         '~~Nukta Bhatia~~'},\n",
      "                {'question': 'Checking long tail of data',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'We can use histogram:\\n'\n",
      "                         'import pandas as pd\\n'\n",
      "                         'import matplotlib.pyplot as plt\\n'\n",
      "                         'import seaborn as sns\\n'\n",
      "                         '# Load the data\\n'\n",
      "                         'url = '\n",
      "                         \"'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\\n\"\n",
      "                         'df = pd.read_csv(url)\\n'\n",
      "                         '# EDA\\n'\n",
      "                         \"sns.histplot(df['median_house_value'], kde=False)\\n\"\n",
      "                         'plt.show()\\n'\n",
      "                         'OR ceck skewness and describe:\\n'\n",
      "                         \"print(df['median_house_value'].describe())\\n\"\n",
      "                         \"# Calculate the skewness of the 'median_house_value' \"\n",
      "                         'variable\\n'\n",
      "                         \"skewness = df['median_house_value'].skew()\\n\"\n",
      "                         '# Print the skewness value\\n'\n",
      "                         'print(\"Skewness of \\'median_house_value\\':\", '\n",
      "                         'skewness)\\n'\n",
      "                         '(Mohammad Emad Sharifi)'},\n",
      "                {'question': 'LinAlgError: Singular matrix',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'It’s possible that when you follow the videos, '\n",
      "                         'you’ll get a Singular Matrix error. We will explain '\n",
      "                         'why it happens in the Regularization video. Don’t '\n",
      "                         'worry, it’s normal that you have it.\\n'\n",
      "                         'You can also have an error because you did the '\n",
      "                         'inverse of X once in your code and you’re doing it a '\n",
      "                         'second time.\\n'\n",
      "                         '(Added by Cécile Guillot)'},\n",
      "                {'question': 'California housing dataset',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'You can find a detailed description of the dataset '\n",
      "                         'ere '\n",
      "                         'https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\\n'\n",
      "                         'KS'},\n",
      "                {'question': 'Getting NaNs after applying .mean()',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'I was using for loops to apply rmse to list of y_val '\n",
      "                         'and y_pred. But the resulting rmse is all nan.\\n'\n",
      "                         'I found out that the problem was when my data '\n",
      "                         'reached the mean step after squaring the error in '\n",
      "                         'the rmse function. Turned out there were nan in the '\n",
      "                         'array, then I traced the problem back to where I '\n",
      "                         'first started to split the data: I had only use '\n",
      "                         'fillna(0) on the train data, not on the validation '\n",
      "                         'and test data. So the problem was fixed after I '\n",
      "                         'applied fillna(0) to all the dataset (train, val, '\n",
      "                         'test). Voila, my for loops to get rmse from all the '\n",
      "                         'seed values work now.\\n'\n",
      "                         'Added by Sasmito Yudha Husada'},\n",
      "                {'question': 'Target variable transformation',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'Why should we transform the target variable to '\n",
      "                         'logarithm distribution? Do we do this for all '\n",
      "                         'machine learning projects?\\n'\n",
      "                         'Only if you see that your target is highly skewed. '\n",
      "                         'The easiest way to evaluate this is by plotting the '\n",
      "                         'distribution of the target variable.\\n'\n",
      "                         'This can help to understand skewness and how it can '\n",
      "                         'be applied to the distribution of your data set.\\n'\n",
      "                         'https://en.wikipedia.org/wiki/Skewness\\n'\n",
      "                         'Pastor Soto'},\n",
      "                {'question': 'Reading the dataset directly from github',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'The dataset can be read directly to pandas dataframe '\n",
      "                         'from the github link using the technique shown '\n",
      "                         'below\\n'\n",
      "                         'dfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'Loading the dataset directly through Kaggle '\n",
      "                             'Notebooks',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'For users of kaggle notebooks, the dataset can be '\n",
      "                         'loaded through widget using the below command. '\n",
      "                         'Please remember that ! before wget is essential\\n'\n",
      "                         '!wget '\n",
      "                         'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\\n'\n",
      "                         'Once the dataset is loaded to the kaggle notebook '\n",
      "                         'server, it can be read through the below pandas '\n",
      "                         'command\\n'\n",
      "                         \"df = pd.read_csv('housing.csv')\\n\"\n",
      "                         'Harish Balasundaram'},\n",
      "                {'question': 'Filter a dataset by using its values',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'We can filter a dataset by using its values as '\n",
      "                         'below.\\n'\n",
      "                         'df = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | '\n",
      "                         '(df[\"ocean_proximity\"] == \"INLAND\")]\\n'\n",
      "                         'You can use | for ‘OR’, and & for ‘AND’\\n'\n",
      "                         'Alternative:\\n'\n",
      "                         \"df = df[df['ocean_proximity'].isin(['<1H OCEAN', \"\n",
      "                         \"'INLAND'])]\\n\"\n",
      "                         'Radikal Lukafiardi'},\n",
      "                {'question': 'Alternative way to load the data using requests',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'Above users showed how to load the dataset directly '\n",
      "                         'from github. Here is another useful way of doing '\n",
      "                         'this using the `requests` library:\\n'\n",
      "                         '# Get data for homework\\n'\n",
      "                         'import requests\\n'\n",
      "                         'url = '\n",
      "                         \"'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\\n\"\n",
      "                         'response = requests.get(url)\\n'\n",
      "                         'if response.status_code == 200:\\n'\n",
      "                         \"with open('housing.csv', 'wb') as file:\\n\"\n",
      "                         'file.write(response.content)\\n'\n",
      "                         'else:\\n'\n",
      "                         'print(\"Download failed.\")\\n'\n",
      "                         'Tyler Simpson'},\n",
      "                {'question': 'Null column is appearing even if I applied '\n",
      "                             '.fillna()',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'When creating a duplicate of your dataframe by doing '\n",
      "                         'the following:\\n'\n",
      "                         'X_train = df_train\\n'\n",
      "                         'X_val = df_val\\n'\n",
      "                         'You’re still referencing the original variable, this '\n",
      "                         'is called a shallow copy. You can make sure that no '\n",
      "                         'references are attaching both variables and still '\n",
      "                         'keep the copy of the data do the following to create '\n",
      "                         'a deep copy:\\n'\n",
      "                         'X_train = df_train.copy()\\n'\n",
      "                         'X_val = df_val.copy()\\n'\n",
      "                         'Added by Ixchel García'},\n",
      "                {'question': 'Can I use Scikit-Learn’s train_test_split for '\n",
      "                             'this week?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'Yes, you can. Here we implement it ourselves to '\n",
      "                         'better understand how it works, but later we will '\n",
      "                         'only rely on Scikit-Learn’s functions. If you want '\n",
      "                         'to start using it earlier — feel free to do it'},\n",
      "                {'question': 'Can I use LinearRegression from Scikit-Learn for '\n",
      "                             'this week?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'Yes, you can. We will also do that next week, so '\n",
      "                         'don’t worry, you will learn how to do it.'},\n",
      "                {'question': 'Corresponding Scikit-Learn functions for Linear '\n",
      "                             'Regression (with and without Regularization)',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'What are equivalents in Scikit-Learn for the linear '\n",
      "                         'regression with and without regularization used in '\n",
      "                         'week 2.\\n'\n",
      "                         'Corresponding function for model without '\n",
      "                         'regularization:\\n'\n",
      "                         'sklearn.linear_model.LinearRegression\\n'\n",
      "                         'Corresponding function for model with '\n",
      "                         'regularization:\\n'\n",
      "                         'sklearn.linear_model.Ridge\\n'\n",
      "                         'The linear model from Scikit-Learn are explained  '\n",
      "                         'here:\\n'\n",
      "                         'https://scikit-learn.org/stable/modules/linear_model.html\\n'\n",
      "                         'Added by Sylvia Schmitt'},\n",
      "                {'question': 'Question 4: what is `r`, is it the same as '\n",
      "                             '`alpha` in sklearn.Ridge()?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': '`r` is a regularization parameter.\\n'\n",
      "                         'It’s similar to `alpha` in sklearn.Ridge(), as both '\n",
      "                         'control the \"strength\" of regularization (increasing '\n",
      "                         'both will lead to stronger regularization), but '\n",
      "                         \"mathematically not quite, here's how both are used:\\n\"\n",
      "                         'sklearn.Ridge()\\n'\n",
      "                         '||y - Xw||^2_2 + alpha * ||w||^2_2\\n'\n",
      "                         'lesson’s notebook (`train_linear_regression_reg` '\n",
      "                         'function)\\n'\n",
      "                         'XTX = XTX + r * np.eye(XTX.shape[0])\\n'\n",
      "                         '`r` adds “noise” to the main diagonal to prevent '\n",
      "                         'multicollinearity, which “breaks” finding inverse '\n",
      "                         'matrix.'},\n",
      "                {'question': 'Why linear regression doesn’t provide a '\n",
      "                             '“perfect” fit?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'Q: “In lesson 2.8 why is y_pred different from y? '\n",
      "                         'After all, we trained X_train to get the weights '\n",
      "                         'that when multiplied by X_train should give exactly '\n",
      "                         'y, or?”\\n'\n",
      "                         'A: linear regression is a pretty simple model, it '\n",
      "                         'neither can nor should fit 100% (nor any other '\n",
      "                         'model, as this would be the sign of overfitting). '\n",
      "                         'This picture might illustrate some intuition behind '\n",
      "                         'this, imagine X is a single feature:\\n'\n",
      "                         'As our model is linear, how would you draw a line to '\n",
      "                         'fit all the \"dots\"?\\n'\n",
      "                         'You could \"fit\" all the \"dots\" on this pic using '\n",
      "                         'something like scipy.optimize.curve_fit (non-linear '\n",
      "                         'least squares) if you wanted to, but imagine how it '\n",
      "                         'would perform on previously unseen data.\\n'\n",
      "                         'Added by Andrii Larkin'},\n",
      "                {'question': 'Random seed 42',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'One of the questions on the homework calls for using '\n",
      "                         'a random seed of 42. When using 42, all my missing '\n",
      "                         'values ended up in my training dataframe and not my '\n",
      "                         'validation or test dataframes, why is that?\\n'\n",
      "                         'The purpose of the seed value is to randomly '\n",
      "                         'generate the proportion split. Using a seed of 42 '\n",
      "                         'ensures that all learners are on the same page by '\n",
      "                         'getting the same behavior (in this case, all missing '\n",
      "                         'values ending up in the training dataframe). If '\n",
      "                         'using a different seed value (e.g. 9), missing '\n",
      "                         'values will then appear in all other dataframes.'},\n",
      "                {'question': 'Shuffling the initial dataset using pandas '\n",
      "                             'built-in function',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'It is possible to do the shuffling of the dataset '\n",
      "                         'with the pandas built-in function '\n",
      "                         'pandas.DataFrame.sample.The complete dataset can be '\n",
      "                         'shuffled including resetting the index with the '\n",
      "                         'following commands:\\n'\n",
      "                         'Setting frac=1 will result in returning a shuffled '\n",
      "                         'version of the complete Dataset.\\n'\n",
      "                         'Setting random_state=seed will result in the same '\n",
      "                         'randomization as used in the course resources.\\n'\n",
      "                         'df_shuffled = df.sample(frac=1, random_state=seed)\\n'\n",
      "                         'df_shuffled.reset_index(drop=True, inplace=True)\\n'\n",
      "                         'Added by Sylvia Schmitt'},\n",
      "                {'question': 'The answer I get for one of the homework '\n",
      "                             \"questions doesn't match any of the options. What \"\n",
      "                             'should I do?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'That’s normal. We all have different environments: '\n",
      "                         'our computers have different versions of OS and '\n",
      "                         'different versions of libraries — even different '\n",
      "                         'versions of Python.\\n'\n",
      "                         'If it’s the case, just select the option that’s '\n",
      "                         'closest to your answer'},\n",
      "                {'question': 'Meaning of mean in homework 2, question 3',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'In question 3 of HW02 it is mentioned: ‘For '\n",
      "                         'computing the mean, use the training only’. What '\n",
      "                         'does that mean?\\n'\n",
      "                         'It means that you should use only the training data '\n",
      "                         'set for computing the mean, not validation or  test '\n",
      "                         'data set. This is how you can calculate the mean\\n'\n",
      "                         \"df_train['column_name'].mean( )\\n\"\n",
      "                         'Another option:\\n'\n",
      "                         'df_train[‘column_name’].describe()\\n'\n",
      "                         '(Bhaskar Sarma)'},\n",
      "                {'question': 'When should we transform the target variable to '\n",
      "                             'logarithm distribution?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'When the target variable has a long tail '\n",
      "                         'distribution, like in prices, with a wide range, you '\n",
      "                         'can transform the target variable with np.log1p() '\n",
      "                         'method, but be aware if your target variable has '\n",
      "                         'negative values, this method will not work'},\n",
      "                {'question': 'ValueError: shapes not aligned',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'If we try to perform an arithmetic operation between '\n",
      "                         '2 arrays of different shapes or different '\n",
      "                         'dimensions, it throws an error like operands could '\n",
      "                         'not be broadcast together with shapes. There are '\n",
      "                         'some scenarios when broadcasting can occur and when '\n",
      "                         'it fails.\\n'\n",
      "                         'If this happens sometimes we can use * operator '\n",
      "                         'instead of dot() method to solve the issue. So that '\n",
      "                         'the error is solved and also we get the dot '\n",
      "                         'product.\\n'\n",
      "                         '(Santhosh Kumar)'},\n",
      "                {'question': 'How to copy a dataframe without changing the '\n",
      "                             'original dataframe?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'Copy of a dataframe is made with X_copy = X.copy().\\n'\n",
      "                         'This is called creating a deep copy.  Otherwise it '\n",
      "                         'will keep changing the original dataframe if used '\n",
      "                         'like this: X_copy = X.\\n'\n",
      "                         'Any changes to X_copy will reflect back to X. This '\n",
      "                         'is not a real copy, instead it is a “view”.\\n'\n",
      "                         '(Memoona Tahira)'},\n",
      "                {'question': 'What does ‘long tail’ mean?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'One of the most important characteristics of the '\n",
      "                         'normal distribution is that mean=median=mode, this '\n",
      "                         'means that the most popular value, the mean of the '\n",
      "                         'distribution and 50% of the sample are under the '\n",
      "                         'same value, this is equivalent to say that the area '\n",
      "                         'under the curve (black) is the same on the left and '\n",
      "                         'on the right. The long tail (red curve) is the '\n",
      "                         'result of having a few observations with high '\n",
      "                         'values, now the behaviour of the distribution '\n",
      "                         'changes, first of all, the area is different on each '\n",
      "                         'side and now the mean, median and mode are '\n",
      "                         'different. As a consequence, the mean is no longer '\n",
      "                         'representative, the range is larger than before and '\n",
      "                         'the probability of being on the left or on the right '\n",
      "                         'is not the same.\\n'\n",
      "                         '(Tatiana Dávila)'},\n",
      "                {'question': 'What is standard deviation?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'In statistics, the standard deviation is a measure '\n",
      "                         'of the amount of variation or dispersion of a set of '\n",
      "                         'values. A low standard deviation indicates that the '\n",
      "                         'values tend to be close to the mean (also called the '\n",
      "                         'expected value) of the set, while a high standard '\n",
      "                         'deviation indicates that the values are spread out '\n",
      "                         'over a wider range. [Wikipedia] The formula to '\n",
      "                         'calculate standard deviation is:\\n'\n",
      "                         '(Aadarsha Shrestha)'},\n",
      "                {'question': 'Do we need to apply regularization techniques '\n",
      "                             'always? Or only in certain scenarios?',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'The application of regularization depends on the '\n",
      "                         'specific situation and problem. It is recommended to '\n",
      "                         'consider it when training machine learning models, '\n",
      "                         'especially with small datasets or complex models, to '\n",
      "                         'prevent overfitting. However, its necessity varies '\n",
      "                         'depending on the data quality and size. Evaluate '\n",
      "                         'each case individually to determine if it is '\n",
      "                         'needed.\\n'\n",
      "                         '(Daniel Muñoz Viveros)'},\n",
      "                {'question': 'Shortcut: define functions for faster execution',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'As it speeds up the development:\\n'\n",
      "                         'prepare_df(initial_df, seed, fill_na_type)  - that '\n",
      "                         'prepared all 3 dataframes and 3 y_vectors. Fillna() '\n",
      "                         'can be done before the initial_df is split.\\n'\n",
      "                         'Of course, you can reuse other functions: rmse() and '\n",
      "                         'train_linear_regression(X,y,r) from the class '\n",
      "                         'notebook\\n'\n",
      "                         '(Ivan Brigida)'},\n",
      "                {'question': 'How to use pandas to find standard deviation',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'If we have a list or series of data for example x = '\n",
      "                         '[1,2,3,4,5]. We can use pandas to find the standard '\n",
      "                         'deviation. We can pass our list into panda series '\n",
      "                         'and call standard deviation directly on the series '\n",
      "                         'pandas.Series(x).std().\\n'\n",
      "                         '(Quinn Avila)'},\n",
      "                {'question': 'Standard Deviation Differences in Numpy and '\n",
      "                             'Pandas',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'Numpy and Pandas packages use different equations to '\n",
      "                         'compute the standard deviation. Numpy uses  '\n",
      "                         'population standard deviation, whereas pandas uses '\n",
      "                         'sample standard deviation by default.\\n'\n",
      "                         'Numpy\\n'\n",
      "                         'Pandas\\n'\n",
      "                         'pandas default standard deviation is computed using '\n",
      "                         'one degree of freedom. You can change degree in of '\n",
      "                         'freedom in NumPy to change this to unbiased '\n",
      "                         'estimator by using ddof parameter:\\n'\n",
      "                         'import numpy as np\\n'\n",
      "                         'np.std(df.weight, ddof=1)\\n'\n",
      "                         'The result will be similar if we change the dof = 1 '\n",
      "                         'in numpy\\n'\n",
      "                         '(Harish Balasundaram)'},\n",
      "                {'question': 'Standard deviation using Pandas built in '\n",
      "                             'Function',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'In pandas you can use built in Pandas function names '\n",
      "                         'std() to get standard deviation. For example\\n'\n",
      "                         \"df['column_name'].std() to get standard deviation of \"\n",
      "                         'that column.\\n'\n",
      "                         \"df[['column_1', 'column_2']].std() to get standard \"\n",
      "                         'deviation of multiple columns.\\n'\n",
      "                         '(Khurram Majeed)'},\n",
      "                {'question': 'How to combine train and validation datasets',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'Use ‘pandas.concat’ function '\n",
      "                         '(https://pandas.pydata.org/docs/reference/api/pandas.concat.html) '\n",
      "                         'to combine two dataframes. To combine two numpy '\n",
      "                         'arrays use numpy.concatenate '\n",
      "                         '(https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) '\n",
      "                         'function. So the code would be as follows:\\n'\n",
      "                         'df_train_combined = pd.concat([df_train, df_val])\\n'\n",
      "                         'y_train = np.concatenate((y_train, y_val), axis=0)\\n'\n",
      "                         '(George Chizhmak)'},\n",
      "                {'question': 'Understanding RMSE and how to calculate RMSE '\n",
      "                             'score',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'The Root Mean Squared Error (RMSE) is one of the '\n",
      "                         'primary metrics to evaluate the performance of a '\n",
      "                         'regression model. It calculates the average '\n",
      "                         \"deviation between the model's predicted values and \"\n",
      "                         'the actual observed values, offering insight into '\n",
      "                         \"the model's ability to accurately forecast the \"\n",
      "                         'target variable. To calculate RMSE score:\\n'\n",
      "                         'Libraries needed\\n'\n",
      "                         'import numpy as np\\n'\n",
      "                         'from sklearn.metrics import mean_squared_error\\n'\n",
      "                         'mse = mean_squared_error(actual_values, '\n",
      "                         'predicted_values)\\n'\n",
      "                         'rmse = np.sqrt(mse)\\n'\n",
      "                         'print(\"Root Mean Squared Error (RMSE):\", rmse)\\n'\n",
      "                         '(Aminat Abolade)'},\n",
      "                {'question': 'What syntax use in Pandas for multiple '\n",
      "                             'conditions using logical AND and OR',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'If you would like to use multiple conditions as an '\n",
      "                         'example below you will get the error. The correct '\n",
      "                         'syntax for OR is |, and for AND is &\\n'\n",
      "                         '(Olga Rudakova)\\n'\n",
      "                         '–'},\n",
      "                {'question': 'Deep dive into normal equation for regression',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'I found this video pretty usual for understanding '\n",
      "                         'how we got the normal form with linear regression '\n",
      "                         'Normal Equation Derivation for Regression'},\n",
      "                {'question': 'Useful Resource for Missing Data Treatment\\n'\n",
      "                             'https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': '(Hrithik Kumar Advani)'},\n",
      "                {'question': 'Caution for applying log transformation in '\n",
      "                             'Week-2 2023 cohort homework',\n",
      "                 'section': '2. Machine Learning for Regression',\n",
      "                 'text': 'The instruction for applying log transformation to '\n",
      "                         'the ‘median_house_value’ variable is provided before '\n",
      "                         'Q3 in the homework for Week-2 under the ‘Prepare and '\n",
      "                         'split the dataset’ heading.\\n'\n",
      "                         'However, this instruction is absent in the '\n",
      "                         'subsequent questions of the homework, and I got '\n",
      "                         'stuck with Q5 for a long time, trying to figure out '\n",
      "                         'why my RMSE was so huge, when it clicked to me that '\n",
      "                         'I forgot to apply log transformation to the target '\n",
      "                         'variable. Please remember to apply log '\n",
      "                         'transformation to the target variable for each '\n",
      "                         'question.\\n'\n",
      "                         '(Added by Soham Mundhada)'},\n",
      "                {'question': 'What sklearn version is Alexey using in the '\n",
      "                             'youtube videos?',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Version 0.24.2 and Python 3.8.11\\n'\n",
      "                         '(Added by Diego Giraldo)'},\n",
      "                {'question': 'How do I get started with Week 3?',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Week 3 HW: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\\n'\n",
      "                         'Submit HW Week 3: '\n",
      "                         'https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\\n'\n",
      "                         'All HWs: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\n'\n",
      "                         'Evaluation Matrix: '\n",
      "                         'https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\n'\n",
      "                         'GitHub for theory: '\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\n'\n",
      "                         'Youtube Link: 3.X --- '\n",
      "                         'https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\\n'\n",
      "                         '~~Nukta Bhatia~~'},\n",
      "                {'question': 'Could not convert string to float:’Nissan’rt '\n",
      "                             \"string to float: 'Nissan'\",\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'The error message “could not convert string to '\n",
      "                         'float: ‘Nissan’” typically occurs when a machine '\n",
      "                         'learning model or function is expecting numerical '\n",
      "                         'input, but receives a string instead. In this case, '\n",
      "                         'it seems like the model is trying to convert the car '\n",
      "                         'brand ‘Nissan’ into a numerical value, which isn’t '\n",
      "                         'possible.\\n'\n",
      "                         'To resolve this issue, you can encode categorical '\n",
      "                         'variables like car brands into numerical values. One '\n",
      "                         'common method is one-hot encoding, which creates new '\n",
      "                         'binary columns for each category/label present in '\n",
      "                         'the original column.\\n'\n",
      "                         'Here’s an example of how you can perform one-hot '\n",
      "                         'encoding using pandas:\\n'\n",
      "                         'import pandas as pd\\n'\n",
      "                         \"# Assuming 'data' is your DataFrame and 'brand' is \"\n",
      "                         'the column with car brands\\n'\n",
      "                         'data_encoded = pd.get_dummies(data, '\n",
      "                         \"columns=['brand'])\\n\"\n",
      "                         'In this code, pd.get_dummies() creates a new '\n",
      "                         'DataFrame where the ‘brand’ column is replaced with '\n",
      "                         'binary columns for each brand (e.g., ‘brand_Nissan’, '\n",
      "                         '‘brand_Toyota’, etc.). Each row in the DataFrame has '\n",
      "                         'a 1 in the column that corresponds to its brand and '\n",
      "                         '0 in all other brand columns.\\n'\n",
      "                         '-Mohammad Emad Sharifi-'},\n",
      "                {'question': 'Why did we change the targets to binary format '\n",
      "                             'when calculating mutual information score in the '\n",
      "                             'homework?',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Solution: Mutual Information score calculates the '\n",
      "                         'relationship between categorical variables or '\n",
      "                         'discrete variables. So in the homework, because the '\n",
      "                         'target which is median_house_value is continuous, we '\n",
      "                         'had to change it to binary format which in other '\n",
      "                         'words, makes its values discrete as either 0 or 1. '\n",
      "                         'If we allowed it to remain in the continuous '\n",
      "                         'variable format, the mutual information score could '\n",
      "                         'be calculated, but the algorithm would have to '\n",
      "                         'divide the continuous variables into bins and that '\n",
      "                         'would be highly subjective. That is why continuous '\n",
      "                         'variables are not used for mutual information score '\n",
      "                         'calculation.\\n'\n",
      "                         '—Odimegwu David—-'},\n",
      "                {'question': 'What data should we use for correlation matrix',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Q2 asks about correlation matrix and converting '\n",
      "                         'median_house_value from numeric to binary. Just to '\n",
      "                         'make sure here we are only dealing with df_train not '\n",
      "                         'df_train_full, right? As the question explicitly '\n",
      "                         'mentions the train dataset.\\n'\n",
      "                         'Yes. I think it is only on df_train. The reason '\n",
      "                         'behind this is that df_train_full also contains the '\n",
      "                         \"validation dataset, so at this stage we don't want \"\n",
      "                         'to make conclusions based on the validation data, '\n",
      "                         'since we want to test how we did without using that '\n",
      "                         'portion of the data.\\n'\n",
      "                         'Pastor Soto'},\n",
      "                {'question': 'Coloring the background of the '\n",
      "                             'pandas.DataFrame.corr correlation matrix '\n",
      "                             'directly',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'The background of any dataframe can be colored (not '\n",
      "                         'only the correlation matrix) based on the numerical '\n",
      "                         'values the dataframe contains by using the method '\n",
      "                         'pandas.io.formats.style.Styler.background_graident.\\n'\n",
      "                         'Here an example on how to color the correlation '\n",
      "                         'matrix. A color map of choice can get passed, here '\n",
      "                         '‘viridis’ is used.\\n'\n",
      "                         '# ensure to have only numerical values in the '\n",
      "                         \"dataframe before calling 'corr'\\n\"\n",
      "                         'corr_mat = df_numerical_only.corr()\\n'\n",
      "                         \"corr_mat.style.background_gradient(cmap='viridis')\\n\"\n",
      "                         'Here is an example of how the coloring will look '\n",
      "                         'like using a dataframe containing random values and '\n",
      "                         'applying “background_gradient” to it.\\n'\n",
      "                         'np.random.seed = 3\\n'\n",
      "                         'df_random = '\n",
      "                         'pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\\n'\n",
      "                         \"df_random.style.background_gradient(cmap='viridis')\\n\"\n",
      "                         'Added by Sylvia Schmitt'},\n",
      "                {'question': 'Identifying highly correlated feature pairs '\n",
      "                             'easily through unstack',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'data_corr = '\n",
      "                         'pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\\n'\n",
      "                         'data_corr.head(10)\\n'\n",
      "                         'Added by Harish Balasundaram\\n'\n",
      "                         'You can also use seaborn to create a heatmap with '\n",
      "                         'the correlation. The code for doing that:\\n'\n",
      "                         'sns.heatmap(df[numerical_features].corr(),\\n'\n",
      "                         'annot=True,\\n'\n",
      "                         'square=True,\\n'\n",
      "                         'fmt=\".2g\",\\n'\n",
      "                         'cmap=\"crest\")\\n'\n",
      "                         'Added by Cecile Guillot\\n'\n",
      "                         'You can refine your heatmap and plot only a '\n",
      "                         'triangle, with a blue to red color gradient, that '\n",
      "                         'will show every correlation between your numerical '\n",
      "                         'variables without redundant information with this '\n",
      "                         'function:\\n'\n",
      "                         'Which outputs, in the case of churn dataset:\\n'\n",
      "                         '(Mélanie Fouesnard)'},\n",
      "                {'question': 'What data should be used for EDA?',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Should we perform EDA on the base of train or '\n",
      "                         'train+validation or train+validation+test dataset?\\n'\n",
      "                         \"It's indeed good practice to only rely on the train \"\n",
      "                         'dataset for EDA. Including validation might be okay. '\n",
      "                         \"But we aren't supposed to touch the test dataset, \"\n",
      "                         \"even just looking at it isn't a good idea. We indeed \"\n",
      "                         'pretend that this is the future unseen data\\n'\n",
      "                         'Alena Kniazeva'},\n",
      "                {'question': 'Fitting DictVectorizer on validation',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Validation dataset helps to validate models and '\n",
      "                         'prediction on unseen data. This helps get an '\n",
      "                         'estimate on its performance on fresh data. It helps '\n",
      "                         'optimize the model.\\n'\n",
      "                         'Edidiong Esu\\n'\n",
      "                         \"Below is an extract of Alexey's book explaining this \"\n",
      "                         'point. Hope is useful\\n'\n",
      "                         'When we apply the fit method, this method is looking '\n",
      "                         'at the content of the df_train dictionaries we are '\n",
      "                         'passing to the DictVectorizer instance, and fit is '\n",
      "                         'figuring out (training) how to map the values of '\n",
      "                         'these dictionaries. If categorical, applies one-hot '\n",
      "                         'encoding, if numerical it will leave it as it is.\\n'\n",
      "                         'With this context, if we apply the fit to the '\n",
      "                         'validation model, we are \"giving the answers\" and we '\n",
      "                         'are not letting the \"fit\" do its job for data that '\n",
      "                         \"we haven't seen. By not applying the fit to the \"\n",
      "                         'validation model we can know how well it was '\n",
      "                         'trained.\\n'\n",
      "                         \"Below is an extract of Alexey's book explaining this \"\n",
      "                         'point.\\n'\n",
      "                         'Humberto Rodriguez\\n'\n",
      "                         'There is no need to initialize another instance of '\n",
      "                         'dictvectorizer after fitting it on the train set as '\n",
      "                         'it will overwrite what it learnt from being fit on '\n",
      "                         'the train data.\\n'\n",
      "                         'The correct way is to fit_transform the train set, '\n",
      "                         'and only transform the validation and test sets.\\n'\n",
      "                         'Memoona Tahira'},\n",
      "                {'question': 'Feature elimination',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'For Q5 in homework, should we calculate the smallest '\n",
      "                         'difference in accuracy in real values (i.e. -0.001 '\n",
      "                         'is less than -0.0002) or in absolute values (i.e. '\n",
      "                         '0.0002 is less than 0.001)?\\n'\n",
      "                         'We should select the “smallest” difference, and not '\n",
      "                         'the “lowest”, meaning we should reason in absolute '\n",
      "                         'values.\\n'\n",
      "                         'If the difference is negative, it means that the '\n",
      "                         'model actually became better when we removed the '\n",
      "                         'feature.'},\n",
      "                {'question': 'FutureWarning: Function get_feature_names is '\n",
      "                             'deprecated; get_feature_names is deprecated in '\n",
      "                             '1.0 and will be removed in 1.2',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Instead use the method “.get_feature_names_out()” '\n",
      "                         'from DictVectorizer function and the warning will be '\n",
      "                         'resolved , but we need not worry about the waning as '\n",
      "                         \"there won't be any warning\\n\"\n",
      "                         'Santhosh Kumar'},\n",
      "                {'question': 'Logistic regression crashing Jupyter kernel',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Fitting the logistic regression takes a long time / '\n",
      "                         'kernel crashes when calling predict() with the '\n",
      "                         'fitted model.\\n'\n",
      "                         'Make sure that the target variable for the logistic '\n",
      "                         'regression is binary.\\n'\n",
      "                         'Konrad Muehlberg'},\n",
      "                {'question': 'Understanding Ridge',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Ridge regression is a linear regression technique '\n",
      "                         'used to mitigate the problem of multicollinearity '\n",
      "                         '(when independent variables are highly correlated) '\n",
      "                         'and prevent overfitting in predictive modeling. It '\n",
      "                         'adds a regularization term to the linear regression '\n",
      "                         'cost function, penalizing large coefficients.\\n'\n",
      "                         'sag Solver: The sag solver stands for \"Stochastic '\n",
      "                         'Average Gradient.\" It\\'s particularly suitable for '\n",
      "                         'large datasets, as it optimizes the regularization '\n",
      "                         'term using stochastic gradient descent (SGD). sag '\n",
      "                         'can be faster than some other solvers for large '\n",
      "                         'datasets.\\n'\n",
      "                         'Alpha: The alpha parameter  controls the strength of '\n",
      "                         'the regularization in Ridge regression. A higher '\n",
      "                         'alpha value leads to stronger regularization, which '\n",
      "                         'means the model will have smaller coefficient '\n",
      "                         'values, reducing the risk of overfitting.\\n'\n",
      "                         'from sklearn.linear_model import Ridge\\n'\n",
      "                         \"ridge = Ridge(alpha=alpha, solver='sag', \"\n",
      "                         'random_state=42)\\n'\n",
      "                         'ridge.fit(X_train, y_train)\\n'\n",
      "                         'Aminat Abolade'},\n",
      "                {'question': 'pandas.get_dummies() and '\n",
      "                             'DictVectorizer(sparse=False) produce the same '\n",
      "                             'type of one-hot encodings:',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'DictVectorizer(sparse=True) produces CSR format, '\n",
      "                         'which is both more memory efficient and converges '\n",
      "                         'better during fit(). Basically it stores non-zero '\n",
      "                         'values and indices instead of adding a column for '\n",
      "                         'each class of each feature (models of cars produced '\n",
      "                         '900+ columns alone in the current task).\\n'\n",
      "                         'Using “sparse” format like on the picture above, '\n",
      "                         'both via pandas.get_dummies() and '\n",
      "                         'DictVectorizer(sparse=False) - is slower (around '\n",
      "                         '6-8min for Q6 task - Linear/Ridge Regression) for '\n",
      "                         'high amount of classes (like models of cars for eg) '\n",
      "                         'and gives a bit “worse” results in both Logistic and '\n",
      "                         'Linear/Ridge Regression, while also producing '\n",
      "                         'convergence warnings for Linear/Ridge Regression.\\n'\n",
      "                         'Larkin Andrii'},\n",
      "                {'question': 'Convergence Problems in W3Q6',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Ridge with sag solver requires feature to be of the '\n",
      "                         'same scale. You may get the following warning: '\n",
      "                         'ConvergenceWarning: The max_iter was reached which '\n",
      "                         'means the coef_ did not converge\\n'\n",
      "                         'Play with different scalers. See '\n",
      "                         'notebook-scaling-ohe.ipynb\\n'\n",
      "                         'Dmytro Durach\\n'\n",
      "                         '(Oscar Garcia)  Use a StandardScaler for the numeric '\n",
      "                         'fields and OneHotEncoder (sparce = False) for the '\n",
      "                         'categorical features.  This help with the warning. '\n",
      "                         'Separate the features (num/cat) without using the '\n",
      "                         'encoder first and see if that helps.'},\n",
      "                {'question': 'Dealing with Convergence in Week 3 q6',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'When encountering convergence errors during the '\n",
      "                         'training of a Ridge regression model, consider the '\n",
      "                         'following steps:\\n'\n",
      "                         'Feature Normalization: Normalize your numerical '\n",
      "                         'features using techniques like MinMaxScaler or '\n",
      "                         'StandardScaler. This ensures that numerical features '\n",
      "                         'are on a \\tsimilar scale, preventing convergence '\n",
      "                         'issues.\\n'\n",
      "                         'Categorical Feature Encoding: If your dataset '\n",
      "                         'includes categorical features, apply \\tcategorical '\n",
      "                         'encoding techniques such as OneHotEncoder (OHE) to \\t'\n",
      "                         'convert them into a numerical format. OHE is '\n",
      "                         'commonly used to represent categorical variables as '\n",
      "                         'binary vectors, making them compatible with '\n",
      "                         'regression models like Ridge.\\n'\n",
      "                         'Combine Features: After \\tnormalizing numerical '\n",
      "                         'features and encoding categorical features using '\n",
      "                         'OneHotEncoder, combine them to form a single feature '\n",
      "                         'matrix (X_train). This combined dataset serves as '\n",
      "                         'the input for training the Ridge regression model.\\n'\n",
      "                         'By following these steps, you can address '\n",
      "                         'convergence errors and enhance the stability of your '\n",
      "                         \"Ridge model training process. It's important to note \"\n",
      "                         'that the choice of encoding method, such as '\n",
      "                         'OneHotEncoder, is appropriate for handling '\n",
      "                         'categorical features in this context.\\n'\n",
      "                         'You can find an example here.\\n'\n",
      "                         ' \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tOsman Ali'},\n",
      "                {'question': 'Sparse matrix compared dense matrix',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'A sparse matrix is more memory-efficient because it '\n",
      "                         'only stores the non-zero values and their positions '\n",
      "                         'in memory. This is particularly useful when working '\n",
      "                         'with large datasets with many zero or missing '\n",
      "                         'values.\\n'\n",
      "                         'The default DictVectorizer configuration is a sparse '\n",
      "                         'matrix. For week3 Q6 using the default sparse is an '\n",
      "                         'interesting option because of the size of the '\n",
      "                         'matrix. Training the model was also more performant '\n",
      "                         'and didn’t give an error message like dense mode.\\n'\n",
      "                         ' \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQuinn Avila'},\n",
      "                {'question': 'How  to Disable/avoid Warnings in Jupyter '\n",
      "                             'Notebooks',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'The warnings on the jupyter notebooks can be '\n",
      "                         'disabled/ avoided with the following comments:\\n'\n",
      "                         'Import warnings\\n'\n",
      "                         'warnings.filterwarnings(“ignore”)\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'How to select the alpha parameter in Q6',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Question: Regarding RMSE, how do we decide on the '\n",
      "                         'correct score to choose? In the study group '\n",
      "                         'discussion    about week two homework, all of us got '\n",
      "                         'it wrong and one person had the lowest score '\n",
      "                         'selected as well.\\n'\n",
      "                         'Answer: You need to find RMSE for each alpha. If '\n",
      "                         'RMSE scores  are equal, you will select the lowest '\n",
      "                         'alpha.\\n'\n",
      "                         'Asia Saeed'},\n",
      "                {'question': 'Second variable that we need to use to calculate '\n",
      "                             'the mutual information score',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Question: Could you please help me with HW3 Q3: '\n",
      "                         '\"Calculate the mutual information score with the '\n",
      "                         '(binarized) price for the categorical variable that '\n",
      "                         'we have. Use the training set only.\" What is the '\n",
      "                         'second variable that we need to use to calculate the '\n",
      "                         'mutual information score?\\n'\n",
      "                         'Answer: You need to calculate the mutual info score '\n",
      "                         'between the binarized price (above_average) variable '\n",
      "                         '& ocean_proximity, the only original categorical '\n",
      "                         'variable in the dataset.\\n'\n",
      "                         'Asia Saeed'},\n",
      "                {'question': 'Features for homework Q5',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Do we need to train the model only with the '\n",
      "                         'features: total_rooms, total_bedrooms, population '\n",
      "                         'and households? or with all the available features '\n",
      "                         'and then pop once at a time each of the previous '\n",
      "                         'features and train the model to make the accuracy '\n",
      "                         'comparison?\\n'\n",
      "                         'You need to create a list of all features in this '\n",
      "                         'question and evaluate the model one time to obtain '\n",
      "                         'the accuracy, this will be the original accuracy, '\n",
      "                         'and then remove one feature each time, and in each '\n",
      "                         'time, train the model, find the accuracy and the '\n",
      "                         'difference between the original accuracy and the '\n",
      "                         'found accuracy. Finally, find out which feature has '\n",
      "                         'the smallest absolute accuracy difference.\\n'\n",
      "                         'While calculating differences between accuracy '\n",
      "                         'scores while training on the whole model, versus '\n",
      "                         'dropping one feature at a time and comparing its '\n",
      "                         'accuracy to the model to judge impact of the feature '\n",
      "                         'on the accuracy of the model, do we take the '\n",
      "                         'smallest difference or smallest absolute '\n",
      "                         'difference?\\n'\n",
      "                         'Since order of subtraction between the two accuracy '\n",
      "                         'scores can result in a negative number, we will take '\n",
      "                         'its absolute value as we are interested in the '\n",
      "                         'smallest value difference, not the lowest difference '\n",
      "                         'value. Case in point, if difference is -4 and -2, '\n",
      "                         'the smallest difference is abs(-2), and not abs(-4)'},\n",
      "                {'question': 'What is the difference between OneHotEncoder and '\n",
      "                             'DictVectorizer?',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Both work in similar ways. That is, to convert '\n",
      "                         'categorical features to numerical variables for use '\n",
      "                         'in training the model. But the difference lies in '\n",
      "                         'the input. OneHotEncoder uses an array as input '\n",
      "                         'while DictVectorizer uses a dictionary.\\n'\n",
      "                         'Both will produce the same result. But when we use '\n",
      "                         'OneHotEncoder, features are sorted alphabetically. '\n",
      "                         'When you use DictVectorizer you stack features that '\n",
      "                         'you want.\\n'\n",
      "                         'Tanya Mard'},\n",
      "                {'question': 'What is the difference between pandas '\n",
      "                             'get_dummies and sklearn OnehotEncoder?',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'They are basically the same. There are some key '\n",
      "                         'differences with regards to their input/output '\n",
      "                         'types, handling of missing values, etc, but they are '\n",
      "                         'both techniques to one-hot-encode categorical '\n",
      "                         'variables with identical results. The biggest '\n",
      "                         'difference is get_dummies are a convenient choice '\n",
      "                         'when you are working with Pandas Dataframes, while '\n",
      "                         'if you are building a scikit-learn-based machine '\n",
      "                         'learning pipeline and need to handle categorical '\n",
      "                         'data as part of that pipeline, OneHotEncoder is a '\n",
      "                         'more suitable choice. [Abhirup Ghosh]'},\n",
      "                {'question': 'Use of random seed in HW3',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': \"For the test_train_split question on week 3's \"\n",
      "                         'homework, are we supposed to use 42 as the '\n",
      "                         'random_state in both splits or only the 1st one?\\n'\n",
      "                         'Answer: for both splits random_state = 42 should be '\n",
      "                         'used\\n'\n",
      "                         '(Bhaskar Sarma)'},\n",
      "                {'question': 'Correlation before or after splitting the data',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Should correlation be calculated after splitting or '\n",
      "                         'before splitting. And lastly I know how to find the '\n",
      "                         'correlation but how do i find the two most '\n",
      "                         'correlated features.\\n'\n",
      "                         'Answer: Correlation matrix of your train dataset. '\n",
      "                         'Thus, after splitting. Two most correlated features '\n",
      "                         'are the ones having the highest correlation '\n",
      "                         'coefficient in terms of absolute values.'},\n",
      "                {'question': 'Features in Ridge Regression Model',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Make sure that the features used in ridge regression '\n",
      "                         'model are only NUMERICAL ones not categorical.\\n'\n",
      "                         'Drop all categorical features first before '\n",
      "                         'proceeding.\\n'\n",
      "                         '(Aileah Gotladera)\\n'\n",
      "                         'While it is True that ridge regression accepts only '\n",
      "                         'numerical values, the categorical ones can be useful '\n",
      "                         'for your model. You have to transform them using '\n",
      "                         'one-hot encoding before training the model. To avoid '\n",
      "                         'the error of non convergence, put sparse=True when '\n",
      "                         'doing so.\\n'\n",
      "                         '(Erjon)'},\n",
      "                {'question': 'Handling Column Information for Homework 3 '\n",
      "                             'Question 6',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'You need to use all features. and price for target. '\n",
      "                         \"Don't include the average variable we created \"\n",
      "                         'before.\\n'\n",
      "                         'If you use DictVectorizer then make sure to use '\n",
      "                         'sparce=True to avoid convergence errors\\n'\n",
      "                         'I also used StandardScalar for numerical variable '\n",
      "                         'you can try running with or without this\\n'\n",
      "                         '(Peter Pan)'},\n",
      "                {'question': 'Transforming Non-Numerical Columns into '\n",
      "                             'Numerical Columns',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'Use sklearn.preprocessing encoders and scalers, e.g. '\n",
      "                         'OneHotEncoder, OrdinalEncoder, and StandardScaler.'},\n",
      "                {'question': 'What is the better option FeatureHasher or '\n",
      "                             'DictVectorizer',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'These both methods receive the dictionary as an '\n",
      "                         'input. While the DictVectorizer will store the big '\n",
      "                         'vocabulary and takes more memory. FeatureHasher '\n",
      "                         'create a vectors with predefined length. They are '\n",
      "                         'both used for categorical features.\\n'\n",
      "                         'When you have a high cardinality for categorical '\n",
      "                         'features better to use FeatureHasher. If you want to '\n",
      "                         'preserve feature names in transformed data and have '\n",
      "                         'a small number of unique values is DictVectorizer. '\n",
      "                         'But your choice will dependence on your data.\\n'\n",
      "                         'You can read more by follow the link '\n",
      "                         'https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\\n'\n",
      "                         'Olga Rudakova'},\n",
      "                {'question': \"Isn't it easier to use DictVertorizer or get \"\n",
      "                             'dummies before splitting the data into '\n",
      "                             \"train/val/test? Is there a reason we wouldn't do \"\n",
      "                             'this? Or is it the same either way?',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': '(Question by Connie S.)\\n'\n",
      "                         \"The reason it's good/recommended practice to do it \"\n",
      "                         \"after splitting is to avoid data leakage - you don't \"\n",
      "                         'want any data from the test set influencing the '\n",
      "                         'training stage (similarly from the validation stage '\n",
      "                         'in the initial training). See e.g. scikit-learn '\n",
      "                         'documentation on \"Common pitfalls and recommended '\n",
      "                         'practices\": '\n",
      "                         'https://scikit-learn.org/stable/common_pitfalls.html\\n'\n",
      "                         'Answered/added by Rileen Sinha'},\n",
      "                {'question': 'HW3Q4 I am getting 1.0 as accuracy. Should I use '\n",
      "                             'the closest option?',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'If you are getting 1.0 as accuracy then there is a '\n",
      "                         'possibility you have overfitted the model. Dropping '\n",
      "                         'the column msrp/price can help you solve this '\n",
      "                         'issue.\\n'\n",
      "                         'Added by Akshar Goyal'},\n",
      "                {'question': 'How to calculate Root Mean Squared Error?',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'We can use sklearn & numpy packages to calculate '\n",
      "                         'Root Mean Squared Error\\n'\n",
      "                         'from sklearn.metrics import mean_squared_error\\n'\n",
      "                         'import numpy as np\\n'\n",
      "                         'Rmse = np.sqrt(mean_squared_error(y_pred, '\n",
      "                         'y_val/ytest)\\n'\n",
      "                         'Added by Radikal Lukafiardi\\n'\n",
      "                         'You can also refer to Alexey’s notebook for Week 2:\\n'\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\\n'\n",
      "                         'which includes the following code:\\n'\n",
      "                         'def rmse(y, y_pred):\\n'\n",
      "                         'error = y_pred - y\\n'\n",
      "                         'mse = (error ** 2).mean()\\n'\n",
      "                         'return np.sqrt(mse)\\n'\n",
      "                         '(added by Rileen Sinha)'},\n",
      "                {'question': \"AttributeError: 'DictVectorizer' object has no \"\n",
      "                             \"attribute 'get_feature_names'\",\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'The solution is to use “get_feature_names_out” '\n",
      "                         'instead. See details: '\n",
      "                         'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\\n'\n",
      "                         'George Chizhmak'},\n",
      "                {'question': 'Root Mean Squared Error',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'To use RMSE without math or numpy, ‘sklearn.metrics’ '\n",
      "                         'has a mean_squared_error function with a squared '\n",
      "                         'kwarg (defaults to True). Setting squared to False '\n",
      "                         'will return the RMSE.\\n'\n",
      "                         'from sklearn.metrics import mean_squared_error\\n'\n",
      "                         'rms = mean_squared_error(y_actual, y_predicted, '\n",
      "                         'squared=False)\\n'\n",
      "                         'See details: '\n",
      "                         'https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\\n'\n",
      "                         'Ahmed Okka'},\n",
      "                {'question': 'Encoding Techniques',\n",
      "                 'section': '3. Machine Learning for Classification',\n",
      "                 'text': 'This article explains different encoding techniques '\n",
      "                         'used '\n",
      "                         'https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\\n'\n",
      "                         'Hrithik Kumar Advani'},\n",
      "                {'question': 'Error in use of accuracy_score from sklearn in '\n",
      "                             'jupyter (sometimes)',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'I got this error multiple times here is the code:\\n'\n",
      "                         '“accuracy_score(y_val, y_pred >= 0.5)”\\n'\n",
      "                         \"TypeError: 'numpy.float64' object is not callable\\n\"\n",
      "                         'I solve it using\\n'\n",
      "                         'from sklearn import metrics\\n'\n",
      "                         'metrics.accuracy_score(y_train, y_pred>= 0.5)\\n'\n",
      "                         'OMAR Wael'},\n",
      "                {'question': 'How do I get started with Week 4?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Week 4 HW: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\\n'\n",
      "                         'All HWs: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\n'\n",
      "                         'Evaluation Matrix: '\n",
      "                         'https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\n'\n",
      "                         'GitHub for theory: '\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\n'\n",
      "                         'YouTube Link: 4.X --- '\n",
      "                         'https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\\n'\n",
      "                         'Sci-Kit Learn on Evaluation:\\n'\n",
      "                         'https://scikit-learn.org/stable/model_selection.html\\n'\n",
      "                         '~~Nukta Bhatia~~'},\n",
      "                {'question': 'Using a variable to score',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\\n'\n",
      "                         'Metrics can be used on a series or a dataframe\\n'\n",
      "                         '~~Ella Sahnan~~'},\n",
      "                {'question': 'Why do we sometimes use random_state and not at '\n",
      "                             'other times?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Ie particularly in module-04 homework Qn2 vs Qn5. '\n",
      "                         'https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\\n'\n",
      "                         'Refer to the sklearn docs, random_state is to ensure '\n",
      "                         'the “randomness” that is used to shuffle dataset is '\n",
      "                         'reproducible, and it usually requires both '\n",
      "                         'random_state and shuffle params to be set '\n",
      "                         'accordingly.\\n'\n",
      "                         '~~Ella Sahnan~~'},\n",
      "                {'question': 'How to get all classification metrics?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'How to get classification metrics - precision, '\n",
      "                         'recall, f1 score, accuracy simultaneously\\n'\n",
      "                         'Use classification_report from sklearn. For more '\n",
      "                         'info check here.\\n'\n",
      "                         'Abhishek N'},\n",
      "                {'question': 'Multiple thresholds for Q4',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'I am getting multiple thresholds with the same F1 '\n",
      "                         'score, does this indicate I am doing something wrong '\n",
      "                         'or is there a method for choosing? I would assume '\n",
      "                         'just pick the lowest?\\n'\n",
      "                         'Choose the one closest to any of the options\\n'\n",
      "                         'Added by Azeez Enitan Edunwale\\n'\n",
      "                         'You can always use scikit-learn (or other standard '\n",
      "                         'libraries/packages) to verify results obtained using '\n",
      "                         'your own code, e.g. you can use  '\n",
      "                         '“classification_report” '\n",
      "                         '(https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) '\n",
      "                         'to obtain precision, recall and F1-score.\\n'\n",
      "                         'Added by Rileen Sinha'},\n",
      "                {'question': 'ValueError: This solver needs samples of at '\n",
      "                             'least 2 classes in the data, but the data '\n",
      "                             'contains only one class: 0',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Solution description: duplicating the\\n'\n",
      "                         \"df.churn = (df.churn == 'yes').astype(int)\\n\"\n",
      "                         \"This is causing you to have only 0's in your churn \"\n",
      "                         'column. In fact, match with the error you are '\n",
      "                         'getting:  ValueError: This solver needs samples of '\n",
      "                         'at least 2 classes in the data, but the data '\n",
      "                         'contains only one class: 0.\\n'\n",
      "                         \"It is telling us that it only contains 0's.\\n\"\n",
      "                         'Delete one of the below cells and you will get the '\n",
      "                         'accuracy\\n'\n",
      "                         'Humberto Rodriguez'},\n",
      "                {'question': 'Method to get beautiful classification report',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Use Yellowbrick. Yellowbrick in a library that '\n",
      "                         'combines scikit-learn with matplotlib to produce '\n",
      "                         'visualizations for your models. It produces colorful '\n",
      "                         'classification reports.\\n'\n",
      "                         'Krishna Annad'},\n",
      "                {'question': 'I’m not getting the exact result in homework',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'That’s fine, use the closest option'},\n",
      "                {'question': 'Use AUC to evaluate feature importance of '\n",
      "                             'numerical variables',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Check the solutions from the 2021 iteration of the '\n",
      "                         'course. You should use roc_auc_score.'},\n",
      "                {'question': 'Help with understanding: “For each numerical '\n",
      "                             'value, use it as score and compute AUC”',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'When calculating the ROC AUC score using '\n",
      "                         'sklearn.metrics.roc_auc_score the function expects '\n",
      "                         'two parameters “y_true” and “y_score”. So for each '\n",
      "                         'numerical value in the dataframe it will be passed '\n",
      "                         'as the “y_score” to the function and the target '\n",
      "                         'variable will get passed a “y_true” each time.\\n'\n",
      "                         'Sylvia Schmitt'},\n",
      "                {'question': 'What dataset should I use to compute the metrics '\n",
      "                             'in Question 3',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'You must use the `dt_val` dataset to compute the '\n",
      "                         'metrics asked in Question 3 and onwards, as you did '\n",
      "                         'in Question 2.\\n'\n",
      "                         'Diego Giraldo'},\n",
      "                {'question': 'What does KFold do?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'What does this line do?\\n'\n",
      "                         'KFold(n_splits=n_splits, shuffle=True, '\n",
      "                         'random_state=1)\\n'\n",
      "                         'If I do it inside the loop [0.01, 0.1, 1, 10] or '\n",
      "                         \"outside the loop in Q6, HW04 it doesn't make any \"\n",
      "                         'difference to my answers. I am wondering why and '\n",
      "                         \"what is the right way, although it doesn't make a \"\n",
      "                         'difference!\\n'\n",
      "                         'Did you try using a different random_state? From my '\n",
      "                         'understanding, KFold just makes N (which is equal to '\n",
      "                         'n_splits) separate pairs of datasets (train+val).\\n'\n",
      "                         'https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\\n'\n",
      "                         'In my case changing random state changed results\\n'\n",
      "                         '(Arthur Minakhmetov)\\n'\n",
      "                         'Changing the random state makes a difference in my '\n",
      "                         'case too, but not whether it is inside or outside '\n",
      "                         'the for loop. I think I have got the answer. kFold = '\n",
      "                         'KFold(n_splits=n_splits, shuffle = True, '\n",
      "                         'random_state = 1)  is just a generator object and it '\n",
      "                         'contains only the information n_splits, shuffle and '\n",
      "                         'random_state. The k-fold splitting actually happens '\n",
      "                         'in the next for loop for train_idx, val_idx in '\n",
      "                         \"kFold.split(df_full_train): . So it doesn't matter \"\n",
      "                         'where we generate the object, before or after the '\n",
      "                         'first loop. It will generate the same information. '\n",
      "                         'But from the programming point of view, it is better '\n",
      "                         'to do it before the loop. No point doing it again '\n",
      "                         'and again inside the loop\\n'\n",
      "                         '(Bhaskar Sarma)\\n'\n",
      "                         'In case of KFold(n_splits=n_splits, shuffle=True, '\n",
      "                         'random_state=1) and C= [0.01, 0.1, 1, 10], it is '\n",
      "                         'better to loop through the different values of Cs as '\n",
      "                         'the video explained. I had separate train() and '\n",
      "                         'predict() functions, which were reused after '\n",
      "                         'dividing the dataset via KFold. The model ran about '\n",
      "                         '10 minutes and provided a good score.\\n'\n",
      "                         '(Ani Mkrtumyan)'},\n",
      "                {'question': \"ValueError: multi_class must be in ('ovo', \"\n",
      "                             \"'ovr')\",\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'I’m getting “ValueError: multi_class must be in '\n",
      "                         \"('ovo', 'ovr')” when using roc_auc_score to evaluate \"\n",
      "                         'feature importance of numerical variables in '\n",
      "                         'question 1.\\n'\n",
      "                         'I was getting this error because I was passing the '\n",
      "                         'parameters to roc_auc_score incorrectly '\n",
      "                         '(df_train[col] , y_train) . The correct way is to '\n",
      "                         'pass the parameters in this way: '\n",
      "                         'roc_auc_score(y_train, df_train[col])\\n'\n",
      "                         'Asia Saeed'},\n",
      "                {'question': 'Monitoring Wait times and progress of the code '\n",
      "                             'execution can be done with:',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'from tqdm.auto import tqdm\\n'\n",
      "                         'Tqdm - terminal progress bar\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'What is the use of inverting or negating the '\n",
      "                             'variables less than the threshold?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Inverting or negating variables with ROC AUC scores '\n",
      "                         'less than the threshold is a valuable technique to '\n",
      "                         'improve feature importance and model performance '\n",
      "                         'when dealing with negatively correlated features. It '\n",
      "                         'helps ensure that the direction of the correlation '\n",
      "                         'aligns with the expectations of most machine '\n",
      "                         'learning algorithms.\\n'\n",
      "                         'Aileah Gotladera'},\n",
      "                {'question': 'Difference between predict(X) and '\n",
      "                             'predict_proba(X)[:, 1]',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'In case of using predict(X) for this task we are '\n",
      "                         'getting the binary classification predictions which '\n",
      "                         'are 0 and 1. This may lead to incorrect evaluation '\n",
      "                         'values.\\n'\n",
      "                         'The solution is to use predict_proba(X)[:,1], where '\n",
      "                         'we get the probability that the value belongs to one '\n",
      "                         'of the classes.\\n'\n",
      "                         'Vladimir Yesipov\\n'\n",
      "                         'Predict_proba shows probailites per class.\\n'\n",
      "                         'Ani Mkrtumyan'},\n",
      "                {'question': 'Why are FPR and TPR equal to 0.0, when threshold '\n",
      "                             '= 1.0?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'For churn/not churn predictions, I need help to '\n",
      "                         'interpret the following scenario please, what is '\n",
      "                         'happening when:\\n'\n",
      "                         'The threshold is 1.0\\n'\n",
      "                         'FPR is 0.0\\n'\n",
      "                         'And TPR is 0.0\\n'\n",
      "                         'When the threshold is 1.0, the condition for '\n",
      "                         'belonging to the positive class (churn class) is '\n",
      "                         'g(x)>=1.0 But g(x) is a sigmoid function for a '\n",
      "                         'binary classification problem. It has values between '\n",
      "                         '0 and 1. This function  never becomes equal to '\n",
      "                         'outermost values, i.e. 0 and 1.\\n'\n",
      "                         'That is why there is no object, for which '\n",
      "                         'churn-condition could be satisfied. And that is why '\n",
      "                         'there is no any positive  (churn) predicted value '\n",
      "                         '(neither true positive, nor false positive), if '\n",
      "                         'threshold is equal to 1.0\\n'\n",
      "                         'Alena Kniazeva'},\n",
      "                {'question': 'How can I annotate a graph?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Matplotlib has a cool method to annotate where you '\n",
      "                         'could provide an X,Y point and annotate with an '\n",
      "                         'arrow and text. For example this will show an arrow '\n",
      "                         'pointing to the x,y point optimal threshold.\\n'\n",
      "                         \"plt.annotate(f'Optimal Threshold: \"\n",
      "                         '{optimal_threshold:.2f}\\\\nOptimal F1 Score: '\n",
      "                         \"{optimal_f1_score:.2f}',\\n\"\n",
      "                         'xy=(optimal_threshold, optimal_f1_score),\\n'\n",
      "                         'xytext=(0.3, 0.5),\\n'\n",
      "                         \"textcoords='axes fraction',\\n\"\n",
      "                         \"arrowprops=dict(facecolor='black', shrink=0.05))\\n\"\n",
      "                         'Quinn Avila'},\n",
      "                {'question': 'I didn’t fully understand the ROC curve. Can I '\n",
      "                             'move on?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': \"It's a complex and abstract topic and it requires \"\n",
      "                         'some time to understand. You can move on without '\n",
      "                         'fully understanding the concept.\\n'\n",
      "                         'Nonetheless, it might be useful for you to rewatch '\n",
      "                         'the video, or even watch videos/lectures/notes by '\n",
      "                         'other people on this topic, as the ROC AUC is one of '\n",
      "                         'the most important metrics used in Binary '\n",
      "                         'Classification models.'},\n",
      "                {'question': 'Why do I have different values of accuracy than '\n",
      "                             'the options in the homework?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'One main reason behind that, is the way of splitting '\n",
      "                         'data. For example, we want to split data into '\n",
      "                         'train/validation/test with the ratios 60%/20%/20% '\n",
      "                         'respectively.\\n'\n",
      "                         'Although the following two options end up with the '\n",
      "                         'same ratio, the data itself is a bit different and '\n",
      "                         'not 100% matching in each case.\\n'\n",
      "                         '1)\\n'\n",
      "                         'df_train, df_temp = train_test_split(df, '\n",
      "                         'test_size=0.4, random_state=42)\\n'\n",
      "                         'df_val, df_test = train_test_split(df_temp, '\n",
      "                         'test_size=0.5, random_state=42)\\n'\n",
      "                         '2)\\n'\n",
      "                         'df_full_train, df_test = train_test_split(df, '\n",
      "                         'test_size=0.2, random_state=42)\\n'\n",
      "                         'df_train, df_val = train_test_split(df_full_train, '\n",
      "                         'test_size=0.25, random_state=42)\\n'\n",
      "                         'Therefore, I would recommend using the second method '\n",
      "                         'which is more consistent with the lessons and thus '\n",
      "                         'the homeworks.\\n'\n",
      "                         'Ibraheem Taha'},\n",
      "                {'question': 'How to find the intercept between precision and '\n",
      "                             'recall curves by using numpy?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'You can find the intercept between these two curves '\n",
      "                         'using numpy diff '\n",
      "                         '(https://numpy.org/doc/stable/reference/generated/numpy.diff.html '\n",
      "                         ') and sign '\n",
      "                         '(https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\\n'\n",
      "                         'I suppose here that you have your df_scores ready '\n",
      "                         'with your three columns ‘threshold’, ‘precision’ and '\n",
      "                         '‘recall’:\\n'\n",
      "                         'You want to know at which index (or indices) you '\n",
      "                         'have your intercept between precision and recall '\n",
      "                         '(namely: where the sign of the difference between '\n",
      "                         'precision and recall changes):\\n'\n",
      "                         'idx = np.argwhere(\\n'\n",
      "                         'np.diff(\\n'\n",
      "                         'np.sign(np.array(df_scores[\"precision\"]) - '\n",
      "                         'np.array(df_scores[\"recall\"]))\\n'\n",
      "                         ')\\n'\n",
      "                         ').flatten()\\n'\n",
      "                         'You can print the result to easily read it:\\n'\n",
      "                         'print(\\n'\n",
      "                         'f\"The precision and recall curves intersect at a '\n",
      "                         'threshold equal to '\n",
      "                         '{df_scores.loc[idx][\\'threshold\\']}.\"\\n'\n",
      "                         ')\\n'\n",
      "                         '(Mélanie Fouesnard)'},\n",
      "                {'question': 'Compute Recall, Precision, and F1 Score using '\n",
      "                             'scikit-learn library',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'In the demonstration video, we are shown how to '\n",
      "                         'calculate the precision and recall manually. You can '\n",
      "                         'use the Scikit Learn library to calculate the '\n",
      "                         'confusion matrix. precision, recall, f1_score '\n",
      "                         'without having to first define true positive, true '\n",
      "                         'negative, false positive, and false negative.\\n'\n",
      "                         'from sklearn.metrics import precision_score, '\n",
      "                         'recall_score, f1_score\\n'\n",
      "                         \"precision_score(y_true, y_pred, average='binary')\\n\"\n",
      "                         \"recall_score(y_true, y_pred, average='binary')\\n\"\n",
      "                         \"f1_score(y_true, y_pred, average='binary')\\n\"\n",
      "                         'Radikal Lukafiardi'},\n",
      "                {'question': 'Why do we use cross validation?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Cross-validation evaluates the performance of a '\n",
      "                         'model and chooses the best hyperparameters. '\n",
      "                         'Cross-validation does this by splitting the dataset '\n",
      "                         'into multiple parts (folds), typically 5 or 10. It '\n",
      "                         'then trains and evaluates your model multiple times, '\n",
      "                         'each time using a different fold as the validation '\n",
      "                         'set and the remaining folds as the training set.\\n'\n",
      "                         '\"C\" is a hyperparameter that is typically associated '\n",
      "                         'with regularization in models like Support Vector '\n",
      "                         'Machines (SVM) and logistic regression.\\n'\n",
      "                         'Smaller \"C\" values: They introduce more '\n",
      "                         'regularization, which means the model will try to '\n",
      "                         'find a simpler decision boundary, potentially '\n",
      "                         'underfitting the data. This is because it penalizes '\n",
      "                         'the misclassification of training examples more '\n",
      "                         'severely.\\n'\n",
      "                         'Larger \"C\" values: They reduce the regularization '\n",
      "                         'effect, allowing the model to fit the training data '\n",
      "                         'more closely, potentially overfitting. This is '\n",
      "                         'because it penalizes misclassification less '\n",
      "                         'severely, allowing the model to prioritize getting '\n",
      "                         'training examples correct.\\n'\n",
      "                         'Aminat Abolade'},\n",
      "                {'question': 'Evaluate the Model using scikit learn metrics',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Model evaluation metrics can be easily computed '\n",
      "                         'using off the shelf calculations available in scikit '\n",
      "                         'learn library. This saves a lot of time and more '\n",
      "                         'precise compared to our own calculations from the '\n",
      "                         'scratch using numpy and pandas libraries.\\n'\n",
      "                         'from sklearn.metrics import (accuracy_score,\\n'\n",
      "                         'precision_score,\\n'\n",
      "                         'recall_score,\\n'\n",
      "                         'f1_score,\\n'\n",
      "                         'roc_auc_score\\n'\n",
      "                         ')\\n'\n",
      "                         'accuracy = accuracy_score(y_val, y_pred)\\n'\n",
      "                         'precision = precision_score(y_val, y_pred)\\n'\n",
      "                         'recall = recall_score(y_val, y_pred)\\n'\n",
      "                         'f1 = f1_score(y_val, y_pred)\\n'\n",
      "                         'roc_auc = roc_auc_score(y_val, y_pred)\\n'\n",
      "                         \"print(f'Accuracy: {accuracy}')\\n\"\n",
      "                         \"print(f'Precision: {precision}')\\n\"\n",
      "                         \"print(f'Recall: {recall}')\\n\"\n",
      "                         \"print(f'F1-Score: {f1}')\\n\"\n",
      "                         \"print(f'ROC AUC: {roc_auc}')\\n\"\n",
      "                         '(Harish Balasundaram)'},\n",
      "                {'question': 'Are there other ways to compute Precision, '\n",
      "                             'Recall and F1 score?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Scikit-learn offers another way: '\n",
      "                         'precision_recall_fscore_support\\n'\n",
      "                         'Example:\\n'\n",
      "                         'from sklearn.metrics import '\n",
      "                         'precision_recall_fscore_support\\n'\n",
      "                         'precision, recall, fscore, support = '\n",
      "                         'precision_recall_fscore_support(y_val, y_val_pred, '\n",
      "                         'zero_division=0)\\n'\n",
      "                         '(Gopakumar Gopinathan)'},\n",
      "                {'question': 'When do I use ROC vs Precision-Recall curves?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': '- ROC curves are appropriate when the observations '\n",
      "                         'are balanced between each class, whereas '\n",
      "                         'precision-recall curves are appropriate for '\n",
      "                         'imbalanced datasets.\\n'\n",
      "                         '- The reason for this recommendation is that ROC '\n",
      "                         'curves present an optimistic picture of the model on '\n",
      "                         'datasets with a class imbalance.\\n'\n",
      "                         '-This is because of the use of true negatives in the '\n",
      "                         'False Positive Rate in the ROC Curve and the careful '\n",
      "                         'avoidance of this rate in the Precision-Recall '\n",
      "                         'curve.\\n'\n",
      "                         '- If the proportion of positive to negative '\n",
      "                         'instances changes in a test set, the ROC curves will '\n",
      "                         'not change. Metrics such as accuracy, precision, '\n",
      "                         'lift and F scores use values from both columns of '\n",
      "                         'the confusion matrix. As a class distribution '\n",
      "                         'changes these measures will change as well, even if '\n",
      "                         'the fundamental classifier performance does not. ROC '\n",
      "                         'graphs are based upon TP rate and FP rate, in which '\n",
      "                         'each dimension is a strict columnar ratio, so cannot '\n",
      "                         'give an accurate picture of performance when there '\n",
      "                         'is class imbalance.\\n'\n",
      "                         '(Anudeep Vanjavakam)'},\n",
      "                {'question': 'How to evaluate feature importance for numerical '\n",
      "                             'variables with AUC?',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'You can use roc_auc_score function from '\n",
      "                         'sklearn.metrics module and pass the vector of the '\n",
      "                         'target variable (‘above_average’) as the first '\n",
      "                         'argument and the vector of feature values as the '\n",
      "                         'second one. This function will return AUC score for '\n",
      "                         'the feature that was passed as a second argument.\\n'\n",
      "                         '(Denys Soloviov)'},\n",
      "                {'question': 'Dependence of the F-score on class imbalance',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'Precision-recall curve, and thus the score, '\n",
      "                         'explicitly depends on the ratio  of positive to '\n",
      "                         'negative test cases. This means that comparison of '\n",
      "                         'the F-score across different problems with differing '\n",
      "                         'class ratios is problematic. One way to address this '\n",
      "                         'issue is to use a standard class ratio  when making '\n",
      "                         'such comparisons.\\n'\n",
      "                         '(George Chizhmak)'},\n",
      "                {'question': 'Quick way to plot Precision-Recall Curve',\n",
      "                 'section': '4. Evaluation Metrics for Classification',\n",
      "                 'text': 'We can import precision_recall_curve from '\n",
      "                         'scikit-learn and plot the graph as follows:\\n'\n",
      "                         'from sklearn.metrics import precision_recall_curve\\n'\n",
      "                         'precision, recall, thresholds = '\n",
      "                         'precision_recall_curve(y_val, y_predict)\\n'\n",
      "                         'plt.plot(thresholds, precision[:-1], '\n",
      "                         \"label='Precision')\\n\"\n",
      "                         \"plt.plot(thresholds, recall[:-1], label='Recall')\\n\"\n",
      "                         'plt.legend()\\n'\n",
      "                         'Hrithik Kumar Advani'},\n",
      "                {'question': 'What is Stratified k-fold?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'For multiclass classification it is important to '\n",
      "                         'keep class balance when you split the data set. In '\n",
      "                         'this case Stratified k-fold returns folds that '\n",
      "                         'contains approximately the sme percentage of samples '\n",
      "                         'of each classes.\\n'\n",
      "                         'Please check the realisation in sk-learn library:\\n'\n",
      "                         'https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\\n'\n",
      "                         'Olga Rudakova'},\n",
      "                {'question': 'How do I get started with Week 5?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Week 5 HW: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\\n'\n",
      "                         'All HWs: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\n'\n",
      "                         'HW 3 Solution: '\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\\n'\n",
      "                         'Evaluation Matrix: '\n",
      "                         'https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\n'\n",
      "                         'GitHub for theory: '\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\n'\n",
      "                         'YouTube Link: 5.X --- '\n",
      "                         'https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\\n'\n",
      "                         '~~~ Nukta Bhatia ~~~'},\n",
      "                {'question': 'Errors related to the default environment: WSL, '\n",
      "                             'Ubuntu, proper Python version, installing pipenv '\n",
      "                             'etc.',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'While weeks 1-4 can relatively easily be followed '\n",
      "                         'and the associated homework completed with just '\n",
      "                         'about any default environment / local setup, week 5 '\n",
      "                         'introduces several layers of abstraction and '\n",
      "                         'dependencies.\\n'\n",
      "                         'It is advised to prepare your “homework environment” '\n",
      "                         'with a cloud provider of your choice. A thorough '\n",
      "                         'step-by-step guide for doing so for an AWS EC2 '\n",
      "                         'instance is provided in an introductory video taken '\n",
      "                         'from the MLOPS course here:\\n'\n",
      "                         'https://www.youtube.com/watch?v=IXSiYkP23zo\\n'\n",
      "                         'Note that (only) small AWS instances can be run for '\n",
      "                         'free, and that larger ones will be billed hourly '\n",
      "                         'based on usage (but can and should be stopped when '\n",
      "                         'not in use).\\n'\n",
      "                         'Alternative ways are sketched here:\\n'\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md'},\n",
      "                {'question': 'How to download CSV data via Jupyter NB and the '\n",
      "                             'Kaggle API, for one seamless experience',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'You’ll need a kaggle account\\n'\n",
      "                         'Go to settings, API and click `Create New Token`. '\n",
      "                         'This will download a `kaggle.json` file which '\n",
      "                         'contains your `username` and `key` information\\n'\n",
      "                         'In the same location as your Jupyter NB, place the '\n",
      "                         '`kaggle.json` file\\n'\n",
      "                         'Run `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\\n'\n",
      "                         'Make sure to import os via `import os` and then '\n",
      "                         'run:\\n'\n",
      "                         \"os.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR \"\n",
      "                         'FILE PATH>\\n'\n",
      "                         'Finally you can run directly in your NB: `!kaggle '\n",
      "                         'datasets download -d '\n",
      "                         'kapturovalexander/bank-credit-scoring`\\n'\n",
      "                         'And then you can unzip the file and access the CSV '\n",
      "                         'via: `!unzip -o bank-credit-scoring.zip`\\n'\n",
      "                         '>>> Michael Fronda <<<'},\n",
      "                {'question': 'Basic Ubuntu Commands:',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Cd .. (go back)\\n'\n",
      "                         'Ls (see current folders)\\n'\n",
      "                         'Cd ‘path’/ (go to this path)\\n'\n",
      "                         'Pwd (home)\\n'\n",
      "                         'Cat “file name’ --edit txt file in ubuntu\\n'\n",
      "                         'Aileah Gotladera'},\n",
      "                {'question': 'Installing and updating to the python version '\n",
      "                             '3.10 and higher',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Open terminal and type the code below to check the '\n",
      "                         'version on your laptop\\n'\n",
      "                         'python3 --version\\n'\n",
      "                         'For windows,\\n'\n",
      "                         'Visit the official python website at  '\n",
      "                         'https://www.python.org/downloads/ to download the '\n",
      "                         'python version you need for installation\\n'\n",
      "                         'Run the installer and  ensure to check the box that '\n",
      "                         'says “Add Python to PATH” during installation and '\n",
      "                         'complete the installation by following the prompts\\n'\n",
      "                         'Or\\n'\n",
      "                         'For Python 3,\\n'\n",
      "                         'Open your command prompt or terminal and run the '\n",
      "                         'following command:\\n'\n",
      "                         'pip install --upgrade python\\n'\n",
      "                         'Aminat Abolade'},\n",
      "                {'question': 'How to install WSL on Windows 10 and 11 ?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'It is quite simple, and you can follow these '\n",
      "                         'instructions here:\\n'\n",
      "                         'https://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\\n'\n",
      "                         'Make sure that you have “Virtual Machine Platform” '\n",
      "                         'feature activated in your Windows “Features”. To do '\n",
      "                         'that, search “features” in the research bar and see '\n",
      "                         'if the checkbox is selected. You also need to make '\n",
      "                         'sure that your system (in the bios) is able to '\n",
      "                         'virtualize. This is usually the case.\\n'\n",
      "                         'In the Microsoft Store: look for ‘Ubuntu’ or '\n",
      "                         '‘Debian’ (or any linux distribution you want) and '\n",
      "                         'install it\\n'\n",
      "                         'Once it is downloaded, open the app and choose a '\n",
      "                         'username and a password (secured one). When you type '\n",
      "                         'your password, nothing will show in the window, '\n",
      "                         'which is normal: the writing is invisible.\\n'\n",
      "                         'You are now inside of your linux system. You can '\n",
      "                         'test some commands such as “pwd”. You are not in '\n",
      "                         'your Windows system.\\n'\n",
      "                         'To go to your windows system: you need to go back '\n",
      "                         'two times with cd ../.. And then go to the “mnt” '\n",
      "                         'directory with cd mnt. If you list here your files, '\n",
      "                         'you will see your disks. You can move to the desired '\n",
      "                         'folder, for example here I moved to the ML_Zoomcamp '\n",
      "                         'folder:\\n'\n",
      "                         'Python should be already installed but you can check '\n",
      "                         'it by running sudo apt install python3 command.\\n'\n",
      "                         'You can make your actual folder your default folder '\n",
      "                         'when you open your Ubuntu terminal with this command '\n",
      "                         ': echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\\n'\n",
      "                         'You can disable bell sounds (when you type something '\n",
      "                         'that does not exist for example) by modifying the '\n",
      "                         'inputrc file with this command: sudo vim '\n",
      "                         '/etc/inputrc\\n'\n",
      "                         'You have to uncomment the set bell-style none line '\n",
      "                         '-> to do that, press the “i” keyboard letter (for '\n",
      "                         'insert) and go with your keyboard to this line. '\n",
      "                         'Delete the # and then press the Escape keyboard '\n",
      "                         'touch and finally press “:wq” to write (it saves '\n",
      "                         'your modifications) then quit.\\n'\n",
      "                         'You can check that your modifications are taken into '\n",
      "                         'account by opening a new terminal (you can pin it to '\n",
      "                         'your task bar so you do not have to go to the '\n",
      "                         'Microsoft app each time).\\n'\n",
      "                         'You will need to install pip by running this command '\n",
      "                         'sudo apt install python3-pip\\n'\n",
      "                         'NB: I had this error message when trying to install '\n",
      "                         'pipenv '\n",
      "                         '(https://github.com/microsoft/WSL/issues/5663):\\n'\n",
      "                         \"/sbin/ldconfig.real: Can't link \"\n",
      "                         '/usr/lib/wsl/lib/libnvoptix_loader.so.1 to '\n",
      "                         'libnvoptix.so.1\\n'\n",
      "                         '/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 '\n",
      "                         'is not a symbolic link\\n'\n",
      "                         'So I had to create the following symbolic link:\\n'\n",
      "                         'sudo ln -s /usr/lib/wsl/lib/libcuda.so.1 '\n",
      "                         '/usr/lib64/libcuda.so\\n'\n",
      "                         '(Mélanie Fouesnard)'},\n",
      "                {'question': 'Error building Docker images on Mac with M1 '\n",
      "                             'silicon',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Do you get errors building the Docker image on the '\n",
      "                         'Mac M1 chipset?\\n'\n",
      "                         'The error I was getting was:\\n'\n",
      "                         \"Could not open '/lib64/ld-linux-x86-64.so.2': No \"\n",
      "                         'such file or directory\\n'\n",
      "                         'The fix (from here): '\n",
      "                         'vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\\n'\n",
      "                         'Open '\n",
      "                         'mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\\n'\n",
      "                         'Replace line 1 with\\n'\n",
      "                         'FROM --platform=linux/amd64 ubuntu:latest\\n'\n",
      "                         'Now build the image as specified. In the end it took '\n",
      "                         'over 2 hours to build the image but it did complete '\n",
      "                         'in the end.\\n'\n",
      "                         'David Colton'},\n",
      "                {'question': 'Method to find the version of any install python '\n",
      "                             'libraries in jupyter notebook',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Import waitress\\n'\n",
      "                         'print(waitress.__version__)\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'Cannot connect to the docker daemon. Is the '\n",
      "                             'Docker daemon running?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Working on getting Docker installed - when I try '\n",
      "                         'running hello-world I am getting the error.\\n'\n",
      "                         'Docker: Cannot connect to the docker daemon at '\n",
      "                         'unix:///var/run/docker.sock. Is the Docker daemon '\n",
      "                         'running ?\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'If you’re getting this error on WSL, re-install your '\n",
      "                         'docker: remove the docker installation from WSL and '\n",
      "                         'install Docker Desktop on your host machine '\n",
      "                         '(Windows).\\n'\n",
      "                         'On Linux, start the docker daemon with either of '\n",
      "                         'these commands:\\n'\n",
      "                         'sudo dockerd\\n'\n",
      "                         'sudo service docker start\\n'\n",
      "                         'Added by Ugochukwu Onyebuchi'},\n",
      "                {'question': \"The command '/bin/sh -c pipenv install --deploy \"\n",
      "                             \"--system &&  rm -rf /root/.cache' returned a \"\n",
      "                             'non-zero code: 1',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'After using the command “docker build -t '\n",
      "                         'churn-prediction .” to build the Docker image, the '\n",
      "                         'above error is raised and the image is not created.\\n'\n",
      "                         'In your Dockerfile, change the Python version in the '\n",
      "                         'first line the Python version installed in your '\n",
      "                         'system:\\n'\n",
      "                         'FROM python:3.7.5-slim\\n'\n",
      "                         'To find your python version, use the command python '\n",
      "                         '--version. For example:\\n'\n",
      "                         'python --version\\n'\n",
      "                         '>> Python 3.9.7\\n'\n",
      "                         'Then, change it on your Dockerfile:\\n'\n",
      "                         'FROM python:3.9.7-slim\\n'\n",
      "                         'Added by Filipe Melo'},\n",
      "                {'question': 'Running “pipenv install sklearn==1.0.2” gives '\n",
      "                             'errors. What should I do?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'When the facilitator was adding sklearn to the '\n",
      "                         'virtual environment in the lectures, he used '\n",
      "                         'sklearn==0.24.1 and it ran smoothly. But while doing '\n",
      "                         'the homework and you are asked to use the 1.0.2 '\n",
      "                         'version of sklearn, it gives errors.\\n'\n",
      "                         'The solution is to use the full name of sklearn. '\n",
      "                         'That is, run it as “pipenv install '\n",
      "                         'scikit-learn==1.0.2” and the error will go away, '\n",
      "                         'allowing you to install sklearn for the version in '\n",
      "                         'your virtual environment.\\n'\n",
      "                         'Odimegwu David\\n'\n",
      "                         'Homework asks you to install 1.3.1\\n'\n",
      "                         'Pipenv install scikit-learn==1.3.1\\n'\n",
      "                         'Use Pipenv to install Scikit-Learn version 1.3.1\\n'\n",
      "                         'Gopakumar Gopinathan'},\n",
      "                {'question': 'Why do we need the --rm flag',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'What is the reason we don’t want to keep the docker '\n",
      "                         'image in our system and why do we need to run docker '\n",
      "                         'containers with `--rm` flag?\\n'\n",
      "                         'For best practice, you don’t want to have a lot of '\n",
      "                         'abandoned docker images in your system. You just '\n",
      "                         'update it in your folder and trigger the build one '\n",
      "                         'more time.\\n'\n",
      "                         'They consume extra space on your disk. Unless you '\n",
      "                         'don’t want to re-run the previously existing '\n",
      "                         'containers, it is better to use the `--rm` option.\\n'\n",
      "                         'The right way to say: “Why do we remove the docker '\n",
      "                         'container in our system?”. Well the docker image is '\n",
      "                         'still kept; it is the container that is not kept. '\n",
      "                         'Upon execution, images are not modified; only '\n",
      "                         'containers are.\\n'\n",
      "                         'The option `--rm` is for removing containers. The '\n",
      "                         'images remain until you remove them manually. If you '\n",
      "                         'don’t specify a version when building an image, it '\n",
      "                         'will always rebuild and replace the latest tag. '\n",
      "                         '`docker images` shows you all the image you have '\n",
      "                         'pulled or build so far.\\n'\n",
      "                         'During development and testing you usually specify '\n",
      "                         '`--rm` to get the containers auto removed upon exit. '\n",
      "                         'Otherwise they get accumulated in a stopped state, '\n",
      "                         'taking up space. `docker ps -a` shows you all the '\n",
      "                         'containers you have in your host. Each time you '\n",
      "                         'change Pipfile (or any file you baked into the '\n",
      "                         'container), you rebuild the image under the same tag '\n",
      "                         'or a new tag. It’s important to understand the '\n",
      "                         'difference between the term “docker image” and '\n",
      "                         '“docker container”. Image is what we build with all '\n",
      "                         'the resources baked in. You can move it around, '\n",
      "                         'maintain it in a repository, share it. Then we use '\n",
      "                         'the image to spin up instances of it and they are '\n",
      "                         'called containers.\\n'\n",
      "                         'Added by Muhammad Awon'},\n",
      "                {'question': 'Failed to read Dockerfile',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'When you create the dockerfile the name should be '\n",
      "                         'dockerfile and needs to be without extension. One of '\n",
      "                         'the problems we can get at this point is to create '\n",
      "                         'the dockerfile as a dockerfile extension '\n",
      "                         'Dockerfile.dockerfile which creates an error when we '\n",
      "                         'build the docker image. Instead we just need to '\n",
      "                         'create the file without extension: Dockerfile and '\n",
      "                         'will run perfectly.\\n'\n",
      "                         'Added by Pastor Soto'},\n",
      "                {'question': 'Install docker on MacOS',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Refer to the page '\n",
      "                         'https://docs.docker.com/desktop/install/mac-install/ '\n",
      "                         'remember to check if you have apple chip or intel '\n",
      "                         'chip.'},\n",
      "                {'question': 'I cannot pull the image with docker pull command',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Problem: When I am trying to pull the image with the '\n",
      "                         'docker pull svizor/zoomcamp-model command I am '\n",
      "                         'getting an error:\\n'\n",
      "                         'Using default tag: latest\\n'\n",
      "                         'Error response from daemon: manifest for '\n",
      "                         'svizor/zoomcamp-model:latest not found: manifest '\n",
      "                         'unknown: manifest unknown\\n'\n",
      "                         'Solution: The docker by default uses the latest tag '\n",
      "                         'to avoid this use the correct tag from image '\n",
      "                         'description. In our case use command:\\n'\n",
      "                         'docker pull svizor/zoomcamp-model:3.10.12-slim\\n'\n",
      "                         'Added by Vladimir Yesipov'},\n",
      "                {'question': 'Dumping/Retrieving only the size of for a '\n",
      "                             'specific Docker image',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Using the command docker images or docker image ls '\n",
      "                         'will dump all information for all local Docker '\n",
      "                         'images. It is possible to dump the information only '\n",
      "                         'for a specified image by using:\\n'\n",
      "                         'docker image ls <image name>\\n'\n",
      "                         'Or alternatively:\\n'\n",
      "                         'docker images <image name>\\n'\n",
      "                         'In action to that it is possible to only dump '\n",
      "                         'specific information provided using the option '\n",
      "                         '--format which will dump only the size for the '\n",
      "                         'specified image name when using the command below:\\n'\n",
      "                         'docker image ls --format \"{{.Size}}\" <image name>\\n'\n",
      "                         'Or alternatively:\\n'\n",
      "                         'docker images --format \"{{.Size}}\" <image name>\\n'\n",
      "                         'Sylvia Schmitt'},\n",
      "                {'question': 'Where does pipenv create environments and how '\n",
      "                             'does it name them?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'It creates them in\\n'\n",
      "                         'OSX/Linux: '\n",
      "                         '~/.local/share/virtualenvs/folder-name_cyrptic-hash\\n'\n",
      "                         'Windows: '\n",
      "                         'C:\\\\Users\\\\<USERNAME>\\\\.virtualenvs\\\\folder-name_cyrptic-hash\\n'\n",
      "                         'Eg: C:\\\\Users\\\\Ella\\\\.virtualenvs\\\\code-qsdUdabf '\n",
      "                         '(for module-05 lesson)\\n'\n",
      "                         'The environment name is the name of the last folder '\n",
      "                         'in the folder directory where we used the pipenv '\n",
      "                         'install command (or any other pipenv command). E.g. '\n",
      "                         'If you run any pipenv command in folder path '\n",
      "                         '~/home/user/Churn-Flask-app, it will create an '\n",
      "                         'environment named '\n",
      "                         \"Churn-Flask-app-some_random_characters, and it's \"\n",
      "                         'path will be like this: '\n",
      "                         '/home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\\n'\n",
      "                         'All libraries of this environment will be installed '\n",
      "                         'inside this folder. To activate this environment, I '\n",
      "                         'will need to cd into the project folder again, and '\n",
      "                         'type pipenv shell. In short, the location of the '\n",
      "                         'project folder acts as an identifier for an '\n",
      "                         'environment, in place of any name.\\n'\n",
      "                         '(Memoona Tahira)'},\n",
      "                {'question': 'How do I debug a docker container?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Launch the container image in interactive mode and '\n",
      "                         'overriding the entrypoint, so that it starts a bash '\n",
      "                         'command.\\n'\n",
      "                         'docker run -it --entrypoint bash <image>\\n'\n",
      "                         'If the container is already running, execute a '\n",
      "                         'command in the specific container:\\n'\n",
      "                         'docker ps (find the container-id)\\n'\n",
      "                         'docker exec -it <container-id> bash\\n'\n",
      "                         '(Marcos MJD)'},\n",
      "                {'question': 'The input device is not a TTY when running '\n",
      "                             'docker in interactive mode (Running Docker on '\n",
      "                             'Windows in GitBash)',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': '$ docker exec -it 1e5a1b663052 bash\\n'\n",
      "                         'the input device is not a TTY.  If you are using '\n",
      "                         \"mintty, try prefixing the command with 'winpty'\\n\"\n",
      "                         'Fix:\\n'\n",
      "                         'winpty docker exec -it 1e5a1b663052 bash\\n'\n",
      "                         'A TTY is a terminal interface that supports escape '\n",
      "                         'sequences, moving the cursor around, etc.\\n'\n",
      "                         'Winpty is a Windows software package providing an '\n",
      "                         'interface similar to a Unix pty-master for '\n",
      "                         'communicating with Windows console programs.\\n'\n",
      "                         'More info on terminal, shell, console applications '\n",
      "                         'hi and so on:\\n'\n",
      "                         'https://conemu.github.io/en/TerminalVsShell.html\\n'\n",
      "                         '(Marcos MJD)'},\n",
      "                {'question': 'Error: failed to compute cache key: '\n",
      "                             '\"/model2.bin\" not found: not found',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Initially, I did not assume there was a model2. I '\n",
      "                         'copied the original model1.bin and dv.bin. Then when '\n",
      "                         'I tried to load using\\n'\n",
      "                         'COPY [\"model2.bin\", \"dv.bin\", \"./\"]\\n'\n",
      "                         'then I got the error above in MINGW64 (git bash) on '\n",
      "                         'Windows.\\n'\n",
      "                         'The temporary solution I found was to use\\n'\n",
      "                         'COPY [\"*\", \"./\"]\\n'\n",
      "                         'which I assume combines all the files from the '\n",
      "                         'original docker image and the files in your working '\n",
      "                         'directory.\\n'\n",
      "                         'Added by Muhammed Tan'},\n",
      "                {'question': 'Failed to write the dependencies to pipfile and '\n",
      "                             'piplock file',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Create a virtual environment using the Cmd command '\n",
      "                         '(command) and use pip freeze command to write the '\n",
      "                         'requirements in the text file\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'f-strings',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'f-String not properly keyed in: does anyone knows '\n",
      "                         'why i am getting error after import pickle?\\n'\n",
      "                         'The first error showed up because your f-string is '\n",
      "                         'using () instead of {} around C. So, should be: '\n",
      "                         'f’model_C={C}.bin’\\n'\n",
      "                         'The second error as noticed by Sriniketh, your are '\n",
      "                         'missing one parenthesis it should be '\n",
      "                         'pickle.dump((dv, model), f_out)\\n'\n",
      "                         '(Humberto R.)'},\n",
      "                {'question': \"'pipenv' is not recognized as an internal or \"\n",
      "                             'external command, operable program or batch '\n",
      "                             'file.',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'This error happens because pipenv is already '\n",
      "                         \"installed but you can't access it from the path.\\n\"\n",
      "                         'This error comes out if you run.\\n'\n",
      "                         'pipenv  --version\\n'\n",
      "                         'pipenv shell\\n'\n",
      "                         'Solution for Windows\\n'\n",
      "                         'Open this option\\n'\n",
      "                         'Click here\\n'\n",
      "                         'Click in Edit Button\\n'\n",
      "                         'Make sure the next two locations are on the PATH, '\n",
      "                         'otherwise, add it.\\n'\n",
      "                         'C:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\\\n'\n",
      "                         'C:\\\\Users\\\\AppData\\\\....\\\\Python\\\\PythonXX\\\\Scripts\\\\\\n'\n",
      "                         'Added by Alejandro Aponte\\n'\n",
      "                         'Note: this answer assumes you don’t use Anaconda. '\n",
      "                         'For Windows, using Anaconda would be a better choice '\n",
      "                         'and less prone to errors.'},\n",
      "                {'question': 'AttributeError: module ‘collections’ has no '\n",
      "                             'attribute ‘MutableMapping’',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Following the instruction from video week-5.6, using '\n",
      "                         'pipenv to install python libraries throws below '\n",
      "                         'error\\n'\n",
      "                         'Solution to this error is to make sure that you are '\n",
      "                         'working with python==3.9 (as informed in the very '\n",
      "                         'first lesson of the zoomcamp) and not python==3.10.\\n'\n",
      "                         'Added by Hareesh Tummala'},\n",
      "                {'question': 'Q: ValueError: Path not found or generated: '\n",
      "                             \"WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')\",\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'After entering `pipenv shell` don’t forget to use '\n",
      "                         '`exit` before `pipenv --rm`, as it may cause errors '\n",
      "                         'when trying to install packages, it is unclear '\n",
      "                         'whether you are “in the shell”(using Windows) at the '\n",
      "                         'moment as there are no clear markers for it.\\n'\n",
      "                         'It can also mess up PATH, if that’s the case, here’s '\n",
      "                         'terminal commands for fixing that:\\n'\n",
      "                         '# for Windows\\n'\n",
      "                         'set VIRTUAL_ENV \"\"\\n'\n",
      "                         '# for Unix\\n'\n",
      "                         'export VIRTUAL_ENV=\"\"\\n'\n",
      "                         'Also manually re-creating removed folder at '\n",
      "                         '`C:\\\\Users\\\\username\\\\.virtualenvs\\\\removed-envname` '\n",
      "                         'can help, removed-envname can be seen at the error '\n",
      "                         'message.\\n'\n",
      "                         'Added by Andrii Larkin'},\n",
      "                {'question': \"ConnectionError: ('Connection aborted.', \"\n",
      "                             \"RemoteDisconnected('Remote end closed connection \"\n",
      "                             \"without response'))\",\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Set the host to ‘0.0.0.0’ on the flask app and '\n",
      "                         'dockerfile then RUN the url using localhost.\\n'\n",
      "                         '(Theresa S.)'},\n",
      "                {'question': 'docker  build ERROR [x/y] COPY …',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Solution:\\n'\n",
      "                         'This error occurred because I used single quotes '\n",
      "                         'around the filenames. Stick to double quotes'},\n",
      "                {'question': 'Fix error during installation of Pipfile inside '\n",
      "                             'Docker container',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'I tried the first solution on Stackoverflow which '\n",
      "                         'recommended running `pipenv lock` to update the '\n",
      "                         'Pipfile.lock. However, this didn’t resolve it. But '\n",
      "                         'the following switch to the pipenv installation '\n",
      "                         'worked\\n'\n",
      "                         'RUN pipenv install --system --deploy '\n",
      "                         '--ignore-pipfile'},\n",
      "                {'question': 'How to fix error after running the Docker run '\n",
      "                             'command',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Solution\\n'\n",
      "                         'This error was because there was another instance of '\n",
      "                         'gunicorn running. So I thought of removing this '\n",
      "                         'along with the zoomcamp_test image. However, it '\n",
      "                         'didn’t let me remove the orphan container. So I did '\n",
      "                         'the following\\n'\n",
      "                         'Running the following commands\\n'\n",
      "                         'docker ps -a <to list all docker containers>\\n'\n",
      "                         'docker images <to list images>\\n'\n",
      "                         'docker stop <container ID>\\n'\n",
      "                         'docker rm <container ID>\\n'\n",
      "                         'docker rmi image\\n'\n",
      "                         'I rebuilt the Docker image, and ran it once again; '\n",
      "                         'this time it worked correctly and I was able to '\n",
      "                         'serve the test script to the endpoint.'},\n",
      "                {'question': 'Bind for 0.0.0.0:9696 failed: port is already '\n",
      "                             'allocated',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'I was getting the below error when I rebuilt the '\n",
      "                         'docker image although the port was not allocated, '\n",
      "                         'and it was working fine.\\n'\n",
      "                         'Error message:\\n'\n",
      "                         'Error response from daemon: driver failed '\n",
      "                         'programming external connectivity on endpoint '\n",
      "                         'beautiful_tharp '\n",
      "                         '(875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): '\n",
      "                         'Bind for 0.0.0.0:9696 failed: port is already '\n",
      "                         'allocated.\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Issue has been resolved running the following '\n",
      "                         'command:\\n'\n",
      "                         'docker kill $(docker ps -q)\\n'\n",
      "                         'https://github.com/docker/for-win/issues/2722\\n'\n",
      "                         'Asia Saeed'},\n",
      "                {'question': 'Bind for 127.0.0.1:5000 showing error',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'I was getting the error on client side with this\\n'\n",
      "                         'Client Side:\\n'\n",
      "                         'File '\n",
      "                         '\"C:\\\\python\\\\lib\\\\site-packages\\\\urllib3\\\\connectionpool.py\", '\n",
      "                         'line 703, in urlopen …………………..\\n'\n",
      "                         'raise ConnectionError(err, request=request)\\n'\n",
      "                         \"requests.exceptions.ConnectionError: ('Connection \"\n",
      "                         \"aborted.', RemoteDisconnected('Remote end closed \"\n",
      "                         \"connection without response'))\\n\"\n",
      "                         'Sevrer Side:\\n'\n",
      "                         'It showed error for gunicorn\\n'\n",
      "                         'The waitress  cmd was running smoothly from server '\n",
      "                         'side\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Use the ip-address as 0.0.0.0:8000 or '\n",
      "                         '0.0.0.0:9696.They are the ones which do work max '\n",
      "                         'times\\n'\n",
      "                         'Aamir Wani'},\n",
      "                {'question': 'Installing md5sum on Macos',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Install it by using command\\n'\n",
      "                         '% brew install md5sha1sum\\n'\n",
      "                         'Then run command to check hash for file to check if '\n",
      "                         'they the same with the provided\\n'\n",
      "                         '% md5sum model1.bin dv.bin\\n'\n",
      "                         'Olga Rudakova'},\n",
      "                {'question': 'How to run a script while a web-server is '\n",
      "                             'working?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Problem description:\\n'\n",
      "                         'I started a web-server in terminal (command window, '\n",
      "                         'powershell, etc.). How can I run another python '\n",
      "                         'script, which makes a request to this server?\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'Just open another terminal (command window, '\n",
      "                         'powershell, etc.) and run a python script.\\n'\n",
      "                         'Alena Kniazeva'},\n",
      "                {'question': 'Version-conflict in pipenv',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Problem description:\\n'\n",
      "                         'In video 5.5 when I do pipenv shell and then pipenv '\n",
      "                         'run gunicorn --bind 0.0.0.0:9696 predict:app, I get '\n",
      "                         'the following warning:\\n'\n",
      "                         'UserWarning: Trying to unpickle estimator '\n",
      "                         'DictVectorizer from version 1.1.1 when using version '\n",
      "                         '0.24.2. This might lead to breaking code or invalid '\n",
      "                         'results. Use at your own risk.\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'When you create a virtual env, you should use the '\n",
      "                         'same version of Scikit-Learn that you used for '\n",
      "                         \"training the model on this case it's 1.1.1. There is \"\n",
      "                         'version conflicts so we need to make sure our model '\n",
      "                         'and dv files are created from the version we are '\n",
      "                         'using for the project.\\n'\n",
      "                         'Bhaskar Sarma'},\n",
      "                {'question': 'Python_version and Python_full_version error '\n",
      "                             'after running pipenv install:',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'If you install packages via pipenv install, and get '\n",
      "                         'an error that ends like this:\\n'\n",
      "                         'pipenv.vendor.plette.models.base.ValidationError: '\n",
      "                         \"{'python_version': '3.9', 'python_full_version': \"\n",
      "                         \"'3.9.13'}\\n\"\n",
      "                         \"python_full_version: 'python_version' must not be \"\n",
      "                         \"present with 'python_full_version'\\n\"\n",
      "                         \"python_version: 'python_full_version' must not be \"\n",
      "                         \"present with 'python_version'\\n\"\n",
      "                         'Do this:\\n'\n",
      "                         'open Pipfile in nano editor, and remove either the '\n",
      "                         'python_version or python_full_version line, press '\n",
      "                         'CTRL+X, type Y and click Enter to save changed\\n'\n",
      "                         'Type pipenv lock to create the Pipfile.lock.\\n'\n",
      "                         'Done. Continue what you were doing'},\n",
      "                {'question': 'Your Pipfile.lock (221d14) is out of date '\n",
      "                             '(during Docker build)',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'If during running the  docker build command, you get '\n",
      "                         'an error like this:\\n'\n",
      "                         'Your Pipfile.lock (221d14) is out of date. Expected: '\n",
      "                         '(939fe0).\\n'\n",
      "                         'Usage: pipenv install [OPTIONS] [PACKAGES]...\\n'\n",
      "                         'ERROR:: Aborting deploy\\n'\n",
      "                         'Option 1: Delete the pipfile.lock via rm Pipfile, '\n",
      "                         'and then rebuild the lock via  pipenv lock from the '\n",
      "                         'terminal before retrying the docker build command.\\n'\n",
      "                         'Option 2:  If it still doesn’t work, remove the '\n",
      "                         'pipenv environment, Pipfile and Pipfile.lock, and '\n",
      "                         'create a new one before building docker again. '\n",
      "                         'Commands to remove pipenv environment and removing '\n",
      "                         'pipfiles:\\n'\n",
      "                         'pipenv  --rm\\n'\n",
      "                         'rm Pipfile*'},\n",
      "                {'question': 'You are using windows. Conda environment. You '\n",
      "                             'then use waitress instead of gunicorn. After a '\n",
      "                             'few runs, suddenly mlflow server fails to run.',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Ans: Pip uninstall waitress mflow. Then reinstall '\n",
      "                         'just mlflow. By this time you should have '\n",
      "                         'successfully built your docker image so you dont '\n",
      "                         'need to reinstall waitress. All good. Happy '\n",
      "                         'learning.\\n'\n",
      "                         'Added by 🅱🅻🅰🆀'},\n",
      "                {'question': 'Completed creating the environment locally but '\n",
      "                             'could not find the environment on AWS.',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Ans: so you have created the env. You need to make '\n",
      "                         \"sure you're in eu-west-1 (ireland) when you check \"\n",
      "                         \"the EB environments. Maybe you're in a different \"\n",
      "                         'region in your console.\\n'\n",
      "                         'Added by Edidiong Esu'},\n",
      "                {'question': 'Installing waitress on Windows via GitBash: '\n",
      "                             '“waitress-serve” command not found',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': \"Running 'pip install waitress' as a command on \"\n",
      "                         'GitBash was not downloading the executable file '\n",
      "                         \"'waitress-serve.exe'. You need this file to be able \"\n",
      "                         'to run commands with waitress in Git Bash. To solve '\n",
      "                         'this:\\n'\n",
      "                         \"open a Jupyter notebook and run the same command ' \"\n",
      "                         \"pip install waitress'. This way the executable file \"\n",
      "                         'will be downloaded. The notebook may give you this '\n",
      "                         \"warning : 'WARNING: The script waitress-serve.exe is \"\n",
      "                         \"installed in 'c:\\\\Users\\\\....\\\\anaconda3\\\\Scripts' \"\n",
      "                         'which is not on PATH. Consider adding this directory '\n",
      "                         'to PATH or, if you prefer to suppress this warning, '\n",
      "                         \"use --no-warn-script-location.'\\n\"\n",
      "                         \"Add the path where 'waitress-serve.exe' is installed \"\n",
      "                         \"into gitbash's PATH as such:\\n\"\n",
      "                         'enter the following command in gitbash: nano '\n",
      "                         '~/.bashrc\\n'\n",
      "                         \"add the path to 'waitress-serve.exe' to PATH using \"\n",
      "                         'this command: export PATH=\"/path/to/waitress:$PATH\"\\n'\n",
      "                         'close gitbash and open it again and you should be '\n",
      "                         'good to go\\n'\n",
      "                         'Added by Bachar Kabalan'},\n",
      "                {'question': 'Warning: the environment variable LANG is not '\n",
      "                             'set!',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Q2.1: Use Pipenv to install Scikit-Learn version '\n",
      "                         '1.3.1\\n'\n",
      "                         'This is an error I got while executing the above '\n",
      "                         'step in the ml-zoomcamp conda environment. The error '\n",
      "                         'is not fatal and just warns you that explicit '\n",
      "                         'language specifications are not set out in our bash '\n",
      "                         'profile. A quick-fix is here:\\n'\n",
      "                         'https://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\\n'\n",
      "                         'But one can proceed without addressing it.\\n'\n",
      "                         'Added by Abhirup Ghosh'},\n",
      "                {'question': 'Module5 HW Question 6',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'The provided image FROM '\n",
      "                         'svizor/zoomcamp-model:3.10.12-slim has a model and '\n",
      "                         'dictvectorizer that should be used for question 6. '\n",
      "                         '\"model2.bin\", \"dv.bin\"\\n'\n",
      "                         'Added by Quinn Avila'},\n",
      "                {'question': 'Terminal Used in Week 5 videos:',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO\\n'\n",
      "                         'Added by Dawuta Smit'},\n",
      "                {'question': 'waitress-serve shows Malformed application',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Question:\\n'\n",
      "                         'When running\\n'\n",
      "                         'pipenv run waitress-serve --listen=localhost:9696 '\n",
      "                         'q4-predict:app\\n'\n",
      "                         'I get the following:\\n'\n",
      "                         'There was an exception (ValueError) importing your '\n",
      "                         'module.\\n'\n",
      "                         'It had these arguments:\\n'\n",
      "                         \"1. Malformed application 'q4-predict:app'\\n\"\n",
      "                         'Answer:\\n'\n",
      "                         'Waitress doesn’t accept a dash in the python file '\n",
      "                         'name.\\n'\n",
      "                         'The solution is to rename the file replacing a dash '\n",
      "                         'with something else for instance with an underscore '\n",
      "                         'eg q4_predict.py\\n'\n",
      "                         'Added by Alex Litvinov'},\n",
      "                {'question': 'Testing HTTP POST requests from command line '\n",
      "                             'using curl',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'I wanted to have a fast and simple way to check if '\n",
      "                         'the HTTP POST requests are working just running a '\n",
      "                         'request from command line. This can be done running '\n",
      "                         '‘curl’. \\n'\n",
      "                         '(Used with WSL2 on Windows, should also work on '\n",
      "                         'Linux and MacOS)\\n'\n",
      "                         \"curl --json '<json data>' <url>\\n\"\n",
      "                         '# piping the structure to the command\\n'\n",
      "                         'cat <json file path> | curl --json @- <url>\\n'\n",
      "                         \"echo '<json data>' | curl --json @- <url>\\n\"\n",
      "                         '# example using piping\\n'\n",
      "                         'echo \\'{\"job\": \"retired\", \"duration\": 445, '\n",
      "                         '\"poutcome\": \"success\"}\\'\\\\\\n'\n",
      "                         '| curl --json @- http://localhost:9696/predict\\n'\n",
      "                         'Added by Sylvia Schmitt'},\n",
      "                {'question': 'NotSupportedError - You can use \"eb local\" only '\n",
      "                             'with preconfigured, generic and multicontainer '\n",
      "                             'Docker platforms.',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Question:\\n'\n",
      "                         'When executing\\n'\n",
      "                         'eb local run  --port 9696\\n'\n",
      "                         'I get the following error:\\n'\n",
      "                         'ERROR: NotSupportedError - You can use \"eb local\" '\n",
      "                         'only with preconfigured, generic and multicontainer '\n",
      "                         'Docker platforms.\\n'\n",
      "                         'Answer:\\n'\n",
      "                         'There are two options to fix this:\\n'\n",
      "                         'Re-initialize by running eb init -i and choosing the '\n",
      "                         'options from a list (the first default option for '\n",
      "                         'docker platform should be fine).\\n'\n",
      "                         'Edit the ‘.elasticbeanstalk/config.yml’ directly '\n",
      "                         'changing the default_platform from Docker to '\n",
      "                         'default_platform: Docker running on 64bit Amazon '\n",
      "                         'Linux 2023\\n'\n",
      "                         'The disadvantage of the second approach is that the '\n",
      "                         'option might not be available the following years\\n'\n",
      "                         'Added by Alex Litvinov'},\n",
      "                {'question': 'Requests Error: No connection adapters were '\n",
      "                             \"found for 'localhost:9696/predict'.\",\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'You need to include the protocol scheme: '\n",
      "                         \"'http://localhost:9696/predict'.\\n\"\n",
      "                         'Without the http:// part, requests has no idea how '\n",
      "                         'to connect to the remote server.\\n'\n",
      "                         'Note that the protocol scheme must be all lowercase; '\n",
      "                         'if your URL starts with HTTP:// for example, it '\n",
      "                         'won’t find the http:// connection adapter either.\\n'\n",
      "                         'Added by George Chizhmak'},\n",
      "                {'question': 'Getting the same result',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'While running the docker image if you get the same '\n",
      "                         'result check which model you are using.\\n'\n",
      "                         'Remember you are using a model downloading model + '\n",
      "                         'python version so remember to change the model in '\n",
      "                         'your file when running your prediction test.\\n'\n",
      "                         'Added by Ahmed Okka'},\n",
      "                {'question': 'Trying to run a docker image I built but it says '\n",
      "                             'it’s unable to start the container process',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'Ensure that you used pipenv to install the necessary '\n",
      "                         'modules including gunicorn. As pipfiles for virtual '\n",
      "                         'environments, you can use pipenv shell and then '\n",
      "                         'build+run your docker image. - Akshar Goyal'},\n",
      "                {'question': 'How do I copy files from my local machine to '\n",
      "                             'docker container?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'You can copy files from your local machine into a '\n",
      "                         \"Docker container using the docker cp command. Here's \"\n",
      "                         'how to do it:\\n'\n",
      "                         'To copy a file or directory from your local machine '\n",
      "                         'into a running Docker container, you can use the '\n",
      "                         '`docker cp command`. The basic syntax is as '\n",
      "                         'follows:\\n'\n",
      "                         'docker cp /path/to/local/file_or_directory '\n",
      "                         'container_id:/path/in/container\\n'\n",
      "                         'Hrithik Kumar Advani'},\n",
      "                {'question': 'How do I copy files from a different folder into '\n",
      "                             'docker container’s working directory?',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'You can copy files from your local machine into a '\n",
      "                         \"Docker container using the docker cp command. Here's \"\n",
      "                         'how to do it:\\n'\n",
      "                         'In the Dockerfile, you can provide the folder '\n",
      "                         'containing the files that you want to copy over. The '\n",
      "                         'basic syntax is as follows:\\n'\n",
      "                         'COPY [\"src/predict.py\", \"models/xgb_model.bin\", '\n",
      "                         '\"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan'},\n",
      "                {'question': 'I can’t create the environment on AWS Elastic '\n",
      "                             'Beanstalk with the command proposed during the '\n",
      "                             'video',\n",
      "                 'section': '5. Deploying Machine Learning Models',\n",
      "                 'text': 'I struggled with the command :\\n'\n",
      "                         'eb init -p docker tumor-diagnosis-serving -r '\n",
      "                         'eu-west-1\\n'\n",
      "                         'Which resulted in an error when running : eb local '\n",
      "                         'run --port 9696\\n'\n",
      "                         'ERROR: NotSupportedError - You can use \"eb local\" '\n",
      "                         'only with preconfigured, generic and multicontainer '\n",
      "                         'Docker platforms.\\n'\n",
      "                         'I replaced it with :\\n'\n",
      "                         'eb init -p \"Docker running on 64bit Amazon Linux 2\" '\n",
      "                         'tumor-diagnosis-serving -r eu-west-1\\n'\n",
      "                         'This allowed the recognition of the Dockerfile and '\n",
      "                         'the build/run of the docker container.\\n'\n",
      "                         'Added by Mélanie Fouesnard'},\n",
      "                {'question': 'Dockerfile missing when creating the AWS '\n",
      "                             'ElasticBean environment',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'I had this error when creating a AWS ElasticBean '\n",
      "                         'environment: eb create tumor-diagnosis-env\\n'\n",
      "                         \"ERROR   Instance deployment: Both 'Dockerfile' and \"\n",
      "                         \"'Dockerrun.aws.json' are missing in your source \"\n",
      "                         'bundle. Include at least one of them. The deployment '\n",
      "                         'failed.\\n'\n",
      "                         'I did not committed the files used to build the '\n",
      "                         'container, particularly the Dockerfile. After a git '\n",
      "                         'add and git commit of the modified files, the '\n",
      "                         'command works.\\n'\n",
      "                         'Added by Mélanie Fouesnard'},\n",
      "                {'question': 'How to get started with Week 6?',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'Week 6 HW: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\\n'\n",
      "                         'All HWs: '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\\n'\n",
      "                         'HW 4 Solution: '\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\\n'\n",
      "                         'Evaluation Matrix: '\n",
      "                         'https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\\n'\n",
      "                         'GitHub for theory: '\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\\n'\n",
      "                         'YouTube Link: 6.X --- '\n",
      "                         'https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\\n'\n",
      "                         'FAQs: '\n",
      "                         'https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\\n'\n",
      "                         '~~~Nukta Bhatia~~~'},\n",
      "                {'question': 'How to get the training and validation metrics '\n",
      "                             'from XGBoost?',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'During the XGBoost lesson, we created a parser to '\n",
      "                         'extract the training and validation auc from the '\n",
      "                         'standard output. However, we can accomplish that in '\n",
      "                         'a more straightforward way.\\n'\n",
      "                         'We can use the evals_result  parameters, which takes '\n",
      "                         'an empty dictionary and updates it for each tree. '\n",
      "                         'Additionally, you can store the data in a dataframe '\n",
      "                         'and plot it in an easier manner.\\n'\n",
      "                         'Added by Daniel Coronel'},\n",
      "                {'question': 'How to solve regression problems with random '\n",
      "                             'forest in scikit-learn?',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'You should create '\n",
      "                         'sklearn.ensemble.RandomForestRegressor object. It’s '\n",
      "                         'rather similar to '\n",
      "                         'sklearn.ensemble.RandomForestClassificator for '\n",
      "                         'classification problems. Check '\n",
      "                         'https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html '\n",
      "                         'for more information.\\n'\n",
      "                         'Alena Kniazeva'},\n",
      "                {'question': 'ValueError: feature_names must be string, and '\n",
      "                             'may not contain [, ] or <',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'In question 6, I was getting ValueError: '\n",
      "                         'feature_names must be string, and may not contain [, '\n",
      "                         '] or < when I was creating DMatrix for train and '\n",
      "                         'validation\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'The cause of this error is that some of the features '\n",
      "                         'names contain special characters like = and <, and I '\n",
      "                         'fixed the error by removing them as  follows:\\n'\n",
      "                         'features= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for '\n",
      "                         'i in features]\\n'\n",
      "                         'Asia Saeed\\n'\n",
      "                         'Alternative Solution:\\n'\n",
      "                         'In my case the equal sign “=” was not a problem, so '\n",
      "                         'in my opinion the first part of Asias solution '\n",
      "                         'features= [i.replace(\"=<\", \"_\") should work as '\n",
      "                         'well.\\n'\n",
      "                         'For me this works:\\n'\n",
      "                         'features = []\\n'\n",
      "                         'for f in dv.feature_names_:\\n'\n",
      "                         'string = f.replace(“=<”, “-le”)\\n'\n",
      "                         'features.append(string)\\n'\n",
      "                         'Peter Ernicke'},\n",
      "                {'question': '`TypeError: Expecting a sequence of strings for '\n",
      "                             \"feature names, got: <class 'numpy.ndarray'> ` \"\n",
      "                             'when training xgboost model.',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'If you’re getting this error, It is likely because '\n",
      "                         'the feature names in dv.get_feature_names_out() are '\n",
      "                         'a np.ndarray instead of a list so you have to '\n",
      "                         'convert them into a list by using the to_list() '\n",
      "                         'method.\\n'\n",
      "                         'Ali Osman'},\n",
      "                {'question': 'Q6: ValueError or TypeError while setting '\n",
      "                             'xgb.DMatrix(feature_names=)',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'If you’re getting TypeError:\\n'\n",
      "                         '“TypeError: Expecting a sequence of strings for '\n",
      "                         \"feature names, got: <class 'numpy.ndarray'>”,\\n\"\n",
      "                         'probably you’ve done this:\\n'\n",
      "                         'features = dv.get_feature_names_out()\\n'\n",
      "                         'It gets you np.ndarray instead of list. Converting '\n",
      "                         'to list list(features) will not fix this, read '\n",
      "                         'below.\\n'\n",
      "                         'If you’re getting ValueError:\\n'\n",
      "                         '“ValueError: feature_names must be string, and may '\n",
      "                         'not contain [, ] or <”,\\n'\n",
      "                         'probably you’ve either done:\\n'\n",
      "                         'features = list(dv.get_feature_names_out())\\n'\n",
      "                         'or:\\n'\n",
      "                         'features = dv.feature_names_\\n'\n",
      "                         'reason is what you get from DictVectorizer here '\n",
      "                         'looks like this:\\n'\n",
      "                         \"['households',\\n\"\n",
      "                         \"'housing_median_age',\\n\"\n",
      "                         \"'latitude',\\n\"\n",
      "                         \"'longitude',\\n\"\n",
      "                         \"'median_income',\\n\"\n",
      "                         \"'ocean_proximity=<1H OCEAN',\\n\"\n",
      "                         \"'ocean_proximity=INLAND',\\n\"\n",
      "                         \"'population',\\n\"\n",
      "                         \"'total_bedrooms',\\n\"\n",
      "                         \"'total_rooms']\\n\"\n",
      "                         'it has symbols XGBoost doesn’t like ([, ] or <).\\n'\n",
      "                         'What you can do, is either do not specify '\n",
      "                         '“feature_names=” while creating xgb.DMatrix or:\\n'\n",
      "                         'import re\\n'\n",
      "                         'features = dv.feature_names_\\n'\n",
      "                         \"pattern = r'[\\\\[\\\\]<>]'\\n\"\n",
      "                         \"features = [re.sub(pattern, '  ', f) for f in \"\n",
      "                         'features]\\n'\n",
      "                         'Added by Andrii Larkin'},\n",
      "                {'question': 'How to Install Xgboost',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'To install Xgboost, use the code below directly in '\n",
      "                         'your jupyter notebook:\\n'\n",
      "                         '(Pip 21.3+ is required)\\n'\n",
      "                         'pip install xgboost\\n'\n",
      "                         'You can update your pip by using the code below:\\n'\n",
      "                         'pip install --upgrade pip\\n'\n",
      "                         'For more about xgbboost and installation, check '\n",
      "                         'here:\\n'\n",
      "                         'https://xgboost.readthedocs.io/en/stable/install.html\\n'\n",
      "                         'Aminat Abolade'},\n",
      "                {'question': 'What is eta in XGBoost',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'Sometimes someone might wonder what eta means in the '\n",
      "                         'tunable hyperparameters of XGBoost and how it helps '\n",
      "                         'the model.\\n'\n",
      "                         'ETA is the learning rate of the model. XGBoost uses '\n",
      "                         'gradient descent to calculate and update the model. '\n",
      "                         'In gradient descent, we are looking for the minimum '\n",
      "                         'weights that help the model to learn the data very '\n",
      "                         'well. This minimum weights for the features is '\n",
      "                         'updated each time the model passes through the '\n",
      "                         'features and learns the features during training. '\n",
      "                         'Tuning the learning rate helps you tell the model '\n",
      "                         'what speed it would use in deriving the minimum for '\n",
      "                         'the weights.'},\n",
      "                {'question': 'What is the difference between bagging and '\n",
      "                             'boosting?',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'For ensemble algorithms, during the week 6, one '\n",
      "                         'bagging algorithm and one boosting algorithm were '\n",
      "                         'presented: Random Forest and XGBoost, respectively.\\n'\n",
      "                         'Random Forest trains several models in parallel. The '\n",
      "                         'output can be, for example, the average value of all '\n",
      "                         'the outputs of each model. This is called bagging.\\n'\n",
      "                         'XGBoost trains several models sequentially: the '\n",
      "                         'previous model error is used to train the following '\n",
      "                         'model. Weights are used to ponderate the models such '\n",
      "                         'as the best models have higher weights and are '\n",
      "                         'therefore favored for the final output. This method '\n",
      "                         'is called boosting.\\n'\n",
      "                         'Note that boosting is not necessarily better than '\n",
      "                         'bagging.\\n'\n",
      "                         'Mélanie Fouesnard\\n'\n",
      "                         'Bagging stands for “Bootstrap Aggregation” - it '\n",
      "                         'involves taking multiple samples with replacement to '\n",
      "                         'derive multiple training datasets from the original '\n",
      "                         'training dataset (bootstrapping), training a '\n",
      "                         'classifier (e.g. decision trees or stumps for Random '\n",
      "                         'Forests) on each such training dataset, and then '\n",
      "                         'combining the the predictions (aggregation) to '\n",
      "                         'obtain the final prediction. For classification, '\n",
      "                         'predictions are combined via voting; for regression, '\n",
      "                         'via averaging. Bagging can be done in parallel, '\n",
      "                         'since the various classifiers are independent. '\n",
      "                         'Bagging decreases variance (but not bias) and is '\n",
      "                         'robust against overfitting.\\n'\n",
      "                         'Boosting, on the other hand, is sequential - each '\n",
      "                         'model learns from the mistakes of its predecessor. '\n",
      "                         'Observations are given different weights - '\n",
      "                         'observations/samples misclassified by the previous '\n",
      "                         'classifier are given a higher weight, and this '\n",
      "                         'process is continued until a stopping condition is '\n",
      "                         'reached (e.g. max. No. of models is reached, or '\n",
      "                         'error is acceptably small, etc.). Boosting reduces '\n",
      "                         'bias & is generally more accurate than bagging, but '\n",
      "                         'can be prone to overfitting.\\n'\n",
      "                         'Rileen'},\n",
      "                {'question': 'Capture stdout for each iterations of a loop '\n",
      "                             'separately',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'I wanted to directly capture the output from the '\n",
      "                         'xgboost training for multiple eta values to a '\n",
      "                         'dictionary without the need to run the same cell '\n",
      "                         'multiple times and manually editing the eta value in '\n",
      "                         'between or copy the code for a second eta value.\\n'\n",
      "                         'Using the magic cell command “%%capture output” I '\n",
      "                         'was only able to capture the complete output for all '\n",
      "                         'iterations for the loop, but. I was able to solve '\n",
      "                         'this using the following approach. This is just a '\n",
      "                         'code sample to grasp the idea.\\n'\n",
      "                         '# This would be the content of the Jupyter Notebook '\n",
      "                         'cell\\n'\n",
      "                         'from IPython.utils.capture import capture_output\\n'\n",
      "                         'import sys\\n'\n",
      "                         'different_outputs = {}\\n'\n",
      "                         'for i in range(3):\\n'\n",
      "                         'with capture_output(sys.stdout) as output:\\n'\n",
      "                         'print(i)\\n'\n",
      "                         'print(\"testing capture\")\\n'\n",
      "                         'different_outputs[i] = output.stdout\\n'\n",
      "                         '# different_outputs\\n'\n",
      "                         \"# {0: '0\\\\ntesting capture\\\\n',\\n\"\n",
      "                         \"#  1: '1\\\\ntesting capture\\\\n',\\n\"\n",
      "                         \"#  2: '2\\\\ntesting capture\\\\n'}\\n\"\n",
      "                         'Added by Sylvia Schmitt'},\n",
      "                {'question': 'ValueError: continuous format is not supported',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'Calling roc_auc_score() to get auc is throwing the '\n",
      "                         'above error.\\n'\n",
      "                         'Solution to this issue is to make sure that you pass '\n",
      "                         'y_actuals as 1st argument and y_pred as 2nd '\n",
      "                         'argument.\\n'\n",
      "                         'roc_auc_score(y_train, y_pred)\\n'\n",
      "                         'Hareesh Tummala'},\n",
      "                {'question': 'Question 3 of homework 6 if i see that rmse goes '\n",
      "                             'up at a certain number of n_estimators but then '\n",
      "                             'goes back down lower than it was before, should '\n",
      "                             'the answer be the number of n_estimators after '\n",
      "                             'which rmse initially went up, or the number '\n",
      "                             'after which it was its overall lowest value?',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'When rmse stops improving means, when it stops to '\n",
      "                         'decrease or remains almost similar.\\n'\n",
      "                         'Pastor Soto'},\n",
      "                {'question': 'One of the method to visualize the decision '\n",
      "                             'trees',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'dot_data = tree.export_graphviz(regr, '\n",
      "                         'out_file=None,\\n'\n",
      "                         'feature_names=boston.feature_names,\\n'\n",
      "                         'filled=True)\\n'\n",
      "                         'graphviz.Source(dot_data, format=\"png\")\\n'\n",
      "                         'Krishna Anand\\n'\n",
      "                         'from sklearn import tree\\n'\n",
      "                         'tree.plot_tree(dt,feature_names=dv.feature_names_)\\n'\n",
      "                         'Added By Ryan Pramana'},\n",
      "                {'question': \"ValueError: Unknown label type: 'continuous'\",\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'Solution: This problem happens because you use '\n",
      "                         'DecisionTreeClassifier instead of '\n",
      "                         'DecisionTreeRegressor. You should check if you want '\n",
      "                         'to use a Decision tree for classification or '\n",
      "                         'regression.\\n'\n",
      "                         'Alejandro Aponte'},\n",
      "                {'question': 'Different values of auc, each time code is '\n",
      "                             're-run',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'When I run dt = DecisionTreeClassifier() in jupyter '\n",
      "                         'in same laptop, each time I re-run it or do (restart '\n",
      "                         'kernel + run) I get different values of auc. Some of '\n",
      "                         'them are 0.674, 0.652, 0.642, 0.669 and so on.  '\n",
      "                         'Anyone knows why it could be? I am referring to '\n",
      "                         '7:40-7:45 of video 6.3.\\n'\n",
      "                         'Solution: try setting the random seed e.g\\n'\n",
      "                         'dt = DecisionTreeClassifier(random_state=22)\\n'\n",
      "                         'Bhaskar Sarma'},\n",
      "                {'question': 'Does it matter if we let the Python file create '\n",
      "                             'the server or if we run gunicorn directly?',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': \"They both do the same, it's just less typing from \"\n",
      "                         'the script.\\n'\n",
      "                         'Asked by Andrew Katoch, Added by Edidiong Esu'},\n",
      "                {'question': 'No module named ‘ping’?',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'When I tried to run example from the video using '\n",
      "                         'function ping and can not import it. I use import '\n",
      "                         'ping and it was unsuccessful. To fix it I use the '\n",
      "                         'statement:\\n'\n",
      "                         '\\n'\n",
      "                         'from [file name] import ping\\n'\n",
      "                         'Olga Rudakova'},\n",
      "                {'question': 'DictVectorizer feature names',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'The DictVectorizer has a function to get the feature '\n",
      "                         'names get_feature_names_out(). This is helpful for '\n",
      "                         'example if you need to analyze feature importance '\n",
      "                         'but use the dict vectorizer for one hot encoding. '\n",
      "                         'Just keep in mind it does return a numpy array so '\n",
      "                         'you may need to convert this to a list depending on '\n",
      "                         'your usage for example dv.get_feature_names_out() '\n",
      "                         'will return a ndarray array of string objects. '\n",
      "                         'list(dv.get_feature_names_out()) will convert to a '\n",
      "                         'standard list of strings. Also keep in mind that you '\n",
      "                         'first need to fit the predictor and response arrays '\n",
      "                         'before you have access to the feature names.\\n'\n",
      "                         'Quinn Avila'},\n",
      "                {'question': 'Does it matter if we let the Python file create '\n",
      "                             'the server or if we run gunicorn directly?',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': \"They both do the same, it's just less typing from \"\n",
      "                         'the script.'},\n",
      "                {'question': 'ValueError: feature_names must be string, and '\n",
      "                             'may not contain [, ] or <',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'This error occurs because the list of feature names '\n",
      "                         'contains some characters like \"<\" that are not '\n",
      "                         'supported. To fix this issue, you can replace those '\n",
      "                         'problematic characters with supported ones. If you '\n",
      "                         'want to create a consistent list of features with no '\n",
      "                         'special characters, you can achieve it like this:\\n'\n",
      "                         'You can address this error by replacing problematic '\n",
      "                         'characters in the feature names with underscores, '\n",
      "                         'like so:\\n'\n",
      "                         \"features = [f.replace('=<', '_').replace('=', '_') \"\n",
      "                         'for f in features]\\n'\n",
      "                         'This code will go through the list of features and '\n",
      "                         'replace any instances of \"=<\" with \"\", as well as '\n",
      "                         'any \"=\" with \"\", ensuring that the feature names '\n",
      "                         'only consist of supported characters.'},\n",
      "                {'question': 'Visualize Feature Importance by using horizontal '\n",
      "                             'bar chart',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'To make it easier for us to determine which features '\n",
      "                         'are important, we can use a horizontal bar chart to '\n",
      "                         'illustrate feature importance sorted by value.\\n'\n",
      "                         '1. # extract the feature importances from the model\\n'\n",
      "                         'feature_importances = list(zip(features_names, '\n",
      "                         'rdr_model.feature_importances_))\\n'\n",
      "                         'importance_df = pd.DataFrame(feature_importances, '\n",
      "                         \"columns=['feature_names', 'feature_importances'])\\n\"\n",
      "                         '2. # sort descending the dataframe by using '\n",
      "                         'feature_importances value\\n'\n",
      "                         'importance_df = '\n",
      "                         \"importance_df.sort_values(by='feature_importances', \"\n",
      "                         'ascending=False)\\n'\n",
      "                         '3. # create a horizontal bar chart\\n'\n",
      "                         'plt.figure(figsize=(8, 6))\\n'\n",
      "                         \"sns.barplot(x='feature_importances', \"\n",
      "                         \"y='feature_names', data=importance_df, \"\n",
      "                         \"palette='Blues_r')\\n\"\n",
      "                         \"plt.xlabel('Feature Importance')\\n\"\n",
      "                         \"plt.ylabel('Feature Names')\\n\"\n",
      "                         \"plt.title('Feature Importance Chart')\\n\"\n",
      "                         'Radikal Lukafiardi'},\n",
      "                {'question': 'RMSE using metrics.root_meas_square()',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'Instead of using np.sqrt() as the second step. You '\n",
      "                         'can extract it using like this way :\\n'\n",
      "                         'mean_squared_error(y_val, '\n",
      "                         'y_predict_val,squared=False)\\n'\n",
      "                         'Ahmed Okka'},\n",
      "                {'question': 'Features Importance graph',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'I like this visual implementation of features '\n",
      "                         'importance in scikit-learn library:\\n'\n",
      "                         'https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\\n'\n",
      "                         'It actually adds std.errors to features importance '\n",
      "                         '-> so that you can trace stability of features '\n",
      "                         '(important for a model’s explainability) over the '\n",
      "                         'different params of the model.\\n'\n",
      "                         'Ivan Brigida'},\n",
      "                {'question': 'xgboost.core.XGBoostError: This app has '\n",
      "                             'encountered an error. The original error message '\n",
      "                             'is redacted to prevent data leaks.',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'Expanded error says: xgboost.core.XGBoostError: '\n",
      "                         'sklearn needs to be installed in order to use this '\n",
      "                         'module. So, sklearn in requirements solved the '\n",
      "                         'problem.\\n'\n",
      "                         'George Chizhmak'},\n",
      "                {'question': 'Information Gain',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'Information gain  in Y due to X, or the mutual '\n",
      "                         'information of Y and X\\n'\n",
      "                         'Where  is the entropy of Y. \\n'\n",
      "                         '\\n'\n",
      "                         'If X is completely uninformative about Y:\\n'\n",
      "                         'If X is completely informative about Y: )\\n'\n",
      "                         'Hrithik Kumar Advani'},\n",
      "                {'question': 'Data Leakage',\n",
      "                 'section': '6. Decision Trees and Ensemble Learning',\n",
      "                 'text': 'Filling in missing values using an entire dataset '\n",
      "                         'before splitting for training/testing/validation '\n",
      "                         'causes'},\n",
      "                {'question': 'Serialized Model Xgboost error',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Save model by calling ‘booster.save_model’, see eg\\n'\n",
      "                         'Load model:\\n'\n",
      "                         'Dawuta Smit\\n'\n",
      "                         'This section is moved to Projects'},\n",
      "                {'question': 'How to get started with Week 8?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'TODO'},\n",
      "                {'question': 'How to use Kaggle for Deep Learning?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Create or import your notebook into Kaggle.\\n'\n",
      "                         'Click on the Three dots at the top right hand side\\n'\n",
      "                         'Click on Accelerator\\n'\n",
      "                         'Choose T4 GPU\\n'\n",
      "                         'Khurram Majeed'},\n",
      "                {'question': 'How to use Google Colab for Deep Learning?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Create or import your notebook into Google Colab.\\n'\n",
      "                         'Click on the Drop Down at the top right hand side\\n'\n",
      "                         'Click on “Change runtime type”\\n'\n",
      "                         'Choose T4 GPU\\n'\n",
      "                         'Khurram Majeed'},\n",
      "                {'question': 'How do I push from Saturn Cloud to Github?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Connecting your GPU on Saturn Cloud to Github '\n",
      "                         'repository is not compulsory, since you can just '\n",
      "                         'download the notebook and copy it to the Github '\n",
      "                         'folder. But if you like technology to do things for '\n",
      "                         'you, then follow the solution description below:\\n'\n",
      "                         'Solution description: Follow the instructions in '\n",
      "                         'these github docs to create an SSH private and '\n",
      "                         'public key:\\n'\n",
      "                         'https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\\n'\n",
      "                         'y-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\\n'\n",
      "                         'Then the second video on this module about saturn '\n",
      "                         'cloud would show you how to add the ssh keys to '\n",
      "                         'secrets and authenticate through a terminal.\\n'\n",
      "                         'Or alternatively, you could just use the public keys '\n",
      "                         'provided by Saturn Cloud by default. To do so, '\n",
      "                         'follow these steps:\\n'\n",
      "                         'Click on your username and on manage\\n'\n",
      "                         'Down below you will see the Git SSH keys section.\\n'\n",
      "                         'Copy the default public key provided by Saturn '\n",
      "                         'Cloud\\n'\n",
      "                         'Paste these key into the SSH keys section of your '\n",
      "                         'github repo\\n'\n",
      "                         'Open a terminal on Saturn Cloud and run this command '\n",
      "                         '“ssh -T git@github.com”\\n'\n",
      "                         'You will receive a successful authentication '\n",
      "                         'notice.\\n'\n",
      "                         'Odimegwu David'},\n",
      "                {'question': 'Where is the Python TensorFlow template on '\n",
      "                             'Saturn Cloud?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'This template is referred to in the video 8.1b '\n",
      "                         'Setting up the Environment on Saturn Cloud\\n'\n",
      "                         'but the location shown in the video is no longer '\n",
      "                         'correct.\\n'\n",
      "                         'This template has been moved to “python deep '\n",
      "                         'learning tutorials’ which is shown on the Saturn '\n",
      "                         'Cloud home page.\\n'\n",
      "                         'Steven Christolis'},\n",
      "                {'question': 'Getting error module scipy not found during '\n",
      "                             'model training in Saturn Cloud tensorflow image',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'The above error happens since module scipy is not '\n",
      "                         'installed in the saturn cloud tensorflow image. '\n",
      "                         'While creating the Jupyter server resource, in the '\n",
      "                         '“Extra Packages” section under pip in the textbox '\n",
      "                         'write scipy. Below the textbox, the pip install '\n",
      "                         'scipy command will be displayed. This will ensure '\n",
      "                         'when the resource spins up, the scipy package will '\n",
      "                         'be automatically installed. This approach can also '\n",
      "                         'be followed for additional python packages.\\n'\n",
      "                         'Sumeet Lalla'},\n",
      "                {'question': 'How to upload kaggle data to Saturn Cloud?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem description: Uploading the data to saturn '\n",
      "                         'cloud from kaggle can be time saving, specially if '\n",
      "                         'the dataset is large.\\n'\n",
      "                         'You can just download to your local machine and then '\n",
      "                         'upload to a folder on saturn cloud, but there is a '\n",
      "                         'better solution that needs to be set once and you '\n",
      "                         'have access to all kaggle datasets in saturn cloud.\\n'\n",
      "                         'On your notebook run:\\n'\n",
      "                         '!pip install -q kaggle\\n'\n",
      "                         'Go to Kaggle website (you need to have an account '\n",
      "                         'for this):\\n'\n",
      "                         'Click on your profile image -> Account\\n'\n",
      "                         'Scroll down to the API box\\n'\n",
      "                         'Click on Create New API token\\n'\n",
      "                         'It will download a json file with the name '\n",
      "                         'kaggle.json store on your local computer. We need to '\n",
      "                         'upload this file in the .kaggle folder\\n'\n",
      "                         'On the notebook click on folder icon on the left '\n",
      "                         'upper corner\\n'\n",
      "                         'This will take you to the root folder\\n'\n",
      "                         'Click on the .kaggle folder\\n'\n",
      "                         'Once inside of the .kaggle folder upload the '\n",
      "                         'kaggle.json file that you downloaded\\n'\n",
      "                         'Run this command on your notebook:\\n'\n",
      "                         '!chmod 600 /home/jovyan/.kaggle/kaggle.json\\n'\n",
      "                         'Download the data using this command:\\n'\n",
      "                         '!kaggle datasets download -d '\n",
      "                         'agrigorev/dino-or-dragon\\n'\n",
      "                         'Create a folder to unzip your files:\\n'\n",
      "                         '!mkdir data\\n'\n",
      "                         'Unzip your files inside that folder\\n'\n",
      "                         '!unzip dino-or-dragon.zip -d data\\n'\n",
      "                         'Pastor Soto'},\n",
      "                {'question': 'How to install CUDA & cuDNN on Ubuntu 22.04',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'In order to run tensorflow with gpu on your local '\n",
      "                         'machine you’ll need to setup cuda and cudnn.\\n'\n",
      "                         'The process can be overwhelming. Here’s a simplified '\n",
      "                         'guide\\n'\n",
      "                         'Osman Ali'},\n",
      "                {'question': 'Error: (ValueError: Unable to load weights saved '\n",
      "                             'in HDF5 format into a subclassed Model which has '\n",
      "                             'not created its variables yet. Call the Model '\n",
      "                             'first, then load the weights.) when loading '\n",
      "                             'model.',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem description:\\n'\n",
      "                         'When loading saved model getting error: ValueError: '\n",
      "                         'Unable to load weights saved in HDF5 format into a '\n",
      "                         'subclassed Model which has not created its variables '\n",
      "                         'yet. Call the Model first, then load the weights.\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'Before loading model need to evaluate the model on '\n",
      "                         'input data: model.evaluate(train_ds)\\n'\n",
      "                         'Added by Vladimir Yesipov'},\n",
      "                {'question': 'Getting error when connect git on Saturn Cloud: '\n",
      "                             'permission denied',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem description:\\n'\n",
      "                         'When follow module 8.1b video to setup git in Saturn '\n",
      "                         'Cloud, run `ssh -T git@github.com` lead error '\n",
      "                         '`git@github.com: Permission denied (publickey).`\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'Alternative way, we can setup git in our Saturn '\n",
      "                         'Cloud env with generate SSH key in our Saturn Cloud '\n",
      "                         'and add it to our git account host. After it, we can '\n",
      "                         'access/manage our git through Saturn’s jupyter '\n",
      "                         'server. All steps detailed on this following '\n",
      "                         'tutorial: '\n",
      "                         'https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\\n'\n",
      "                         'Added by Ryan Pramana'},\n",
      "                {'question': 'Host key verification failed.',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem description:\\n'\n",
      "                         'Getting an error using <git clone '\n",
      "                         'git@github.com:alexeygrigorev/clothing-dataset-small.git>\\n'\n",
      "                         'The error:\\n'\n",
      "                         \"Cloning into 'clothing-dataset'...\\n\"\n",
      "                         'Host key verification failed.\\n'\n",
      "                         'fatal: Could not read from remote repository.\\n'\n",
      "                         'Please make sure you have the correct access rights\\n'\n",
      "                         'and the repository exists.\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'when cloning the repo, you can also chose https - '\n",
      "                         \"then it should work. This happens when you don't \"\n",
      "                         'have your ssh key configured.\\n'\n",
      "                         '<git clone '\n",
      "                         'https://github.com/alexeygrigorev/clothing-dataset-small.git>\\n'\n",
      "                         'Added by Gregory Morris'},\n",
      "                {'question': 'The same accuracy on epochs',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'The accuracy and the loss are both still the same or '\n",
      "                         'nearly the same while training.\\n'\n",
      "                         'Solution description\\n'\n",
      "                         \"In the homework, you should set class_mode='binary' \"\n",
      "                         'while reading the data.\\n'\n",
      "                         'Also, problem occurs when you choose the wrong '\n",
      "                         'optimizer, batch size, or learning rate\\n'\n",
      "                         'Added by Ekaterina Kutovaia'},\n",
      "                {'question': 'Model breaking after augmentation – high loss + '\n",
      "                             'bad accuracy',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem:\\n'\n",
      "                         'When resuming training after augmentation, the loss '\n",
      "                         'skyrockets (1000+ during first epoch) and accuracy '\n",
      "                         'settles around 0.5 – i.e. the model becomes as good '\n",
      "                         'as a random coin flip.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Check that the augmented ImageDataGenerator still '\n",
      "                         'includes the option “rescale” as specified in the '\n",
      "                         'preceding step.\\n'\n",
      "                         'Added by Konrad Mühlberg'},\n",
      "                {'question': 'Missing channel value error while reloading '\n",
      "                             'model:',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'While doing:\\n'\n",
      "                         'import tensorflow as tf\\n'\n",
      "                         'from tensorflow import keras\\n'\n",
      "                         'model = '\n",
      "                         \"tf.keras.models.load_model('model_saved.h5')\\n\"\n",
      "                         'If you get an error message like this:\\n'\n",
      "                         'ValueError: The channel dimension of the inputs '\n",
      "                         'should be defined. The input_shape received is '\n",
      "                         '(None, None, None, None), where axis -1 (0-based) is '\n",
      "                         'the channel dimension, which found to be `None`.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Saving a model (either yourself via model.save() or '\n",
      "                         'via checkpoint when save_weights_only = False) saves '\n",
      "                         'two things: The trained model weights (for example '\n",
      "                         'the best weights found during training) and the '\n",
      "                         'model architecture.  If the number of channels is '\n",
      "                         'not explicitly specified in the Input layer of the '\n",
      "                         'model, and is instead defined as a variable, the '\n",
      "                         'model architecture will not have the value in the '\n",
      "                         'variable stored. Therefore when the model is '\n",
      "                         'reloaded, it will complain about not knowing the '\n",
      "                         'number of channels. See the code below, in the first '\n",
      "                         'line, you need to specify number of channels '\n",
      "                         'explicitly:\\n'\n",
      "                         '# model architecture:\\n'\n",
      "                         'inputs = keras.Input(shape=(input_size, input_size, '\n",
      "                         '3))\\n'\n",
      "                         'base = base_model(inputs, training=False)\\n'\n",
      "                         'vectors = '\n",
      "                         'keras.layers.GlobalAveragePooling2D()(base)\\n'\n",
      "                         'inner = keras.layers.Dense(size_inner, '\n",
      "                         \"activation='relu')(vectors)\\n\"\n",
      "                         'drop = keras.layers.Dropout(droprate)(inner)\\n'\n",
      "                         'outputs = keras.layers.Dense(10)(drop)\\n'\n",
      "                         'model = keras.Model(inputs, outputs)\\n'\n",
      "                         '(Memoona Tahira)'},\n",
      "                {'question': 'How to unzip a folder with an image dataset and '\n",
      "                             'suppress output?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem:\\n'\n",
      "                         'A dataset for homework is in a zipped folder. If you '\n",
      "                         'unzip it within a jupyter notebook by means of ! '\n",
      "                         'unzip command, you’ll see a huge amount of output '\n",
      "                         'messages about unzipping of each image. So you need '\n",
      "                         'to suppress this output\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Execute the next cell:\\n'\n",
      "                         '%%capture\\n'\n",
      "                         '! unzip zipped_folder_name.zip -d '\n",
      "                         'destination_folder_name\\n'\n",
      "                         'Added by Alena Kniazeva\\n'\n",
      "                         'Inside a Jupyter Notebook:\\n'\n",
      "                         'import zipfile\\n'\n",
      "                         \"local_zip = 'data.zip'\\n\"\n",
      "                         \"zip_ref = zipfile.ZipFile(local_zip, 'r')\\n\"\n",
      "                         \"zip_ref.extractall('data')\\n\"\n",
      "                         'zip_ref.close()'},\n",
      "                {'question': 'How keras flow_from_directory know the names of '\n",
      "                             'classes in images?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem:\\n'\n",
      "                         'When we run train_gen.flow_from_directory() as in '\n",
      "                         'video 8.5, it finds images belonging to 10 classes. '\n",
      "                         'Does it understand the names of classes from the '\n",
      "                         'names of folders? Or, there is already something '\n",
      "                         'going on deep behind?\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'The name of class is the folder name\\n'\n",
      "                         'If you just create some random folder with the name '\n",
      "                         '\"xyz\", it will also be considered as a class!! The '\n",
      "                         'name itself is saying flow_from_directory\\n'\n",
      "                         'a clear explanation below:\\n'\n",
      "                         'https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\\n'\n",
      "                         'Added by Bhaskar Sarma'},\n",
      "                {'question': 'Error with scipy missing module in SaturnCloud',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem:\\n'\n",
      "                         'I created a new environment in SaturnCloud and chose '\n",
      "                         'the image corresponding to Saturn with Tensorflow, '\n",
      "                         'but when I tried to fit the model it showed an error '\n",
      "                         'about the missing module: scipy\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Install the module in a new cell: !pip install '\n",
      "                         'scipy\\n'\n",
      "                         'Restart the kernel and fit the model again\\n'\n",
      "                         'Added by Erick Calderin'},\n",
      "                {'question': 'How are numeric class labels determined in '\n",
      "                             'flow_from_directroy using binary class mode and '\n",
      "                             'what is meant by the single probability '\n",
      "                             'predicted by a binary Keras model:',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'The command to read folders in the dataset in the '\n",
      "                         'tensorflow source code is:\\n'\n",
      "                         'for subdir in sorted(os.listdir(directory)):\\n'\n",
      "                         '…\\n'\n",
      "                         'Reference: '\n",
      "                         'https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, '\n",
      "                         'line 563\\n'\n",
      "                         'This means folders will be read in alphabetical '\n",
      "                         'order. For example, in the case of a folder named '\n",
      "                         'dino, and another named dragon, dino will read first '\n",
      "                         'and will have class label 0, whereas dragon will be '\n",
      "                         'read in next and will have class label 1.\\n'\n",
      "                         'When a Keras model predicts binary labels, it will '\n",
      "                         'only return one value, and this is the probability '\n",
      "                         'of class 1 in case of sigmoid activation function in '\n",
      "                         'the last dense layer with 2 neurons. The probability '\n",
      "                         'of class 0 can be found out by:\\n'\n",
      "                         'prob(class(0)) = 1- prob(class(1))\\n'\n",
      "                         'In case of using from_logits to get results, you '\n",
      "                         'will get two values for each of the labels.\\n'\n",
      "                         'A prediction of 0.8 is saying the probability that '\n",
      "                         'the image has class label 1 (in this case dragon), '\n",
      "                         'is 0.8, and conversely we can infer the probability '\n",
      "                         'that the image has class label 0 is 0.2.\\n'\n",
      "                         '(Added by Memoona Tahira)'},\n",
      "                {'question': 'Does the actual values matter after predicting '\n",
      "                             'with a neural network or it should be treated as '\n",
      "                             'like hood of falling in a class?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': \"It's fine, some small changes are expected\\n\"\n",
      "                         'Alexey Grigorev'},\n",
      "                {'question': 'What if your accuracy and std training loss '\n",
      "                             'don’t match HW?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Problem:\\n'\n",
      "                         'I found running the wasp/bee model on my mac laptop '\n",
      "                         'had higher reported accuracy and lower std deviation '\n",
      "                         'than the HW answers. This may be because of the SGD '\n",
      "                         'optimizer. Running this on my mac printed a message '\n",
      "                         'about a new and legacy version that could be used.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Try running the same code on google collab or '\n",
      "                         'another way. The answers were closer for me on '\n",
      "                         'collab. Another tip is to change the runtime to use '\n",
      "                         'T4 and the model run’s faster than just CPU\\n'\n",
      "                         'Added by Quinn Avila'},\n",
      "                {'question': 'Using multi-threading for data generation in '\n",
      "                             '“model.fit()”',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'When running “model.fit(...)” an additional '\n",
      "                         'parameter “workers” can be specified for speeding up '\n",
      "                         'the data loading/generation. The default value is '\n",
      "                         '“1”. Try out which value between 1 and the cpu count '\n",
      "                         'on your system performs best.\\n'\n",
      "                         'https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\\n'\n",
      "                         'Added by Sylvia Schmitt'},\n",
      "                {'question': 'Reproducibility with TensorFlow using a seed '\n",
      "                             'point',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Reproducibility for training runs can be achieved '\n",
      "                         'following these instructions: \\n'\n",
      "                         'https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\\n'\n",
      "                         'seed = 1234\\n'\n",
      "                         'tf.keras.utils.set_random_seed(seed)\\n'\n",
      "                         'tf.config.experimental.enable_op_determinism()\\n'\n",
      "                         'This will work for a script, if this gets executed '\n",
      "                         'multiple times.\\n'\n",
      "                         'Added by Sylvia Schmitt'},\n",
      "                {'question': 'Can we use pytorch for this lesson/homework ?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Pytorch is also a deep learning framework that '\n",
      "                         'allows to do equivalent tasks as keras. Here is a '\n",
      "                         'tutorial to create a CNN from scratch using pytorch '\n",
      "                         ':\\n'\n",
      "                         'https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\\n'\n",
      "                         'The functions have similar goals. The syntax can be '\n",
      "                         'slightly different. For the lessons and the '\n",
      "                         'homework, we use keras, but one can feel free to '\n",
      "                         'make a pull request with the equivalent with pytorch '\n",
      "                         'for lessons and homework!\\n'\n",
      "                         'Mélanie Fouesnard'},\n",
      "                {'question': 'Keras model training fails with “Failed to find '\n",
      "                             'data adapter”',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'While training a Keras model you get the error '\n",
      "                         '“Failed to find data adapter that can handle input: '\n",
      "                         '<class '\n",
      "                         \"'keras.src.preprocessing.image.ImageDataGenerator'>, \"\n",
      "                         \"<class 'NoneType'>” you may have unintentionally \"\n",
      "                         'passed the image generator instead of the dataset to '\n",
      "                         'the model\\n'\n",
      "                         'train_gen = ImageDataGenerator(rescale=1./255)\\n'\n",
      "                         'train_ds = train_gen.flow_from_directory(…)\\n'\n",
      "                         'history_after_augmentation = model.fit(\\n'\n",
      "                         'train_gen, # this should be train_ds!!!\\n'\n",
      "                         'epochs=10,\\n'\n",
      "                         'validation_data=test_gen # this should be '\n",
      "                         'test_ds!!!\\n'\n",
      "                         ')\\n'\n",
      "                         'The fix is simple and probably obvious once pointed '\n",
      "                         'out, use the training and validation dataset '\n",
      "                         '(train_ds and val_ds) returned from '\n",
      "                         'flow_from_directory\\n'\n",
      "                         'Added by Tzvi Friedman'},\n",
      "                {'question': 'Running ‘nvidia-smi’ in a loop without using '\n",
      "                             '‘watch’',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'The command ‘nvidia-smi’ has a built-in function '\n",
      "                         'which will run it in subsequently updating it every '\n",
      "                         'N seconds without the need of using the command '\n",
      "                         '‘watch’.\\n'\n",
      "                         'nvidia-smi -l <N seconds>\\n'\n",
      "                         'The following command will run ‘nvidia-smi’ every 2 '\n",
      "                         'seconds until interrupted using CTRL+C.\\n'\n",
      "                         'nvidia-smi -l 2\\n'\n",
      "                         'Added by Sylvia Schmitt'},\n",
      "                {'question': 'Checking GPU and CPU utilization using ‘nvitop’',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'The Python package ‘’ is an interactive GPU process '\n",
      "                         'viewer similar to ‘htop’ for CPU.\\n'\n",
      "                         'https://pypi.org/project//\\n'\n",
      "                         'Image source: https://pypi.org/project//\\n'\n",
      "                         'Added by Sylvia Schmitt'},\n",
      "                {'question': 'Q: Where does the number of Conv2d layer’s '\n",
      "                             'params come from? Where does the number of '\n",
      "                             '“features” we get after the Flatten layer come '\n",
      "                             'from?',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'Let’s say we define our Conv2d layer like this:\\n'\n",
      "                         '>> tf.keras.layers.Conv2D(32, (3,3), '\n",
      "                         \"activation='relu', input_shape=(150, 150, 3))\\n\"\n",
      "                         'It means our input image is RGB (3 channels, 150 by '\n",
      "                         '150 pixels), kernel is 3x3 and number of filters '\n",
      "                         '(layer’s width) is 32.\\n'\n",
      "                         'If we check model.summary() we will get this:\\n'\n",
      "                         '_________________________________________________________________\\n'\n",
      "                         'Layer (type)                Output '\n",
      "                         'Shape              Param #\\n'\n",
      "                         '=================================================================\\n'\n",
      "                         'conv2d (Conv2D)             (None, 148, 148, '\n",
      "                         '32)      896\\n'\n",
      "                         'So where does 896 params come from? It’s computed '\n",
      "                         'like this:\\n'\n",
      "                         '>>> (3*3*3 +1) * 32\\n'\n",
      "                         '896\\n'\n",
      "                         '# 3x3 kernel, 3 channels RGB, +1 for bias, 32 '\n",
      "                         'filters\\n'\n",
      "                         'What about the number of “features” we get after the '\n",
      "                         'Flatten layer?\\n'\n",
      "                         'For our homework model.summary() for last '\n",
      "                         'MaxPooling2d and Flatten layers looked like this:\\n'\n",
      "                         '_________________________________________________________________\\n'\n",
      "                         'Layer (type)                Output '\n",
      "                         'Shape              Param #\\n'\n",
      "                         '=================================================================\\n'\n",
      "                         'max_pooling2d_3       (None, 7, 7, 128)         0\\n'\n",
      "                         'flatten (Flatten)           (None, '\n",
      "                         '6272)              0\\n'\n",
      "                         'So where do 6272 vectors come from? It’s computed '\n",
      "                         'like this:\\n'\n",
      "                         '>>> 7*7*128\\n'\n",
      "                         '6272\\n'\n",
      "                         '# 7x7 “image shape” after several convolutions and '\n",
      "                         'poolings, 128 filters\\n'\n",
      "                         'Added by Andrii Larkin'},\n",
      "                {'question': 'Sequential vs. Functional Model Modes in Keras '\n",
      "                             '(TF2)',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'It’s quite useful to understand that all types of '\n",
      "                         'models in the course are a plain stack of layers '\n",
      "                         'where each layer has exactly one input tensor and '\n",
      "                         'one output tensor (Sequential model TF page, '\n",
      "                         'Sequential class).\\n'\n",
      "                         'You can simply start from an “empty” model and add '\n",
      "                         'more and more layers in a sequential order.\\n'\n",
      "                         'This mode is called “Sequential Model API”  '\n",
      "                         '(easier)\\n'\n",
      "                         'In Alexey’s videos it is implemented as chained '\n",
      "                         'calls of different entities (“inputs”,“base”, '\n",
      "                         '“vectors”,  “outputs”) in a more advanced mode '\n",
      "                         '“Functional Model API”.\\n'\n",
      "                         'Maybe a more complicated way makes sense when you do '\n",
      "                         'Transfer Learning and want to separate “Base” model '\n",
      "                         'vs. rest, but in the HW you need to recreate the '\n",
      "                         'full model from scratch ⇒ I believe it is easier to '\n",
      "                         'work with a sequence of “similar” layers.\\n'\n",
      "                         'You can read more about it in this TF2 tutorial.\\n'\n",
      "                         'A really useful Sequential model example is shared '\n",
      "                         'in the Kaggle’s “Bee or Wasp” dataset folder with '\n",
      "                         'code: notebook\\n'\n",
      "                         'Added by Ivan Brigida\\n'\n",
      "                         'Fresh Run on Neural Nets\\n'\n",
      "                         'While correcting an error on neural net '\n",
      "                         'architecture, it is advised to do fresh run by '\n",
      "                         'restarting kernel, else the model learns on top of '\n",
      "                         'previous runs.\\n'\n",
      "                         'Added by Abhijit Chakraborty'},\n",
      "                {'question': 'Out of memory errors when running tensorflow',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'I found this code snippet fixed my OOM errors, as I '\n",
      "                         \"have an Nvidia GPU. Can't speak to OOM errors on \"\n",
      "                         'CPU, though.\\n'\n",
      "                         'https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\\n'\n",
      "                         '```\\n'\n",
      "                         'physical_devices = '\n",
      "                         \"tf.configlist_physical_devices('GPU')\\n\"\n",
      "                         'try:\\n'\n",
      "                         'tf.config.experimental.set_memory_growth(physical_devices[0],True)\\n'\n",
      "                         'except:\\n'\n",
      "                         '# Invalid device or cannot modify virtual devices '\n",
      "                         'once initialized.\\n'\n",
      "                         'pass\\n'\n",
      "                         '```'},\n",
      "                {'question': 'Model training very slow in google colab with T4 '\n",
      "                             'GPU',\n",
      "                 'section': '8. Neural Networks and Deep Learning',\n",
      "                 'text': 'When training the models, in the fit function, you '\n",
      "                         'can specify the number of workers/threads.\\n'\n",
      "                         'The number of threads apparently also works for '\n",
      "                         'GPUs, and came very handy in google colab for the T4 '\n",
      "                         'GPU, since it was very very slow, and workers '\n",
      "                         'default value is 1.\\n'\n",
      "                         'I changed the workers variable to 2560, following '\n",
      "                         'this thread in stackoverflow. I am using the free T4 '\n",
      "                         'GPU.  '\n",
      "                         '(https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\\n'\n",
      "                         'Added by Ibai Irastorza'},\n",
      "                {'question': 'Using image_dataset_from_directory instead of '\n",
      "                             'ImageDataGeneratorn for loading images',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'From the keras documentation:\\n'\n",
      "                         'Deprecated: '\n",
      "                         'tf.keras.preprocessing.image.ImageDataGenerator is '\n",
      "                         'not recommended for new code. Prefer loading images '\n",
      "                         'with tf.keras.utils.image_dataset_from_directory and '\n",
      "                         'transforming the output tf.data.Dataset with '\n",
      "                         'preprocessing layers. For more information, see the '\n",
      "                         'tutorials for loading images and augmenting images, '\n",
      "                         'as well as the preprocessing layer guide.\\n'\n",
      "                         'Hrithik Kumar Advani'},\n",
      "                {'question': 'How to get started with Week 9?',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'TODO'},\n",
      "                {'question': 'Where is the model for week 9?',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'The week 9 uses a link to github to fetch the '\n",
      "                         'models.\\n'\n",
      "                         'The original link was moved to here:\\n'\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/releases'},\n",
      "                {'question': 'Executing the command echo ${REMOTE_URI} returns '\n",
      "                             'nothing.',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Solution description\\n'\n",
      "                         'In the unit 9.6, Alexey ran the command echo '\n",
      "                         '${REMOTE_URI} which turned the URI address in the '\n",
      "                         'terminal. There workaround is to set a local '\n",
      "                         'variable (REMOTE_URI) and assign your URI address in '\n",
      "                         'the terminal and use it to login the registry, for '\n",
      "                         'instance, '\n",
      "                         'REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images '\n",
      "                         '(fake address). One caveat is that you will lose '\n",
      "                         'this variable once the session is terminated.\\n'\n",
      "                         'I also had the same problem on Ubuntu terminal. I '\n",
      "                         'executed the following two commands:\\n'\n",
      "                         '$ export '\n",
      "                         'REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\n'\n",
      "                         '$ echo $REMOTE_URI\\n'\n",
      "                         '111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\\n'\n",
      "                         'Note: 1. no curly brackets (e.g. echo ${REMOTE_URI}) '\n",
      "                         'needed unlike in video 9.6,\\n'\n",
      "                         '2. Replace REMOTE_URI with your URI\\n'\n",
      "                         '(Bhaskar Sarma)'},\n",
      "                {'question': 'Getting a syntax error while trying to get the '\n",
      "                             'password from aws-cli',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'The command aws ecr get-login --no-include-email '\n",
      "                         'returns an invalid choice error:\\n'\n",
      "                         'The solution is to use the following command '\n",
      "                         'instead:  aws ecr get-login-password\\n'\n",
      "                         'Could simplify the login process with, just replace '\n",
      "                         'the <ACCOUNT_NUMBER> and <REGION> with your values:\\n'\n",
      "                         'export PASSWORD=`aws ecr get-login-password`\\n'\n",
      "                         'docker login -u AWS -p $PASSWORD '\n",
      "                         '<ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\\n'\n",
      "                         'Added by Martin Uribe'},\n",
      "                {'question': 'Pass many parameters in the model at once',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'We can use the keras.models.Sequential() function to '\n",
      "                         'pass many parameters of the cnn at once.\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'Getting  ERROR [internal] load metadata for '\n",
      "                             'public.ecr.aws/lambda/python:3.8',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'This error is produced sometimes when building your '\n",
      "                         'docker image from the Amazon python base image.\\n'\n",
      "                         'Solution description: The following could solve the '\n",
      "                         'problem.\\n'\n",
      "                         'Update your docker desktop if you haven’t done so.\\n'\n",
      "                         'Or restart docker desktop and terminal and then '\n",
      "                         'build the image all over again.\\n'\n",
      "                         'Or if all else fails, first run the following '\n",
      "                         'command: DOCKER_BUILDKIT=0  docker build .  then '\n",
      "                         'build your image.\\n'\n",
      "                         '(optional) Added by Odimegwu David'},\n",
      "                {'question': \"Problem: 'ls' is not recognized as an internal \"\n",
      "                             'or external command, operable program or batch '\n",
      "                             'file.',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'When trying to run the command  !ls -lh in windows '\n",
      "                         'jupyter notebook  , I was getting an error message '\n",
      "                         \"that says “'ls' is not recognized as an internal or \"\n",
      "                         'external command,operable program or batch file.\\n'\n",
      "                         'Solution description :\\n'\n",
      "                         'Instead of !ls -lh , you can use this command !dir , '\n",
      "                         'and you will get similar output\\n'\n",
      "                         'Asia Saeed'},\n",
      "                {'question': 'ImportError: generic_type: type '\n",
      "                             '\"InterpreterWrapper\" is already registered!',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'When I run   import tflite_runtime.interpreter as '\n",
      "                         'tflite , I get an error message says “ImportError: '\n",
      "                         'generic_type: type \"InterpreterWrapper\" is already '\n",
      "                         'registered!”\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'This error occurs when you import both tensorflow  '\n",
      "                         'and tflite_runtime.interpreter  “import tensorflow '\n",
      "                         'as tf” and “import tflite_runtime.interpreter as '\n",
      "                         'tflite” in the same notebook.  To fix the issue, '\n",
      "                         'restart the kernel and import only '\n",
      "                         'tflite_runtime.interpreter \" import '\n",
      "                         'tflite_runtime.interpreter as tflite\".\\n'\n",
      "                         'Asia Saeed'},\n",
      "                {'question': 'Windows version might not be up-to-date',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Problem description:\\n'\n",
      "                         'In command line try to do $ docker build -t '\n",
      "                         'dino_dragon\\n'\n",
      "                         'got this Using default tag: latest\\n'\n",
      "                         '[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] '\n",
      "                         'Windows version might not be up-to-date: The system '\n",
      "                         'cannot find the file specified.\\n'\n",
      "                         'error during connect: This error may indicate that '\n",
      "                         'the docker daemon is not running.: Post\\n'\n",
      "                         '.\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'You need to make sure that Docker is not stopped by '\n",
      "                         'a third-party program.\\n'\n",
      "                         'Andrei Ilin'},\n",
      "                {'question': 'WARNING: You are using pip version 22.0.4; '\n",
      "                             'however, version 22.3.1 is available',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'When running docker build -t dino-dragon-model it '\n",
      "                         'returns the above error\\n'\n",
      "                         'The most common source of this error in this week is '\n",
      "                         'because Alex video shows a version of the wheel with '\n",
      "                         'python 8, we need to find a wheel with the version '\n",
      "                         'that we are working on. In this case python 9. '\n",
      "                         'Another common error is to copy the link, this will '\n",
      "                         'also produce the same error, we need to download the '\n",
      "                         'raw format:\\n'\n",
      "                         'https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\\n'\n",
      "                         'Pastor Soto'},\n",
      "                {'question': 'How to do AWS configure after installing awscli',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Problem description:\\n'\n",
      "                         'In video 9.6, after installing aswcli, we should '\n",
      "                         'configure it with aws configure . There it asks for '\n",
      "                         'Access Key ID, Secret Access Key, Default Region '\n",
      "                         'Name and also Default output format. What we should '\n",
      "                         'put for Default output format? Leaving it as  None '\n",
      "                         'is okay?\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'Yes, in my I case I left everything as the provided '\n",
      "                         'defaults (except, obviously, the Access key and the '\n",
      "                         'secret access key)\\n'\n",
      "                         'Added by Bhaskar Sarma'},\n",
      "                {'question': 'Object of type float32 is not JSON serializable',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Problem:\\n'\n",
      "                         'While passing local testing of the lambda function '\n",
      "                         'without issues, trying to test the same input with a '\n",
      "                         'running docker instance results in an error message '\n",
      "                         'like\\n'\n",
      "                         '{‘errorMessage’: ‘Unable to marshal response: Object '\n",
      "                         'of type float32 is not JSON serializable’, '\n",
      "                         '‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: '\n",
      "                         '‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, '\n",
      "                         '‘stackTrace’: []}\\n'\n",
      "                         'This happens when a model (in this case the dino vs '\n",
      "                         'dragon model) returns individual estimation values '\n",
      "                         'as numpy float32 values (arrays). They need to be '\n",
      "                         'converted individually to base-Python floats in '\n",
      "                         'order to become “serializable”.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'In my particular case, I set up the dino vs dragon '\n",
      "                         'model in such a way as to return a label + predicted '\n",
      "                         'probability for each class as follows (below is a '\n",
      "                         'two-line extract of function predict() in the '\n",
      "                         'lambda_function.py):\\n'\n",
      "                         'preds = [interpreter.get_tensor(output_index)[0][0], '\n",
      "                         '\\\\\\n'\n",
      "                         '1-interpreter.get_tensor(output_index)[0][0]]\\n'\n",
      "                         'In which case the above described solution will look '\n",
      "                         'like this:\\n'\n",
      "                         'preds = '\n",
      "                         '[float(interpreter.get_tensor(output_index)[0][0]), '\n",
      "                         '\\\\\\n'\n",
      "                         'float(1-interpreter.get_tensor(output_index)[0][0])]\\n'\n",
      "                         'The rest can be made work by following the chapter 9 '\n",
      "                         '(and/or chapter 5!) lecture videos step by step.\\n'\n",
      "                         'Added by Konrad Muehlberg'},\n",
      "                {'question': 'Error with the line '\n",
      "                             '“interpreter.set_tensor(input_index, X”)',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'I had this error when running the command line : '\n",
      "                         'interpreter.set_tensor(input_index, x) that can be '\n",
      "                         'seen in the video 9.3 around 12 minutes.\\n'\n",
      "                         'ValueError: Cannot set tensor: Got value of type '\n",
      "                         'UINT8 but expected type FLOAT32 for input 0, name: '\n",
      "                         'serving_default_conv2d_input:0\\n'\n",
      "                         'This is because the X is an int but a float is '\n",
      "                         'expected.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'I found this solution from this question here '\n",
      "                         'https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float '\n",
      "                         ':\\n'\n",
      "                         '# Need to convert to float32 before set_tensor\\n'\n",
      "                         'X = np.float32(X)\\n'\n",
      "                         'Then, it works. I work with tensorflow 2.15.0, maybe '\n",
      "                         'the fact that this version is more recent involves '\n",
      "                         'this change ?\\n'\n",
      "                         'Added by Mélanie Fouesnard'},\n",
      "                {'question': 'How to easily get file size in powershell '\n",
      "                             'terminal ?',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'To check your file size using the powershell '\n",
      "                         'terminal, you can do the following command lines:\\n'\n",
      "                         '$File = Get-Item -Path path_to_file\\n'\n",
      "                         '$FileSize = (Get-Item -Path $FilePath).Length\\n'\n",
      "                         'Now you can check the size of your file, for example '\n",
      "                         'in MB:\\n'\n",
      "                         'Write-host \"MB\":($FileSize/1MB)\\n'\n",
      "                         'Source: '\n",
      "                         'https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\\n'\n",
      "                         'Added by Mélanie Fouesnard'},\n",
      "                {'question': 'How do Lambda container images work?',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'I wanted to understand how lambda container images '\n",
      "                         'work in depth and how lambda functions are '\n",
      "                         'initialized, for this reason, I found the following '\n",
      "                         'documentation\\n'\n",
      "                         'https://docs.aws.amazon.com/lambda/latest/dg/images-create.html\\n'\n",
      "                         'https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\\n'\n",
      "                         'Added by Alejandro aponte'},\n",
      "                {'question': 'How to use AWS Serverless Framework to deploy on '\n",
      "                             'AWS Lambda and expose it as REST API through '\n",
      "                             'APIGatewayService?',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'The docker image for aws lambda can be created and '\n",
      "                         'pushed to aws ecr and the same can be exposed as a '\n",
      "                         'REST API through APIGatewayService in a single go '\n",
      "                         'using AWS Serverless Framework. Refer the below '\n",
      "                         'article for a detailed walkthrough.\\n'\n",
      "                         'https://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\\n'\n",
      "                         'Added by Sumeet Lalla'},\n",
      "                {'question': 'Error building docker image on M1 Mac',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Problem:\\n'\n",
      "                         'While trying to build docker image in Section 9.5 '\n",
      "                         'with the command:\\n'\n",
      "                         'docker build -t clothing-model .\\n'\n",
      "                         'It throws a pip install error for the tflite runtime '\n",
      "                         'whl\\n'\n",
      "                         'ERROR: failed to solve: process \"/bin/sh -c pip '\n",
      "                         'install '\n",
      "                         'https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" '\n",
      "                         'did not complete successfully: exit code: 1\\n'\n",
      "                         'Try to use this link: '\n",
      "                         'https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\\n'\n",
      "                         'If the link above does not work:\\n'\n",
      "                         'The problem is because of the arm architecture of '\n",
      "                         'the M1. You will need to run the code on a PC or '\n",
      "                         'Ubuntu OS.\\n'\n",
      "                         'Or try the code bellow.\\n'\n",
      "                         'Added by Dashel Ruiz Perez\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'To build the Docker image, use the command:\\n'\n",
      "                         'docker build --platform linux/amd64 -t '\n",
      "                         'clothing-model .\\n'\n",
      "                         'To run the built image, use the command:\\n'\n",
      "                         'docker run -it --rm -p 8080:8080 --platform '\n",
      "                         'linux/amd64 clothing-model:latest\\n'\n",
      "                         'Added by Daniel Egbo'},\n",
      "                {'question': 'Error invoking API Gateway deploy API locally',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Problem: Trying to test API gateway in 9.7 - API '\n",
      "                         'Gateway: Exposing the Lambda Function, running: $ '\n",
      "                         'python test.py\\n'\n",
      "                         'With error message:\\n'\n",
      "                         \"{'message': 'Missing Authentication Token'}\\n\"\n",
      "                         'Solution:\\n'\n",
      "                         'Need to get the deployed API URL for the specific '\n",
      "                         'path you are invoking. Example:\\n'\n",
      "                         'https://<random '\n",
      "                         'string>.execute-api.us-east-2.amazonaws.com/test/predict\\n'\n",
      "                         'Added by Andrew Katoch'},\n",
      "                {'question': 'Error: Could not find a version that satisfies '\n",
      "                             'the requirement tflite_runtime (from '\n",
      "                             'versions:none)',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Problem: When trying to install tflite_runtime with\\n'\n",
      "                         '!pip install --extra-index-url '\n",
      "                         'https://google-coral.github.io/py-repo/ '\n",
      "                         'tflite_runtime\\n'\n",
      "                         'one gets an error message above.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'fflite_runtime is only available for the os-python '\n",
      "                         'version combinations that can be found here: '\n",
      "                         'https://google-coral.github.io/py-repo/tflite-runtime/\\n'\n",
      "                         'your combination must be missing here\\n'\n",
      "                         'you can see if any of these work for you '\n",
      "                         'https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\n'\n",
      "                         'and install the needed one using pip\\n'\n",
      "                         'eg\\n'\n",
      "                         'pip install '\n",
      "                         'https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\n'\n",
      "                         'as it is done in the lectures code:\\n'\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\\n'\n",
      "                         'Alternatively, use a virtual machine (with VM '\n",
      "                         'VirtualBox, for example) with a Linux system. The '\n",
      "                         'other way is to run a code at a virtual machine '\n",
      "                         'within cloud service, for example you can use Vertex '\n",
      "                         'AI Workbench at GCP (notebooks and terminals are '\n",
      "                         'provided there, so all tasks may be performed).\\n'\n",
      "                         'Added by Alena Kniazeva, modified by Alex Litvinov'},\n",
      "                {'question': 'Docker run error',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'docker: Error response from daemon: mkdir '\n",
      "                         '/var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: '\n",
      "                         'read-only file system.\\n'\n",
      "                         'You need to restart the docker services to get rid '\n",
      "                         'of the above error\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'Save Docker Image to local machine and view '\n",
      "                             'contents',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'The docker image can be saved/exported to tar format '\n",
      "                         'in local machine using the below command:\\n'\n",
      "                         'docker image save <image-name> -o '\n",
      "                         '<name-of-tar-file.tar>\\n'\n",
      "                         'The individual layers of the docker image for the '\n",
      "                         'filesystem content can be viewed by extracting the '\n",
      "                         'layer.tar present in the <name-of-tar-file.tar> '\n",
      "                         'created from above.\\n'\n",
      "                         'Sumeet Lalla'},\n",
      "                {'question': 'Jupyter notebook not seeing package',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'On vscode running jupyter notebook. After I ‘pip '\n",
      "                         'install pillow’, my notebook did not recognize using '\n",
      "                         'the import for example from PIL import image. After '\n",
      "                         'restarting the jupyter notebook the imports worked.\\n'\n",
      "                         'Quinn Avila'},\n",
      "                {'question': 'Running out of space for AWS instance.',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Due to experimenting back and forth so much without '\n",
      "                         'care for storage, I just ran out of it on my 30-GB '\n",
      "                         'AWS instance. It turns out that deleting docker '\n",
      "                         'images does not actually free up any space as you '\n",
      "                         'might expect. After removing images, you also need '\n",
      "                         'to run docker system prune'},\n",
      "                {'question': 'Using Tensorflow 2.15 for AWS deployment',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Using the 2.14 version with python 3.11 works fine.\\n'\n",
      "                         'In case it doesn’t work, I tried with tensorflow '\n",
      "                         '2.4.4 whl, however, make sure to run it on top of '\n",
      "                         'supported python versions like 3.8, else there will '\n",
      "                         'be issues installing tf==2.4.4\\n'\n",
      "                         'Added by Abhijit Chakraborty'},\n",
      "                {'question': 'Command aws ecr get-login --no-include-email '\n",
      "                             'returns “aws: error: argument operation: Invalid '\n",
      "                             'choice…”',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'see here'},\n",
      "                {'question': 'What IAM permission policy is needed to complete '\n",
      "                             'Week 9: Serverless?',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Sign in to the AWS Console: Log in to the AWS '\n",
      "                         'Console.\\n'\n",
      "                         'Navigate to IAM: Go to the IAM service by clicking '\n",
      "                         'on \"Services\" in the top left corner and selecting '\n",
      "                         '\"IAM\" under the \"Security, Identity, & Compliance\" '\n",
      "                         'section.\\n'\n",
      "                         'Create a new policy: In the left navigation pane, '\n",
      "                         'select \"Policies\" and click on \"Create policy.\"\\n'\n",
      "                         'Select the service and actions:\\n'\n",
      "                         'Click on \"JSON\" and copy and paste the JSON policy '\n",
      "                         'you provided earlier for the specific ECR actions.\\n'\n",
      "                         'Review and create the policy:\\n'\n",
      "                         'Click on \"Review policy.\"\\n'\n",
      "                         'Provide a name and description for the policy.\\n'\n",
      "                         'Click on \"Create policy.\"\\n'\n",
      "                         'JSON policy:\\n'\n",
      "                         '{\\n'\n",
      "                         '\"Version\": \"2012-10-17\",\\n'\n",
      "                         '\"Statement\": [\\n'\n",
      "                         '{\\n'\n",
      "                         '\"Sid\": \"VisualEditor0\",\\n'\n",
      "                         '\"Effect\": \"Allow\",\\n'\n",
      "                         '\"Action\": [\\n'\n",
      "                         '\"ecr:CreateRepository\",\\n'\n",
      "                         '\"ecr:GetAuthorizationToken\",\\n'\n",
      "                         '\"ecr:BatchCheckLayerAvailability\",\\n'\n",
      "                         '\"ecr:BatchGetImage\",\\n'\n",
      "                         '\"ecr:InitiateLayerUpload\",\\n'\n",
      "                         '\"ecr:UploadLayerPart\",\\n'\n",
      "                         '\"ecr:CompleteLayerUpload\",\\n'\n",
      "                         '\"ecr:PutImage\"\\n'\n",
      "                         '],\\n'\n",
      "                         '\"Resource\": \"*\"\\n'\n",
      "                         '}\\n'\n",
      "                         ']\\n'\n",
      "                         '}\\n'\n",
      "                         'Added by: Daniel Muñoz-Viveros\\n'\n",
      "                         'ERROR: failed to solve: '\n",
      "                         'public.ecr.aws/lambda/python:3.10: error getting '\n",
      "                         'credentials - err: exec: '\n",
      "                         '\"docker-credential-desktop.exe\": executable file not '\n",
      "                         'found in $PATH, out: ``\\n'\n",
      "                         '(WSL2 system)\\n'\n",
      "                         'Solved: Delete the file ~/.docker/config.json\\n'\n",
      "                         'Yishan Zhan'},\n",
      "                {'question': 'Docker Temporary failure in name resolution',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Add the next lines to vim /etc/docker/daemon.json\\n'\n",
      "                         '{\\n'\n",
      "                         '\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\\n'\n",
      "                         '}\\n'\n",
      "                         'Then, restart docker:  sudo service docker restart\\n'\n",
      "                         'Ibai Irastorza'},\n",
      "                {'question': 'Keras model *.h5 doesn’t load. Error: '\n",
      "                             'weight_decay is not a valid argument, kwargs '\n",
      "                             'should be empty  for '\n",
      "                             '`optimizer_experimental.Optimizer`',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Solution: add compile = False to the load_model '\n",
      "                         'function\\n'\n",
      "                         \"keras.models.load_model('model_name.h5', \"\n",
      "                         'compile=False)\\n'\n",
      "                         'Nadia Paz'},\n",
      "                {'question': 'How to test AWS Lambda + Docker locally?',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'This deployment setup can be tested locally using '\n",
      "                         'AWS RIE (runtime interface emulator).\\n'\n",
      "                         'Basically, if your Docker image was built upon base '\n",
      "                         'AWS Lambda image (FROM '\n",
      "                         'public.ecr.aws/lambda/python:3.10) - just use '\n",
      "                         'certain ports for “docker run” and a certain '\n",
      "                         '“localhost link” for testing:\\n'\n",
      "                         'docker run -it --rm -p 9000:8080 name\\n'\n",
      "                         'This command runs the image as a container and '\n",
      "                         'starts up an endpoint locally at:\\n'\n",
      "                         'localhost:9000/2015-03-31/functions/function/invocations\\n'\n",
      "                         'Post an event to the following endpoint using a curl '\n",
      "                         'command:\\n'\n",
      "                         'curl -XPOST '\n",
      "                         '\"http://localhost:9000/2015-03-31/functions/function/invocations\" '\n",
      "                         \"-d '{}'\\n\"\n",
      "                         'Examples of curl testing:\\n'\n",
      "                         '* windows testing:\\n'\n",
      "                         'curl -XPOST '\n",
      "                         '\"http://localhost:9000/2015-03-31/functions/function/invocations\" '\n",
      "                         '-d \"{\\\\\"url\\\\\": '\n",
      "                         '\\\\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\\\"}\"\\n'\n",
      "                         '* unix testing:\\n'\n",
      "                         'curl -XPOST '\n",
      "                         '\"http://localhost:9000/2015-03-31/functions/function/invocations\" '\n",
      "                         '-d \\'{\"url\": '\n",
      "                         '\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\n'\n",
      "                         'If during testing you encounter an error like this:\\n'\n",
      "                         '# {\"errorMessage\": \"Unable to marshal response: '\n",
      "                         'Object of type float32 is not JSON serializable\", '\n",
      "                         '\"errorType\": \"Runtime.MarshalError\", \"requestId\": '\n",
      "                         '\"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", '\n",
      "                         '\"stackTrace\": []}\\n'\n",
      "                         'just turn your response at lambda_handler() to '\n",
      "                         'string - str(result).\\n'\n",
      "                         'Added by Andrii Larkin'},\n",
      "                {'question': '\"Unable to import module \\'lambda_function\\': No '\n",
      "                             'module named \\'tensorflow\\'\" when run python '\n",
      "                             'test.py',\n",
      "                 'section': '9. Serverless Deep Learning',\n",
      "                 'text': 'Make sure all codes in test.py dont have any '\n",
      "                         'dependencies with tensorflow library. One of most '\n",
      "                         'common reason that lead the this error is tflite '\n",
      "                         'still imported from tensorflow. Change import '\n",
      "                         'tensorflow.lite as tflite to import '\n",
      "                         'tflite_runtime.interpreter as tflite\\n'\n",
      "                         'Added by Ryan Pramana'},\n",
      "                {'question': 'Install Docker (udocker) in Google Colab',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'I’ve tried to do everything in Google Colab. Here is '\n",
      "                         'a way to work with Docker in Google Colab:\\n'\n",
      "                         'https://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\\n'\n",
      "                         '\\uec03%%shell\\n'\n",
      "                         'pip install udocker\\n'\n",
      "                         'udocker --allow-root install\\n'\n",
      "                         '\\uec02!udocker --allow-root run hello-world\\n'\n",
      "                         'Added by Ivan Brigida\\n'\n",
      "                         'Lambda API Gateway errors:\\n'\n",
      "                         \"`Authorization header requires 'Credential' \"\n",
      "                         \"parameter. Authorization header requires 'Signature' \"\n",
      "                         'parameter. Authorization header requires '\n",
      "                         \"'SignedHeaders' parameter. Authorization header \"\n",
      "                         \"requires existence of either a 'X-Amz-Date' or a \"\n",
      "                         \"'Date' header.`\\n\"\n",
      "                         '`Missing Authentication Token`\\n'\n",
      "                         'import boto3\\n'\n",
      "                         \"client = boto3.client('apigateway')\\n\"\n",
      "                         'response = client.test_invoke_method(\\n'\n",
      "                         \"restApiId='your_rest_api_id',\\n\"\n",
      "                         \"resourceId='your_resource_id',\\n\"\n",
      "                         \"httpMethod='POST',\\n\"\n",
      "                         \"pathWithQueryString='/test/predict', #depend how you \"\n",
      "                         'set up the api\\n'\n",
      "                         'body=\\'{\"url\": '\n",
      "                         '\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}\\'\\n'\n",
      "                         ')\\n'\n",
      "                         \"print(response['body'])\\n\"\n",
      "                         'Yishan Zhan\\n'\n",
      "                         'Unable to run pip install tflite_runtime from github '\n",
      "                         'wheel links?\\n'\n",
      "                         'To overcome this issue, you can download the whl '\n",
      "                         'file to your local project folder and in the Docker '\n",
      "                         'file add the following lines:\\n'\n",
      "                         'COPY <file-name> .\\n'\n",
      "                         'RUN pip install <file-name>\\n'\n",
      "                         'Abhijit Chakraborty'},\n",
      "                {'question': 'How to get started with Week 10?',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'TODO'},\n",
      "                {'question': 'How to install Tensorflow in Ubuntu WSL2',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Running a CNN on your CPU can take a long time and '\n",
      "                         'once you’ve run out of free time on some cloud '\n",
      "                         'providers, it’s time to pay up. Both can be tackled '\n",
      "                         'by installing tensorflow with CUDA support on your '\n",
      "                         'local machine if you have the right hardware.\\n'\n",
      "                         'I was able to get it working by using the following '\n",
      "                         'resources:\\n'\n",
      "                         'CUDA on WSL :: CUDA Toolkit Documentation '\n",
      "                         '(nvidia.com)\\n'\n",
      "                         'Install TensorFlow with pip\\n'\n",
      "                         'Start Locally | PyTorch\\n'\n",
      "                         'I included the link to PyTorch so that you can get '\n",
      "                         'that one installed and working too while everything '\n",
      "                         'is fresh on your mind. Just select your options, and '\n",
      "                         'for Computer Platform, I chose CUDA 11.7 and it '\n",
      "                         'worked for me.\\n'\n",
      "                         'Added by Martin Uribe'},\n",
      "                {'question': 'Getting: Allocator ran out of memory errors?',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'If you are running tensorflow on your own machine '\n",
      "                         'and you start getting the following errors:\\n'\n",
      "                         'Allocator (GPU_0_bfc) ran out of memory trying to '\n",
      "                         'allocate 6.88GiB with freed_by_count=0. The caller '\n",
      "                         'indicates that this is not a failure, but this may '\n",
      "                         'mean that there could be performance gains if more '\n",
      "                         'memory were available.\\n'\n",
      "                         'Try adding this code in a cell at the beginning of '\n",
      "                         'your notebook:\\n'\n",
      "                         'config = tf.compat.v1.ConfigProto()\\n'\n",
      "                         'config.gpu_options.allow_growth = True\\n'\n",
      "                         'session = tf.compat.v1.Session(config=config)\\n'\n",
      "                         'After doing this most of my issues went away. I say '\n",
      "                         'most because there was one instance when I still got '\n",
      "                         'the error once more, but only during one epoch. I '\n",
      "                         'ran the code again, right after it finished, and I '\n",
      "                         'never saw the error again.\\n'\n",
      "                         'Added by Martin Uribe'},\n",
      "                {'question': 'Problem with recent version of protobuf',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'In session 10.3, when creating the virtual '\n",
      "                         'environment with pipenv and trying to run the script '\n",
      "                         'gateway.py, you might get this error:\\n'\n",
      "                         'TypeError: Descriptors cannot not be created '\n",
      "                         'directly.\\n'\n",
      "                         'If this call came from a _pb2.py file, your '\n",
      "                         'generated code is out of date and must be '\n",
      "                         'regenerated with protoc >= 3.19.0.\\n'\n",
      "                         'If you cannot immediately regenerate your protos, '\n",
      "                         'some other possible workarounds are:\\n'\n",
      "                         '1. Downgrade the protobuf package to 3.20.x or '\n",
      "                         'lower.\\n'\n",
      "                         '2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python '\n",
      "                         '(but this will use pure-Python parsing and will be '\n",
      "                         'much slower).\\n'\n",
      "                         'More information: '\n",
      "                         'https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\\n'\n",
      "                         'This will happen if your version of protobuf is one '\n",
      "                         'of the newer ones. As a workaround, you can fix the '\n",
      "                         'protobuf version to an older one. In my case I got '\n",
      "                         'around the issue by creating the environment with:\\n'\n",
      "                         'pipenv install --python 3.9.13 requests '\n",
      "                         'grpcio==1.42.0 flask gunicorn \\\\\\n'\n",
      "                         'keras-image-helper tensorflow-protobuf==2.7.0 '\n",
      "                         'protobuf==3.19.6\\n'\n",
      "                         'Added by Ángel de Vicente'},\n",
      "                {'question': 'WSL Cannot Connect To Docker Daemon',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Due to the uncertainties associated with machines, '\n",
      "                         'sometimes you can get the error message like this '\n",
      "                         'when you try to run a docker command:\\n'\n",
      "                         '”Cannot connect to the Docker daemon at '\n",
      "                         'unix:///var/run/docker.sock. Is the docker daemon '\n",
      "                         'running?”\\n'\n",
      "                         'Solution: The solution is simple. The Docker Desktop '\n",
      "                         'might no longer be connecting to the WSL Linux '\n",
      "                         'distro. What you need to do is go to your Docker '\n",
      "                         'Desktop setting and then click on resources. Under '\n",
      "                         'resources, click on WSL Integration. You will get a '\n",
      "                         'tab like the image below:\\n'\n",
      "                         'Just enable additional distros. That’s all. Even if '\n",
      "                         'the additional distro is the same as the default WSL '\n",
      "                         'distro.\\n'\n",
      "                         'Odimegwu David'},\n",
      "                {'question': 'HPA instance doesn’t run properly',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'In case the HPA instance does not run correctly even '\n",
      "                         'after installing the latest version of Metrics '\n",
      "                         'Server from the components.yaml manifest with:\\n'\n",
      "                         '>>kubectl apply -f '\n",
      "                         'https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\n'\n",
      "                         'And the targets still appear as <unknown>\\n'\n",
      "                         'Run >>kubectl edit deploy -n kube-system '\n",
      "                         'metrics-server\\n'\n",
      "                         'And search for this line:\\n'\n",
      "                         'args:\\n'\n",
      "                         '- '\n",
      "                         '--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\n'\n",
      "                         'Add this line in the middle:  - '\n",
      "                         '--kubelet-insecure-tls\\n'\n",
      "                         'So that it stays like this:\\n'\n",
      "                         'args:\\n'\n",
      "                         '- --kubelet-insecure-tls\\n'\n",
      "                         '- '\n",
      "                         '--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\\n'\n",
      "                         'Save and run again >>kubectl get hpa\\n'\n",
      "                         'Added by Marilina Orihuela'},\n",
      "                {'question': 'HPA instance doesn’t run properly (easier '\n",
      "                             'solution)',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'In case the HPA instance does not run correctly even '\n",
      "                         'after installing the latest version of Metrics '\n",
      "                         'Server from the components.yaml manifest with:\\n'\n",
      "                         '>>kubectl apply -f '\n",
      "                         'https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\\n'\n",
      "                         'And the targets still appear as <unknown>\\n'\n",
      "                         'Run the following command:\\n'\n",
      "                         'kubectl apply -f '\n",
      "                         'https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\\n'\n",
      "                         'Which uses a metrics server deployment file already '\n",
      "                         'embedding the - --kubelet-insecure-tls option.\\n'\n",
      "                         'Added by Giovanni Pecoraro'},\n",
      "                {'question': 'Could not install packages due to an OSError: '\n",
      "                             '[WinError 5] Access is denied',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'When I run pip install grpcio==1.42.0 '\n",
      "                         'tensorflow-serving-api==2.7.0 to install the '\n",
      "                         'libraries in windows machine,  I was getting the '\n",
      "                         'below error :\\n'\n",
      "                         'ERROR: Could not install packages due to an OSError: '\n",
      "                         '[WinError 5] Access is denied: '\n",
      "                         \"'C:\\\\\\\\Users\\\\\\\\Asia\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\google\\\\\\\\protobuf\\\\\\\\internal\\\\\\\\_api_implementation.cp39-win_amd64.pyd'\\n\"\n",
      "                         'Consider using the `--user` option or check the '\n",
      "                         'permissions.\\n'\n",
      "                         'Solution description :\\n'\n",
      "                         'I was able to install the libraries using below '\n",
      "                         'command:\\n'\n",
      "                         'pip --user install grpcio==1.42.0 '\n",
      "                         'tensorflow-serving-api==2.7.0\\n'\n",
      "                         'Asia Saeed'},\n",
      "                {'question': 'TypeError: Descriptors cannot not be created '\n",
      "                             'directly.',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'I was getting the below error message when I run '\n",
      "                         'gateway.py after modifying the code & creating '\n",
      "                         'virtual environment in  video 10.3 :\\n'\n",
      "                         'File '\n",
      "                         '\"C:\\\\Users\\\\Asia\\\\Data_Science_Code\\\\Zoompcamp\\\\Kubernetes\\\\gat.py\", '\n",
      "                         'line 9, in <module>\\n'\n",
      "                         'from tensorflow_serving.apis import predict_pb2\\n'\n",
      "                         'File '\n",
      "                         '\"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow_serving\\\\apis\\\\predict_pb2.py\", '\n",
      "                         'line 14, in <module>\\n'\n",
      "                         'from tensorflow.core.framework import tensor_pb2 as '\n",
      "                         'tensorflow_dot_core_dot_framework_dot_tensor__pb2\\n'\n",
      "                         'File '\n",
      "                         '\"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_pb2.py\", '\n",
      "                         'line 14, in <module>\\n'\n",
      "                         'from tensorflow.core.framework import '\n",
      "                         'resource_handle_pb2 as '\n",
      "                         'tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\\n'\n",
      "                         'File '\n",
      "                         '\"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\resource_handle_pb2.py\", '\n",
      "                         'line 14, in <module>\\n'\n",
      "                         'from tensorflow.core.framework import '\n",
      "                         'tensor_shape_pb2 as '\n",
      "                         'tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\\n'\n",
      "                         'File '\n",
      "                         '\"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\tensorflow\\\\core\\\\framework\\\\tensor_shape_pb2.py\", '\n",
      "                         'line 36, in <module>\\n'\n",
      "                         '_descriptor.FieldDescriptor(\\n'\n",
      "                         'File '\n",
      "                         '\"C:\\\\Users\\\\Asia\\\\.virtualenvs\\\\Kubernetes-Ge6Ts1D5\\\\lib\\\\site-packages\\\\google\\\\protobuf\\\\descriptor.py\", '\n",
      "                         'line 560, in __new__\\n'\n",
      "                         '_message.Message._CheckCalledFromGeneratedFile()\\n'\n",
      "                         'TypeError: Descriptors cannot not be created '\n",
      "                         'directly.\\n'\n",
      "                         'If this call came from a _pb2.py file, your '\n",
      "                         'generated code is out of date and must be '\n",
      "                         'regenerated with protoc >= 3.19.0.\\n'\n",
      "                         'If you cannot immediately regenerate your protos, '\n",
      "                         'some other possible workarounds are:\\n'\n",
      "                         '1. Downgrade the protobuf package to 3.20.x or '\n",
      "                         'lower.\\n'\n",
      "                         '2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python '\n",
      "                         '(but this will use pure-Python parsing and will be '\n",
      "                         'much slower).\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'Issue has been resolved by downgrading protobuf to '\n",
      "                         'version 3.20.1.\\n'\n",
      "                         'pipenv install protobuf==3.20.1\\n'\n",
      "                         'Asia Saeed'},\n",
      "                {'question': 'How to install easily kubectl on windows ?',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'To install kubectl on windows using the terminal in '\n",
      "                         'vscode (powershell), I followed this tutorial: '\n",
      "                         'https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\\n'\n",
      "                         'I first downloaded kubectl with curl, with these '\n",
      "                         'command lines: '\n",
      "                         'https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\\n'\n",
      "                         'At step 3, I followed the tutorial with the copy of '\n",
      "                         'the exe file in a specific folder on C drive.\\n'\n",
      "                         'Then I added this folder path to PATH in my '\n",
      "                         'environment variables.\\n'\n",
      "                         'Kind can be installed the same way with the curl '\n",
      "                         'command on windows, by specifying a folder that will '\n",
      "                         'be added to the path environment variable.\\n'\n",
      "                         'Added by Mélanie Fouesnard'},\n",
      "                {'question': 'Install kind through choco library',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'First you need to launch a powershell terminal with '\n",
      "                         'administrator privilege.\\n'\n",
      "                         'For this we need to install choco library first '\n",
      "                         'through the following syntax in powershell:\\n'\n",
      "                         'Set-ExecutionPolicy Bypass -Scope Process -Force; '\n",
      "                         '[System.Net.ServicePointManager]::SecurityProtocol = '\n",
      "                         '[System.Net.ServicePointManager]::SecurityProtocol '\n",
      "                         '-bor 3072; iex ((New-Object '\n",
      "                         \"System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\\n\"\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'Install Kind via Go package',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'If you are having challenges installing Kind through '\n",
      "                         'the Windows Powershell as provided on the website '\n",
      "                         'and Choco Library as I did, you can simply install '\n",
      "                         'Kind through Go.\\n'\n",
      "                         '> Download and Install Go '\n",
      "                         '(https://go.dev/doc/install)\\n'\n",
      "                         '> Confirm installation by typing the following in '\n",
      "                         'Command Prompt -  go version\\n'\n",
      "                         '> Proceed by installing Kind by following this '\n",
      "                         'command - go install sigs.k8s.io/kind@v0.20.0\\n'\n",
      "                         '>Confirm Installation kind --version\\n'\n",
      "                         'It works perfectly.'},\n",
      "                {'question': 'The connection to the server localhost:8080 was '\n",
      "                             'refused - did you specify the right host or '\n",
      "                             'port?',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': \"I ran into an issue where kubectl wasn't working.\\n\"\n",
      "                         'I kept getting the following error:\\n'\n",
      "                         'kubectl get service\\n'\n",
      "                         'The connection to the server localhost:8080 was '\n",
      "                         'refused - did you specify the right host or port?\\n'\n",
      "                         'I searched online for a resolution, but everyone '\n",
      "                         'kept talking about creating an environment variable '\n",
      "                         'and creating some admin.config file in my home '\n",
      "                         'directory.\\n'\n",
      "                         'All hogwash.\\n'\n",
      "                         'The solution to my problem was to just start over.\\n'\n",
      "                         'kind delete cluster\\n'\n",
      "                         'rm -rf ~/.kube\\n'\n",
      "                         'kind create cluster\\n'\n",
      "                         'Now when I try the same command again:\\n'\n",
      "                         'kubectl get service\\n'\n",
      "                         'NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   '\n",
      "                         'PORT(S)   AGE\\n'\n",
      "                         'kubernetes   ClusterIP   10.96.0.1    <none>        '\n",
      "                         '443/TCP   53s\\n'\n",
      "                         'Added by Martin Uribe'},\n",
      "                {'question': 'Running out of storage after building many '\n",
      "                             'docker images',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'Due to experimenting back and forth so much without '\n",
      "                         'care for storage, I just ran out of it on my 30-GB '\n",
      "                         'AWS instance.\\n'\n",
      "                         'My first reflex was to remove some zoomcamp '\n",
      "                         'directories, but of course those are mostly code so '\n",
      "                         'it didn’t help much.\\n'\n",
      "                         'Solution description\\n'\n",
      "                         '> docker images\\n'\n",
      "                         'revealed that I had over 20 GBs worth of superseded '\n",
      "                         '/ duplicate models lying around, so I proceeded to > '\n",
      "                         'docker rmi\\n'\n",
      "                         'a bunch of those — but to no avail!\\n'\n",
      "                         'It turns out that deleting docker images does not '\n",
      "                         'actually free up any space as you might expect. '\n",
      "                         'After removing images, you also need to run\\n'\n",
      "                         '> docker system prune\\n'\n",
      "                         'See also: '\n",
      "                         'https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\\n'\n",
      "                         'Added by Konrad Mühlberg'},\n",
      "                {'question': 'In HW10 Q6 what does it mean “correct value for '\n",
      "                             'CPU and memory”? Aren’t they arbitrary?',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Yes, the question does require for you to specify '\n",
      "                         'values for CPU and memory in the yaml file, however '\n",
      "                         'the question that it is use in the form only refers '\n",
      "                         'to the port which do have a define correct value for '\n",
      "                         'this specific homework.\\n'\n",
      "                         'Pastor Soto'},\n",
      "                {'question': 'Why cpu vals for Kubernetes deployment.yaml look '\n",
      "                             'like “100m” and “500m”? What does \"m\" mean?',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'In Kubernetes resource specifications, such as CPU '\n",
      "                         'requests and limits, the \"m\" stands for milliCPU, '\n",
      "                         'which is a unit of computing power. It represents '\n",
      "                         'one thousandth of a CPU core.\\n'\n",
      "                         'cpu: \"100m\" means the container is requesting 100 '\n",
      "                         'milliCPUs, which is equivalent to 0.1 CPU core.\\n'\n",
      "                         'cpu: \"500m\" means the container has a CPU limit of '\n",
      "                         '500 milliCPUs, which is equivalent to 0.5 CPU core.\\n'\n",
      "                         'These values are specified in milliCPUs to allow '\n",
      "                         'fine-grained control over CPU resources. It allows '\n",
      "                         'you to express CPU requirements and limits in a more '\n",
      "                         'granular way, especially in scenarios where your '\n",
      "                         'application might not need a full CPU core.\\n'\n",
      "                         'Added by Andrii Larkin'},\n",
      "                {'question': 'Kind cannot load docker image',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Problem: Failing to load docker-image to cluster '\n",
      "                         '(when you’ved named a cluster)\\n'\n",
      "                         'kind load docker-image '\n",
      "                         'zoomcamp-10-model:xception-v4-001\\n'\n",
      "                         'ERROR: no nodes found for cluster \"kind\"\\n'\n",
      "                         'Solution: Specify cluster name with -n\\n'\n",
      "                         'kind -n clothing-model load docker-image '\n",
      "                         'zoomcamp-10-model:xception-v4-001\\n'\n",
      "                         'Andrew Katoch'},\n",
      "                {'question': \"'kind' is not recognized as an internal or \"\n",
      "                             'external command, operable program or batch '\n",
      "                             'file. (In Windows)',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Problem: I download kind from the next command:\\n'\n",
      "                         'curl.exe -Lo kind-windows-amd64.exe '\n",
      "                         'https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\\n'\n",
      "                         'When I try\\n'\n",
      "                         'kind --version\\n'\n",
      "                         \"I get: 'kind' is not recognized as an internal or \"\n",
      "                         'external command, operable program or batch file\\n'\n",
      "                         'Solution: The default name of executable is '\n",
      "                         'kind-windows-amd64.exe, so that you have to rename '\n",
      "                         'this file to  kind.exe. Put this file in specific '\n",
      "                         'folder, and add it to PATH\\n'\n",
      "                         'Alejandro Aponte'},\n",
      "                {'question': 'Running kind on Linux with Rootless Docker or '\n",
      "                             'Rootless Podman',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Using kind with Rootless Docker or Rootless Podman '\n",
      "                         'requires some changes on the system (Linux), see '\n",
      "                         'kind – Rootless (k8s.io).\\n'\n",
      "                         'Sylvia Schmitt'},\n",
      "                {'question': 'Kubernetes-dashboard',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Deploy and Access the Kubernetes Dashboard\\nLuke'},\n",
      "                {'question': 'Correct AWS CLI version for eksctl',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Make sure you are on AWS CLI v2 (check with aws '\n",
      "                         '--version)\\n'\n",
      "                         'https://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html'},\n",
      "                {'question': 'TypeError: __init__() got an unexpected keyword '\n",
      "                             \"argument 'unbound_message' while importing Flask\",\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Problem Description:\\n'\n",
      "                         'In video 10.3, when I was testing a flask service, I '\n",
      "                         'got the above error. I ran docker run .. in one '\n",
      "                         'terminal. When in second terminal I run python '\n",
      "                         'gateway.py, I get the above error.\\n'\n",
      "                         'Solution: This error has something to do with '\n",
      "                         'versions of Flask and Werkzeug. I got the same '\n",
      "                         'error, if I just import flask with from flask import '\n",
      "                         'Flask.\\n'\n",
      "                         'By running pip freeze > requirements.txt,I found '\n",
      "                         'that their versions are Flask==2.2.2 and '\n",
      "                         'Werkzeug==2.2.2. This error appears while using an '\n",
      "                         'old version of werkzeug (2.2.2) with new version of '\n",
      "                         'flask (2.2.2). I solved it by pinning version of '\n",
      "                         'Flask into an older version with pipenv install '\n",
      "                         'Flask==2.1.3.\\n'\n",
      "                         'Added by Bhaskar Sarma'},\n",
      "                {'question': 'Command aws ecr get-login --no-include-email '\n",
      "                             'returns “aws: error: argument operation: Invalid '\n",
      "                             'choice…”',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'As per AWS documentation:\\n'\n",
      "                         'https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\\n'\n",
      "                         'You need to do: (change the fields in red)\\n'\n",
      "                         'aws ecr get-login-password --region region | docker '\n",
      "                         'login --username AWS --password-stdin '\n",
      "                         'aws_account_id.dkr.ecr.region.amazonaws.com\\n'\n",
      "                         'Alternatively you can run the following command '\n",
      "                         'without changing anything given you have a default '\n",
      "                         'region configured\\n'\n",
      "                         'aws ecr get-login-password --region $(aws configure '\n",
      "                         'get region) | docker login --username AWS '\n",
      "                         '--password-stdin \"$(aws sts get-caller-identity '\n",
      "                         '--query \"Account\" --output text).dkr.ecr.$(aws '\n",
      "                         'configure get region).amazonaws.com\"\\n'\n",
      "                         'Added by Humberto Rodriguez'},\n",
      "                {'question': 'Error downloading  tensorflow/serving:2.7.0 on '\n",
      "                             'Apple M1 Mac',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'While trying to run the docker code on M1:\\n'\n",
      "                         'docker run --platform linux/amd64 -it --rm \\\\\\n'\n",
      "                         '-p 8500:8500 \\\\\\n'\n",
      "                         '-v $(pwd)/clothing-model:/models/clothing-model/1 '\n",
      "                         '\\\\\\n'\n",
      "                         '-e MODEL_NAME=\"clothing-model\" \\\\\\n'\n",
      "                         'tensorflow/serving:2.7.0\\n'\n",
      "                         'It outputs the error:\\n'\n",
      "                         'Error:\\n'\n",
      "                         'Status: Downloaded newer image for '\n",
      "                         'tensorflow/serving:2.7.0\\n'\n",
      "                         '[libprotobuf FATAL '\n",
      "                         'external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] '\n",
      "                         'CHECK failed: file != nullptr:\\n'\n",
      "                         'terminate called after throwing an instance of '\n",
      "                         \"'google::protobuf::FatalException'\\n\"\n",
      "                         'what():  CHECK failed: file != nullptr:\\n'\n",
      "                         'qemu: uncaught target signal 6 (Aborted) - core '\n",
      "                         'dumped\\n'\n",
      "                         '/usr/bin/tf_serving_entrypoint.sh: line 3:     8 '\n",
      "                         'Aborted                 tensorflow_model_server '\n",
      "                         '--port=8500 --rest_api_port=8501 '\n",
      "                         '--model_name=${MODEL_NAME} '\n",
      "                         '--model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} '\n",
      "                         '\"$@\"\\n'\n",
      "                         'Solution\\n'\n",
      "                         'docker pull emacski/tensorflow-serving:latest\\n'\n",
      "                         'docker run -it --rm \\\\\\n'\n",
      "                         '-p 8500:8500 \\\\\\n'\n",
      "                         '-v $(pwd)/clothing-model:/models/clothing-model/1 '\n",
      "                         '\\\\\\n'\n",
      "                         '-e MODEL_NAME=\"clothing-model\" \\\\\\n'\n",
      "                         'emacski/tensorflow-serving:latest-linux_arm64\\n'\n",
      "                         'See more here: '\n",
      "                         'https://github.com/emacski/tensorflow-serving-arm\\n'\n",
      "                         'Added by Daniel Egbo'},\n",
      "                {'question': 'Illegal instruction error when running '\n",
      "                             'tensorflow/serving image on Mac M2 Apple Silicon '\n",
      "                             '(potentially on M1 as well)',\n",
      "                 'section': '10. Kubernetes and TensorFlow Serving',\n",
      "                 'text': 'Similar to the one above but with a different '\n",
      "                         'solution the main reason is that emacski doesn’t '\n",
      "                         'seem to maintain the repo any more, the latest image '\n",
      "                         'is from 2 years ago at the time of writing (December '\n",
      "                         '2023)\\n'\n",
      "                         'Problem:\\n'\n",
      "                         'While trying to run the docker code on Mac M2 apple '\n",
      "                         'silicon:\\n'\n",
      "                         'docker run --platform linux/amd64 -it --rm \\\\\\n'\n",
      "                         '-p 8500:8500 \\\\\\n'\n",
      "                         '-v $(pwd)/clothing-model:/models/clothing-model/1 '\n",
      "                         '\\\\\\n'\n",
      "                         '-e MODEL_NAME=\"clothing-model\" \\\\\\n'\n",
      "                         'tensorflow/serving\\n'\n",
      "                         'You get an error:\\n'\n",
      "                         '/usr/bin/tf_serving_entrypoint.sh: line 3:     7 '\n",
      "                         'Illegal instruction     tensorflow_model_server '\n",
      "                         '--port=8500 --rest_api_port=8501 '\n",
      "                         '--model_name=${MODEL_NAME} '\n",
      "                         '--model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} '\n",
      "                         '\"$@\"\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Use bitnami/tensorflow-serving base image\\n'\n",
      "                         'Launch it either using docker run\\n'\n",
      "                         'docker run -d \\\\\\n'\n",
      "                         '--name tf_serving \\\\\\n'\n",
      "                         '-p 8500:8500 \\\\\\n'\n",
      "                         '-p 8501:8501 \\\\\\n'\n",
      "                         '-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\\\\n'\n",
      "                         '-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\\\\n'\n",
      "                         'bitnami/tensorflow-serving:2\\n'\n",
      "                         'Or the following docker-compose.yaml\\n'\n",
      "                         \"version: '3'\\n\"\n",
      "                         'services:\\n'\n",
      "                         'tf_serving:\\n'\n",
      "                         'image: bitnami/tensorflow-serving:2\\n'\n",
      "                         'volumes:\\n'\n",
      "                         '- ${PWD}/clothing-model:/bitnami/model-data/1\\n'\n",
      "                         'ports:\\n'\n",
      "                         '- 8500:8500\\n'\n",
      "                         '- 8501:8501\\n'\n",
      "                         'environment:\\n'\n",
      "                         '- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\\n'\n",
      "                         'And run it with\\n'\n",
      "                         'docker compose up\\n'\n",
      "                         'Added by Alex Litvinov'},\n",
      "                {'question': 'HPA doesn’t show CPU metrics',\n",
      "                 'section': '11. KServe',\n",
      "                 'text': 'Problem: CPU metrics Shows Unknown\\n'\n",
      "                         'NAME         REFERENCE           TARGETS         '\n",
      "                         'MINPODS   MAXPODS   REPLICAS   AGE\\n'\n",
      "                         'credit-hpa   Deployment/credit   <unknown>/20%   '\n",
      "                         '1         3         1          18s\\n'\n",
      "                         'FailedGetResourceMetric       2m15s (x169 over 44m)  '\n",
      "                         'horizontal-pod-autoscaler  failed to get cpu '\n",
      "                         'utilization: unable to get metrics for resource cpu: '\n",
      "                         'unable to fetch metrics from resource metrics API:\\n'\n",
      "                         'Solution:\\n'\n",
      "                         '-> Delete HPA (kubectl delete hpa credit-hpa)\\n'\n",
      "                         '-> kubectl apply -f '\n",
      "                         'https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\\n'\n",
      "                         '-> Create HPA\\n'\n",
      "                         'This should solve the cpu metrics report issue.\\n'\n",
      "                         'Added by Priya V'},\n",
      "                {'question': 'Errors with istio during installation',\n",
      "                 'section': '11. KServe',\n",
      "                 'text': 'Problem description:\\n'\n",
      "                         'Running this:\\n'\n",
      "                         'curl -s '\n",
      "                         '\"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" '\n",
      "                         '| bash\\n'\n",
      "                         'Fails with errors because of istio failing to update '\n",
      "                         'resources, and you are on kubectl > 1.25.0.\\n'\n",
      "                         'Check kubectl version with kubectl version\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Edit the file “quick_install.bash” by downloading it '\n",
      "                         'with curl without running bash. Edit the versions of '\n",
      "                         'Istio and Knative as per the matrix on the KServe '\n",
      "                         'website.\\n'\n",
      "                         'Run the bash script now.\\n'\n",
      "                         'Added by Andrew Katoch'},\n",
      "                {'question': 'Problem title',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'Solution description\\n'\n",
      "                         '(optional) Added by Name'},\n",
      "                {'question': 'What are the project deadlines?',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'Answer: You can see them here (it’s taken from the '\n",
      "                         '2022 cohort page). Go to the cohort folder for your '\n",
      "                         'own cohort’s deadline.'},\n",
      "                {'question': 'Are projects solo or collaborative/group work?',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'Answer: All midterms and capstones are meant to be '\n",
      "                         'solo projects. [source @Alexey]'},\n",
      "                {'question': 'What modules, topics, problem-sets should a '\n",
      "                             'midterm/capstone project cover? Can I do xyz?',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'Answer: Ideally midterms up to module-06, capstones '\n",
      "                         'include all modules in that cohort’s syllabus. But '\n",
      "                         'you can include anything extra that you want to '\n",
      "                         'feature. Just be sure to document anything not '\n",
      "                         'covered in class.\\n'\n",
      "                         'Also watch office hours from previous cohorts. Go to '\n",
      "                         'DTC youtube channel and click on Playlists and '\n",
      "                         'search for {course yyyy}. ML Zoomcamp was first '\n",
      "                         'launched in 2021.\\n'\n",
      "                         'More discussions:\\n'\n",
      "                         '[source1] [source2] [source3]'},\n",
      "                {'question': 'Crucial Links',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'These links apply to all projects, actually. Again, '\n",
      "                         'for some cohorts, the modules/syllabus might be '\n",
      "                         'different, so always check in your cohort’s folder '\n",
      "                         'as well for additional or different instructions, if '\n",
      "                         'any.\\n'\n",
      "                         'Midterm Project Sample: '\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\\n'\n",
      "                         'MidTerm Project Deliverables: '\n",
      "                         'https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\\n'\n",
      "                         'Submit MidTerm Project: '\n",
      "                         'https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\\n'\n",
      "                         'Datasets:\\n'\n",
      "                         'https://www.kaggle.com/datasets and '\n",
      "                         'https://www.kaggle.com/competitions\\n'\n",
      "                         'https://archive.ics.uci.edu/ml/index.php\\n'\n",
      "                         'https://data.europa.eu/en\\n'\n",
      "                         'https://www.openml.org/search?type=data\\n'\n",
      "                         'https://newzealand.ai/public-data-sets\\n'\n",
      "                         'https://datasetsearch.research.google.com\\n'\n",
      "                         'What to do and Deliverables\\n'\n",
      "                         \"Think of a problem that's interesting for you and \"\n",
      "                         'find a dataset for that\\n'\n",
      "                         'Describe this problem and explain how a model could '\n",
      "                         'be used\\n'\n",
      "                         'Prepare the data and doing EDA, analyze important '\n",
      "                         'features\\n'\n",
      "                         'Train multiple models, tune their performance and '\n",
      "                         'select the best model\\n'\n",
      "                         'Export the notebook into a script\\n'\n",
      "                         'Put your model into a web service and deploy it '\n",
      "                         'locally with Docker\\n'\n",
      "                         'Bonus points for deploying the service to the cloud'},\n",
      "                {'question': 'How to conduct peer reviews for projects?',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'Answer: Previous cohorts projects page has '\n",
      "                         'instructions (youtube).\\n'\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\\n'\n",
      "                         'Alexey and his team will compile a g-sheet with '\n",
      "                         'links to submitted projects with our hashed emails '\n",
      "                         '(just like when we check leaderboard for homework) '\n",
      "                         'that are ours to review within the evaluation '\n",
      "                         'deadline.\\n'\n",
      "                         '~~~ Added by Nukta Bhatia ~~~'},\n",
      "                {'question': 'Computing the hash for project review',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'See the answer here.'},\n",
      "                {'question': 'Learning in public links for the projects',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'For the learning in public for this midterm project '\n",
      "                         'it seems that has a total value of 14!, Does this '\n",
      "                         'mean that we need make 14 posts?, Or the regular '\n",
      "                         'seven posts for each module and each one with a '\n",
      "                         'value of 2?, Or just one with a total value of 14?\\n'\n",
      "                         '14 posts, one for each day'},\n",
      "                {'question': \"My dataset is too large and I can't loaded in \"\n",
      "                             'GitHub , do anyone knows about a solution?',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'You can use git-lfs (https://git-lfs.com/) for '\n",
      "                         'upload large file to github repository.\\n'\n",
      "                         'Ryan Pramana'},\n",
      "                {'question': 'What If I submitted only two projects and failed '\n",
      "                             'to submit the third?',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'If you have submitted two projects (and '\n",
      "                         'peer-reviewed at least 3 course-mates’ projects for '\n",
      "                         'each submission), you will get the certificate for '\n",
      "                         'the course. According to the course coordinator, '\n",
      "                         'Alexey Grigorev, only two projects are needed to get '\n",
      "                         'the course certificate.\\n'\n",
      "                         '(optional) David Odimegwu'},\n",
      "                {'question': 'I did the first two projects and skipped the '\n",
      "                             \"last one so I wouldn't have two peer review in \"\n",
      "                             'second capstone right?',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'Yes. You only need to review peers when you submit '\n",
      "                         'your project.\\n'\n",
      "                         'Confirmed on Slack by Alexey Grigorev (added by '\n",
      "                         'Rileen Sinha)'},\n",
      "                {'question': 'How many models should I train?',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'Regarding Point 4 in the midterm deliverables, which '\n",
      "                         'states, \"Train multiple models, tune their '\n",
      "                         'performance, and select the best model,\" you might '\n",
      "                         'wonder, how many models should you train? The answer '\n",
      "                         'is simple: train as many as you can. The term '\n",
      "                         '\"multiple\" implies having more than one model, so as '\n",
      "                         \"long as you have more than one, you're on the right \"\n",
      "                         'track.'},\n",
      "                {'question': 'How does the project evaluation work for you as '\n",
      "                             'a peer reviewer?',\n",
      "                 'section': 'Projects (Midterm and Capstone)',\n",
      "                 'text': 'I am not sure how the project evaluate assignment '\n",
      "                         'works? Where do I find this? I have access to all '\n",
      "                         'the capstone 2 project, perhaps, I can randomly pick '\n",
      "                         'any to review.\\n'\n",
      "                         'Answer:\\n'\n",
      "                         'The link provided for example (2023/Capstone link ): '\n",
      "                         'https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform '\n",
      "                         'contains a list of all submitted projects to be '\n",
      "                         'evaluated. More specific, you are to review 3 '\n",
      "                         'assigned peer projects. In the spreadsheet are 3 '\n",
      "                         'hash values of your assigned peer projects. However, '\n",
      "                         'you need to derive the your hash value of your email '\n",
      "                         'address and find the value on the spreadsheet under '\n",
      "                         'the (reviewer_hash) heading.\\n'\n",
      "                         'To calculate your hash value run the python code '\n",
      "                         'below:\\n'\n",
      "                         'from hashlib import sha1\\n'\n",
      "                         'def compute_hash(email):\\n'\n",
      "                         'return '\n",
      "                         \"sha1(email.lower().encode('utf-8')).hexdigest()\\n\"\n",
      "                         '# Example usage **** enter your email below '\n",
      "                         '(Example1@gmail.com)****\\n'\n",
      "                         'email = \"Example1@gmail.com\"\\n'\n",
      "                         'hashed_email = compute_hash(email)\\n'\n",
      "                         'print(\"Original Email:\", email)\\n'\n",
      "                         'print(\"Hashed Email (SHA-1):\", hashed_email)\\n'\n",
      "                         'Edit the above code to replace Example1@gmail.com as '\n",
      "                         'your email address\\n'\n",
      "                         'Store and run the above python code from your '\n",
      "                         'terminal. See below as the Hashed Email (SHA-1) '\n",
      "                         'value\\n'\n",
      "                         'You then go to the link: '\n",
      "                         'https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\\n'\n",
      "                         'Lastly, copy the “Hashed Email (SHA-1): '\n",
      "                         'bd9770be022dede87419068aa1acd7a2ab441675” value and '\n",
      "                         'search for 3 identical entries. There you should see '\n",
      "                         'your peer project to be reviewed.\\n'\n",
      "                         'By Emmanuel Ayeni'},\n",
      "                {'question': 'Do you pass a project based on the average of '\n",
      "                             'everyone else’s scores or based on the total '\n",
      "                             'score you earn?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Alexey Grigorev: “It’s based on all the scores to '\n",
      "                         'make sure most of you '\n",
      "                         'pass.”                                                   '\n",
      "                         'By Annaliese Bronz\\n'\n",
      "                         'Other course-related questions that don’t fall into '\n",
      "                         'any of the categories above or can apply to more '\n",
      "                         'than one category/module'},\n",
      "                {'question': 'Why do I need to provide a train.py file when I '\n",
      "                             'already have the notebook.ipynb file?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Answer: The train.py file will be used by your peers '\n",
      "                         'to review your midterm project. It is for them to '\n",
      "                         'cross-check that your training process works on '\n",
      "                         'someone else’s system. It should also be included in '\n",
      "                         'the environment in conda or with pipenv.\\n'\n",
      "                         'Odimegwu David'},\n",
      "                {'question': 'Loading the Image with PILLOW library and '\n",
      "                             'converting to numpy array',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Pip install pillow - install pillow library\\n'\n",
      "                         'from PIL import Image\\n'\n",
      "                         \"img = Image.open('aeroplane.png')\\n\"\n",
      "                         'From numpy import asarray\\n'\n",
      "                         'numdata=asarray(img)\\n'\n",
      "                         'Krishna Anand'},\n",
      "                {'question': 'Is a train.py file necessary when you have a '\n",
      "                             'train.ipynb file in your midterm project '\n",
      "                             'directory?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Ans: train.py has to be a python file. This is '\n",
      "                         'because running a python script for training a model '\n",
      "                         \"is much simpler than running a notebook and that's \"\n",
      "                         'how training jobs usually look like in real life.'},\n",
      "                {'question': 'Is there a way to serve up a form for users to '\n",
      "                             'enter data for the model to crunch on?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Yes, you can create a mobile app or interface that '\n",
      "                         'manages these forms and validations. But you should '\n",
      "                         'also perform validations on backend.\\n'\n",
      "                         'You can also check Streamlit: '\n",
      "                         'https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\\n'\n",
      "                         'Alejandro Aponte'},\n",
      "                {'question': 'How to get feature importance for XGboost model',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Using model.feature_importances_ can gives you an '\n",
      "                         'error:\\n'\n",
      "                         \"AttributeError: 'Booster' object has no attribute \"\n",
      "                         \"'feature_importances_'\\n\"\n",
      "                         'Answer: if you train the model like this: model = '\n",
      "                         'xgb.train you should use get_score() instead\\n'\n",
      "                         'Ekaterina Kutovaia'},\n",
      "                {'question': '[Errno 12] Cannot allocate memory in AWS Elastic '\n",
      "                             'Container Service',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'In the Elastic Container Service task log, error '\n",
      "                         '“[Errno 12] Cannot allocate memory” showed up.\\n'\n",
      "                         'Just increase the RAM and CPU in your task '\n",
      "                         'definition.\\n'\n",
      "                         'Humberto Rodriguez'},\n",
      "                {'question': 'Pickle error: can’t get attribute XXX on module '\n",
      "                             '__main__',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'When running a docker container with waitress '\n",
      "                         'serving the app.py for making predictions, pickle '\n",
      "                         \"will throw an error that can't get attribute \"\n",
      "                         '<name_of_class> on module __main__.\\n'\n",
      "                         'This does not happen when Flask is used directly, '\n",
      "                         'i.e. not through waitress.\\n'\n",
      "                         'The problem is that the model uses a custom column '\n",
      "                         'transformer class, and when the model was saved, it '\n",
      "                         'was saved from the __main__ module (e.g. python '\n",
      "                         'train.py). Pickle will reference the class in the '\n",
      "                         'global namespace (top-level code): '\n",
      "                         '__main__.<custom_class>.\\n'\n",
      "                         'When using waitress, waitress will load the '\n",
      "                         'predict_app module and this will call pickle.load, '\n",
      "                         'that will try to find __main__.<custom_class> that '\n",
      "                         'does not exist.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Put the class into a separate module and import it '\n",
      "                         'in both the script that saves the model (e.g. '\n",
      "                         'train.py) and the script that loads the model (e.g. '\n",
      "                         'predict.py)\\n'\n",
      "                         'Note: If Flask is used (no waitress) in predict.py, '\n",
      "                         'and predict.py has the definition of the class, '\n",
      "                         'When  it is run: python predict.py, it will work '\n",
      "                         'because the class is in the same namespace as the '\n",
      "                         'one used when the model was saved (__main__).\\n'\n",
      "                         'Detailed info: '\n",
      "                         'https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\\n'\n",
      "                         'Marcos MJD'},\n",
      "                {'question': 'How to handle outliers in a dataset?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'There are different techniques, but the most common '\n",
      "                         'used are the next:\\n'\n",
      "                         'Dataset transformation (for example, log '\n",
      "                         'transformation)\\n'\n",
      "                         'Clipping high values\\n'\n",
      "                         'Dropping these observations\\n'\n",
      "                         'Alena Kniazeva'},\n",
      "                {'question': 'Failed loading Bento from directory '\n",
      "                             '/home/bentoml/bento: Failed to import module '\n",
      "                             '\"service\": No module named \\'sklearn\\'',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'I was getting the below error message when I was '\n",
      "                         'trying to create docker image using bentoml\\n'\n",
      "                         '[bentoml-cli] `serve` failed: Failed loading Bento '\n",
      "                         'from directory /home/bentoml/bento: Failed to import '\n",
      "                         'module \"service\": No module named \\'sklearn\\'\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'The cause was because , in bentofile.yaml, I wrote '\n",
      "                         'sklearn instead of scikit-learn. Issue was fixed '\n",
      "                         'after I modified the packages list as below.\\n'\n",
      "                         'packages: # Additional pip packages required by the '\n",
      "                         'service\\n'\n",
      "                         '- xgboost\\n'\n",
      "                         '- scikit-learn\\n'\n",
      "                         '- pydantic\\n'\n",
      "                         'Asia Saeed'},\n",
      "                {'question': 'BentoML not working with –production flag at any '\n",
      "                             'stage: e.g. with bentoml serve and while running '\n",
      "                             'the bentoml container',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'You might see a long error message with something '\n",
      "                         'about sparse matrices, and in the swagger UI, you '\n",
      "                         'get a code 500 error with “” (empty string) as '\n",
      "                         'output.\\n'\n",
      "                         'Potential reason: Setting DictVectorizer or OHE to '\n",
      "                         'sparse while training, and then storing this in a '\n",
      "                         'pipeline or custom object in the benotml model '\n",
      "                         'saving stage in train.py. This means that when the '\n",
      "                         'custom object is called in service.py, it will '\n",
      "                         'convert each input to a different sized sparse '\n",
      "                         \"matrix, and this can't be batched due to \"\n",
      "                         'inconsistent length. In this case, bentoml model '\n",
      "                         'signatures should have batchable set to False for '\n",
      "                         'production during saving the bentoml mode in '\n",
      "                         'train.py.\\n'\n",
      "                         '(Memoona Tahira)'},\n",
      "                {'question': 'Reproducibility',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Problem description:\\n'\n",
      "                         'Do we have to run everything?\\n'\n",
      "                         'You are encouraged, if you can, to run them. As this '\n",
      "                         'provides another opportunity to learn from others.\\n'\n",
      "                         'Not everyone will be able to run all the files, in '\n",
      "                         'particular the neural networks.\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'Alternatively, can you see that everything you need '\n",
      "                         'to reproduce is there: the dataset is there, the '\n",
      "                         'instructions are there, are there any obvious errors '\n",
      "                         'and so on.\\n'\n",
      "                         'Related slack conversation here.\\n'\n",
      "                         '(Gregory Morris)'},\n",
      "                {'question': 'Model too big',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'If your model is too big for github one option is to '\n",
      "                         'try and compress the model using joblib. For example '\n",
      "                         \"joblib.dump(model, model_filename, compress=('zlib', \"\n",
      "                         '6) will use zlib to compress the model. Just note '\n",
      "                         'this could take a few moments as the model is being '\n",
      "                         'compressed.\\n'\n",
      "                         'Quinn Avila'},\n",
      "                {'question': 'Permissions to push docker to Google Container '\n",
      "                             'Registry',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'When you try to push the docker image to Google '\n",
      "                         'Container Registry and get this message '\n",
      "                         \"“unauthorized: You don't have the needed permissions \"\n",
      "                         'to perform this operation, and you may have invalid '\n",
      "                         'credentials.”, type this below on console, but first '\n",
      "                         'install https://cloud.google.com/sdk/docs/install, '\n",
      "                         'this is to be able to use gcloud in console:\\n'\n",
      "                         'gcloud auth configure-docker\\n'\n",
      "                         '(Jesus Acuña)'},\n",
      "                {'question': 'Tflite_runtime unable to install',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'I am getting this error message when I tried to '\n",
      "                         'install tflite in a pipenv environment\\n'\n",
      "                         'Error:  An error occurred while installing '\n",
      "                         'tflite_runtime!\\n'\n",
      "                         'Error text:\\n'\n",
      "                         'ERROR: Could not find a version that satisfies the '\n",
      "                         'requirement tflite_runtime (from versions: none)\\n'\n",
      "                         'ERROR: No matching distribution found for '\n",
      "                         'tflite_runtime\\n'\n",
      "                         'This version of tflite do not run on python 3.10, '\n",
      "                         'the way we can make it work is by install python '\n",
      "                         '3.9, after that it would install the tflite_runtime '\n",
      "                         'without problem.\\n'\n",
      "                         'Pastor Soto\\n'\n",
      "                         'Check all available versions here:\\n'\n",
      "                         'https://google-coral.github.io/py-repo/tflite-runtime/\\n'\n",
      "                         'If you don’t find a combination matching your setup, '\n",
      "                         'try out the options at\\n'\n",
      "                         'https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\\n'\n",
      "                         'which you can install as shown in the lecture, e.g.\\n'\n",
      "                         'pip install '\n",
      "                         'https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\\n'\n",
      "                         'Finally, if nothing works, use the TFLite included '\n",
      "                         'in TensorFlow for local development, and use Docker '\n",
      "                         'for testing Lambda.\\n'\n",
      "                         'Rileen Sinha (based on discussions on Slack)'},\n",
      "                {'question': 'Error when running '\n",
      "                             'ImageDataGenerator.flow_from_dataframe',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': \"Error: ImageDataGenerator name 'scipy' is not \"\n",
      "                         'defined.\\n'\n",
      "                         'Check that scipy is installed in your environment.\\n'\n",
      "                         'Restart jupyter kernel and try again.\\n'\n",
      "                         'Marcos MJD'},\n",
      "                {'question': 'How to pass BentoML content / docker container '\n",
      "                             'to Amazon Lambda',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Tim from BentoML has prepared a dedicated video '\n",
      "                         'tutorial wrt this use case here:\\n'\n",
      "                         'https://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\\n'\n",
      "                         'Konrad Muehlberg'},\n",
      "                {'question': 'Error UnidentifiedImageError: cannot identify '\n",
      "                             'image file',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'In deploying model part, I wanted to test my model '\n",
      "                         'locally on a test-image data and I had this silly '\n",
      "                         'error after the following command:\\n'\n",
      "                         'url = '\n",
      "                         \"'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\\n\"\n",
      "                         'X = preprocessor.from_url(url)\\n'\n",
      "                         'I got the error:\\n'\n",
      "                         'UnidentifiedImageError: cannot identify image file '\n",
      "                         '<_io.BytesIO object at 0x7f797010a590>\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Add ?raw=true after .jpg in url. E.g. as below\\n'\n",
      "                         'url = '\n",
      "                         '‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’\\n'\n",
      "                         'Bhaskar Sarma'},\n",
      "                {'question': '[pipenv.exceptions.ResolutionFailure]: Warning: '\n",
      "                             'Your dependencies could not be resolved. You '\n",
      "                             'likely have a mismatch in your sub-dependencies',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Problem: If you run pipenv install and get this '\n",
      "                         'message. Maybe manually change Pipfile and '\n",
      "                         'Pipfile.lock.\\n'\n",
      "                         'Solution: Run: ` pipenv lock` for fix this problem '\n",
      "                         'and dependency files\\n'\n",
      "                         'Alejandro Aponte'},\n",
      "                {'question': 'Get_feature_names() not found',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Problem: In the course this function worked to get '\n",
      "                         'the features from the dictVectorizer instance: '\n",
      "                         'dv.get_feature_names(). But in my computer did not '\n",
      "                         'work. I think it has to do with library versions and '\n",
      "                         'but apparently that function will be deprecated '\n",
      "                         'soon:\\n'\n",
      "                         'Old: '\n",
      "                         'https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\n'\n",
      "                         'New: '\n",
      "                         'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\\n'\n",
      "                         'Solution: change the line dv.get_feature_names() to '\n",
      "                         'list(dv.get_feature_names_out))\\n'\n",
      "                         'Ibai Irastorza'},\n",
      "                {'question': 'Error decoding JSON response: Expecting value: '\n",
      "                             'line 1 column 1 (char 0)',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Problem happens when contacting the server waiting '\n",
      "                         'to send your predict-test and your data here in the '\n",
      "                         'correct shape.\\n'\n",
      "                         'The problem was the format input to the model wasn’t '\n",
      "                         'in the right shape. Server receives the data in json '\n",
      "                         'format (dict) which is not suitable for the model. U '\n",
      "                         'should convert it to like numpy arrays.\\n'\n",
      "                         'Ahmed Okka'},\n",
      "                {'question': 'Free cloud alternatives',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Q: Hii folks, I tried deploying my docker image on '\n",
      "                         \"Render, but it won't I get SIGTERM everytime.\\n\"\n",
      "                         'I think .5GB RAM is not enough, is there any other '\n",
      "                         'free alternative available ?\\n'\n",
      "                         'A: aws (amazon), gcp (google), saturn.\\n'\n",
      "                         'Both aws and gcp give microinstance for free for a '\n",
      "                         'VERY long time, and a bunch more free stuff.\\n'\n",
      "                         'Saturn even provides free GPU instances. Recent '\n",
      "                         'promo link from mlzoomcamp for Saturn:\\n'\n",
      "                         '“You can sign up here: '\n",
      "                         'https://bit.ly/saturn-mlzoomcamp\\n'\n",
      "                         \"When you sign up, write in the chat box that you're \"\n",
      "                         'an ML Zoomcamp student and you should get extra GPU '\n",
      "                         'hours (something like 150)”\\n'\n",
      "                         'Added by Andrii Larkin'},\n",
      "                {'question': 'Getting day of the year from day and month '\n",
      "                             'column',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Problem description: I have one column '\n",
      "                         'day_of_the_month . It has values 1, 2, 20, 25 etc. '\n",
      "                         'and int . I have a second column month_of_the_year. '\n",
      "                         'It has values jan, feb, ..dec. and are string. I '\n",
      "                         'want to convert these two columns into one column '\n",
      "                         'day_of_the_year and I want them to be int. 2 and jan '\n",
      "                         'should give me 2, i.e. 2nd day of the year, 1 and '\n",
      "                         'feb should give me 32, i.e. 32 nd day of the year. '\n",
      "                         'What is the simplest pandas-way to do it?\\n'\n",
      "                         'Solution description:\\n'\n",
      "                         'convert dtype in day_of_the_month column from int to '\n",
      "                         \"str with df['day_of_the_month'] = \"\n",
      "                         \"df['day_of_the_month'].map(str)\\n\"\n",
      "                         'convert month_of_the_year column in jan, feb ...,dec '\n",
      "                         'into 1,2, ..,12 string using map()\\n'\n",
      "                         'convert day and month into a datetime object with:\\n'\n",
      "                         \"df['date_formatted'] = pd.to_datetime(\\n\"\n",
      "                         'dict(\\n'\n",
      "                         \"year='2055',\\n\"\n",
      "                         \"month=df['month'],\\n\"\n",
      "                         \"day=df['day']\\n\"\n",
      "                         ')\\n'\n",
      "                         ')\\n'\n",
      "                         'get day of year with: '\n",
      "                         \"df['day_of_year']=df['date_formatted'].dt.dayofyear\\n\"\n",
      "                         '(Bhaskar Sarma)'},\n",
      "                {'question': 'Chart for classes and predictions',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'How to visualize the predictions per classes after '\n",
      "                         'training a neural net\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'classes, predictions = zip(*dict(zip(classes, '\n",
      "                         'predictions)).items())\\n'\n",
      "                         'plt.figure(figsize=(12, 3))\\n'\n",
      "                         'plt.bar(classes, predictions)\\n'\n",
      "                         'Luke'},\n",
      "                {'question': 'Convert dictionary values to Dataframe table',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'You can convert the prediction output values to a '\n",
      "                         'datafarme using \\n'\n",
      "                         \"df = pd.DataFrame.from_dict(dict, orient='index' , \"\n",
      "                         'columns=[\"Prediction\"])\\n'\n",
      "                         'Edidiong Esu'},\n",
      "                {'question': 'Kitchenware Classification Competition Dataset '\n",
      "                             'Generator',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'The image dataset for the competition was in a '\n",
      "                         'different layout from what we used in the dino vs '\n",
      "                         'dragon lesson. Since that’s what was covered, some '\n",
      "                         'folks were more comfortable with that setup, so I '\n",
      "                         'wrote a script that would generate it for them\\n'\n",
      "                         'It can be found here: kitchenware-dataset-generator '\n",
      "                         '| Kaggle\\n'\n",
      "                         'Martin Uribe'},\n",
      "                {'question': 'CUDA toolkit and cuDNN Install for Tensorflow',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Install Nvidia drivers: '\n",
      "                         'https://www.nvidia.com/download/index.aspx.\\n'\n",
      "                         'Windows:\\n'\n",
      "                         'Install Anaconda prompt https://www.anaconda.com/\\n'\n",
      "                         'Two options:\\n'\n",
      "                         'Install package ‘tensorflow-gpu’ in Anaconda\\n'\n",
      "                         'Install the Tensorflow way '\n",
      "                         'https://www.tensorflow.org/install/pip#windows-native\\n'\n",
      "                         'WSL/Linux:\\n'\n",
      "                         'WSL: Use the Windows Nvida drivers, do not touch '\n",
      "                         'that.\\n'\n",
      "                         'Two options:\\n'\n",
      "                         'Install the Tensorflow way '\n",
      "                         'https://www.tensorflow.org/install/pip#linux_1\\n'\n",
      "                         'Make sure to follow step 4 to install CUDA by '\n",
      "                         'environment\\n'\n",
      "                         'Also run:\\n'\n",
      "                         'echo ‘export '\n",
      "                         'XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> '\n",
      "                         '$CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\\n'\n",
      "                         'Install CUDA toolkit 11.x.x '\n",
      "                         'https://developer.nvidia.com/cuda-toolkit-archive\\n'\n",
      "                         'Install '\n",
      "                         'https://developer.nvidia.com/rdp/cudnn-download\\n'\n",
      "                         'Now you should be able to do training/inference with '\n",
      "                         'GPU in Tensorflow\\n'\n",
      "                         '(Learning in public links Links to social media '\n",
      "                         'posts where you share your progress with others '\n",
      "                         '(LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The '\n",
      "                         'scores for this part will be capped at 7 points. '\n",
      "                         'Please make sure the posts are valid URLs starting '\n",
      "                         'with \"https://\" Does it mean that I should provide '\n",
      "                         'my linkedin link? or it means that I should write a '\n",
      "                         'post that I have completed my first assignement? (\\n'\n",
      "                         'ANS (by ezehcp7482@gmail.com): Yes, provide the '\n",
      "                         'linkedIN link to where you posted.\\n'\n",
      "                         'ezehcp7482@gmail.com:\\n'\n",
      "                         'PROBLEM: Since I had to put up a link to a public '\n",
      "                         'repository, I had to use Kaggle and uploading the '\n",
      "                         'dataset therein was a bit difficult; but I had to '\n",
      "                         '‘google’ my way out.\\n'\n",
      "                         'ANS: See this link for a guide '\n",
      "                         '(https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)'},\n",
      "                {'question': 'About getting the wrong result when multiplying '\n",
      "                             'matrices',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'When multiplying matrices, the order of '\n",
      "                         'multiplication is important.\\n'\n",
      "                         'For example:\\n'\n",
      "                         'A (m x n) * B (n x p) = C (m x p)\\n'\n",
      "                         'B (n x p) * A (m x n) = D (n x n)\\n'\n",
      "                         'C and D are matrices of different sizes and usually '\n",
      "                         'have different values. Therefore the order is '\n",
      "                         'important in matrix multiplication and changing the '\n",
      "                         'order changes the result.\\n'\n",
      "                         'Baran Akın'},\n",
      "                {'question': 'None of the videos have how to install the '\n",
      "                             'environment in Mac, does someone have '\n",
      "                             'instructions for Mac with M1 chip?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Refer to '\n",
      "                         'https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md\\n'\n",
      "                         '(added by Rileen Sinha)'},\n",
      "                {'question': 'I may end up submitting the assignment late. '\n",
      "                             'Would it be evaluated?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Depends on whether the form will still be open. If '\n",
      "                         \"you're lucky and it's open, you can submit your \"\n",
      "                         \"homework and it will be evaluated. if closed - it's \"\n",
      "                         'too late.\\n'\n",
      "                         '(Added by Rileen Sinha, based on answer by Alexey on '\n",
      "                         'Slack)'},\n",
      "                {'question': 'Does the github repository need to be public?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Yes. Whoever corrects the homework will only be able '\n",
      "                         'to access the link if the repository is public.\\n'\n",
      "                         '(added by Tano Bugelli)\\n'\n",
      "                         'How to install Conda environment in my local '\n",
      "                         'machine?\\n'\n",
      "                         'Which ide is recommended for machine learning?'},\n",
      "                {'question': 'How to use wget with Google Colab?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Install w get:\\n'\n",
      "                         '!which wget\\n'\n",
      "                         'Download data:\\n'\n",
      "                         '!wget -P /content/drive/My\\\\ Drive/Downloads/ URL\\n'\n",
      "                         '(added by Paulina Hernandez)'},\n",
      "                {'question': 'Features in scikit-learn?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Features (X) must always be formatted as a 2-D array '\n",
      "                         'to be accepted by scikit-learn.\\n'\n",
      "                         'Use reshape to reshape a 1D array to a 2D.\\n'\n",
      "                         '\\t\\t\\t\\t\\t\\t\\t(-Aileah) :>\\n'\n",
      "                         '(added by Tano\\n'\n",
      "                         \"filtered_df = df[df['ocean_proximity'].isin(['<1H \"\n",
      "                         \"OCEAN', 'INLAND'])]\\n\"\n",
      "                         '# Select only the desired columns\\n'\n",
      "                         'selected_columns = [\\n'\n",
      "                         \"'latitude',\\n\"\n",
      "                         \"'longitude',\\n\"\n",
      "                         \"'housing_median_age',\\n\"\n",
      "                         \"'total_rooms',\\n\"\n",
      "                         \"'total_bedrooms',\\n\"\n",
      "                         \"'population',\\n\"\n",
      "                         \"'households',\\n\"\n",
      "                         \"'median_income',\\n\"\n",
      "                         \"'median_house_value'\\n\"\n",
      "                         ']\\n'\n",
      "                         'filtered_df = filtered_df[selected_columns]\\n'\n",
      "                         '# Display the first few rows of the filtered '\n",
      "                         'DataFrame\\n'\n",
      "                         'print(filtered_df.head())'},\n",
      "                {'question': 'When I plotted using Matplot lib to check if '\n",
      "                             'median has a tail, I got the error below how can '\n",
      "                             'one bypass?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'FutureWarning: is_categorical_dtype is deprecated '\n",
      "                         'and will be removed in a future version. Use '\n",
      "                         'isinstance(dtype, CategoricalDtype) instead'},\n",
      "                {'question': 'Reproducibility in different OS',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'When trying to rerun the docker file in Windows, as '\n",
      "                         'opposed to developing in WSL/Linux, I got the error '\n",
      "                         'of:\\n'\n",
      "                         '```\\n'\n",
      "                         'Warning: Python 3.11 was not found on your system…\\n'\n",
      "                         'Neither ‘pipenv’ nor ‘asdf’ could be found to '\n",
      "                         'install Python.\\n'\n",
      "                         'You can specify specific versions of Python with:\\n'\n",
      "                         '$ pipenv –python path\\\\to\\\\python\\n'\n",
      "                         '```\\n'\n",
      "                         'The solution was to add Python311 installation '\n",
      "                         'folder to the PATH and restart the system and run '\n",
      "                         'the docker file again. That solved the error.\\n'\n",
      "                         '(Added by Abhijit Chakraborty)'},\n",
      "                {'question': 'Deploying to Digital Ocean',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'You may quickly deploy your project to DigitalOcean '\n",
      "                         'App Cloud. The process is relatively '\n",
      "                         'straightforward. The deployment costs about 5 '\n",
      "                         'USD/month. The container needs to be up until the '\n",
      "                         'end of the project evaluation.\\n'\n",
      "                         'Steps:\\n'\n",
      "                         'Register in DigitalOcean\\n'\n",
      "                         'Go to Apps -> Create App.\\n'\n",
      "                         'You will need to choose GitHub as a service '\n",
      "                         'provider.\\n'\n",
      "                         'Edit Source Directory (if your project is not in the '\n",
      "                         'repo root)\\n'\n",
      "                         'IMPORTANT: Go to settings -> App Spec and edit the '\n",
      "                         'Dockerfile path so it looks like '\n",
      "                         './project/Dockerfile path relative to your repo '\n",
      "                         'root\\n'\n",
      "                         'Remember to add model files if they are not built '\n",
      "                         'automatically during the container build process.\\n'\n",
      "                         'By Dmytro Durach'},\n",
      "                {'question': 'Is it best to train your model only on the most '\n",
      "                             'important features?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'I’m just looking back at the lessons in week 3 '\n",
      "                         '(churn prediction project), and lesson 3.6 talks '\n",
      "                         'about Feature Importance for categorical values. At '\n",
      "                         '8.12, the mutual info scores show that the some '\n",
      "                         'features are more important than others, but then in '\n",
      "                         'lesson 3.10 the Logistic Regression model is trained '\n",
      "                         'on all of the categorical variables (see 1:35). Once '\n",
      "                         'we have done feature importance, is it best to train '\n",
      "                         'your model only on the most important features?\\n'\n",
      "                         'Not necessarily - rather, any feature that can offer '\n",
      "                         'additional predictive value should be included (so, '\n",
      "                         'e.g. predict with & without including that feature; '\n",
      "                         'if excluding it drops performance, keep it, else '\n",
      "                         'drop it). A few individually important features '\n",
      "                         'might in fact be highly correlated with others, & '\n",
      "                         'dropping some might be fine. There are many feature '\n",
      "                         'selection algorithms, it might be interesting to '\n",
      "                         \"read up on them (among the methods we've learned so \"\n",
      "                         'far in this course, L1 regularization (Lasso) '\n",
      "                         'implicitly does feature selection by shrinking some '\n",
      "                         'weights all the way to zero).\\n'\n",
      "                         'By Rileen Sinha'},\n",
      "                {'question': 'How can I work with very large datasets, e.g. '\n",
      "                             'the New York Yellow Taxi dataset, with over a '\n",
      "                             'million rows?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'You can consider several different approaches:\\n'\n",
      "                         'Sampling: In the exploratory phase, you can use '\n",
      "                         'random samples of the data.\\n'\n",
      "                         'Chunking: When you do need all the data, you can '\n",
      "                         'read and process it in chunks that do fit in the '\n",
      "                         'memory.\\n'\n",
      "                         'Optimizing data types: Pandas’ automatic data type '\n",
      "                         'inference (when reading data in) might result in '\n",
      "                         'e.g. float64 precision being used to represent '\n",
      "                         'integers, which wastes space. You might achieve '\n",
      "                         'substantial memory reduction by optimizing the data '\n",
      "                         'types.\\n'\n",
      "                         'Using Dask, an open-source python project which '\n",
      "                         'parallelizes Numpy and Pandas.\\n'\n",
      "                         '(see, e.g. '\n",
      "                         'https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\\n'\n",
      "                         'By Rileen Sinha'},\n",
      "                {'question': 'Can I do the course in other languages, like R '\n",
      "                             'or Scala?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Technically, yes. Advisable? Not really. Reasons:\\n'\n",
      "                         'Some homework(s) asks for specific python library '\n",
      "                         'versions.\\n'\n",
      "                         'Answers may not match in MCQ options if using '\n",
      "                         'different languages other than Python 3.10 (the '\n",
      "                         'recommended version for 2023 cohort)\\n'\n",
      "                         'And as for midterms/capstones, your peer-reviewers '\n",
      "                         'may not know these other languages. Do you want to '\n",
      "                         'be penalized for others not knowing these other '\n",
      "                         'languages?\\n'\n",
      "                         'You can create a separate repo using course’s '\n",
      "                         'lessons but written in other languages for your own '\n",
      "                         'learnings, but not advisable for submissions.\\n'\n",
      "                         'tx[source]'},\n",
      "                {'question': 'Is use of libraries like fast.ai or huggingface '\n",
      "                             'allowed in the capstone and competition, or are '\n",
      "                             'they considered to be \"too much help\"?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'Yes, it’s allowed (as per Alexey).\\n'\n",
      "                         'Added By Rileen Sinha'},\n",
      "                {'question': 'Flask image was built and tested successfully, '\n",
      "                             'but tensorflow serving image was built and '\n",
      "                             'unable to test successfully. What could be the '\n",
      "                             'problem?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'The TF and TF Serving versions have to match (as per '\n",
      "                         'solution from the slack channel)\\n'\n",
      "                         'Added by Chiedu Elue'},\n",
      "                {'question': 'Any advice for adding the Machine Learning '\n",
      "                             'Zoomcamp experience to your LinkedIn profile?',\n",
      "                 'section': 'Miscellaneous',\n",
      "                 'text': 'I’ve seen LinkedIn users list DataTalksClub as '\n",
      "                         'Experience with titles as:\\n'\n",
      "                         'Machine Learning Fellow\\n'\n",
      "                         'Machine Learning Student\\n'\n",
      "                         'Machine Learning Participant\\n'\n",
      "                         'Machine Learning Trainee\\n'\n",
      "                         'Please note it is best advised that you do not list '\n",
      "                         'the experience as an official “job” or “internship” '\n",
      "                         'experience since DataTalksClub did not hire you, nor '\n",
      "                         'financially compensate you.\\n'\n",
      "                         'Other ways you can incorporate the experience in the '\n",
      "                         'following sections:\\n'\n",
      "                         'Organizations\\n'\n",
      "                         'Projects\\n'\n",
      "                         'Skills\\n'\n",
      "                         'Featured\\n'\n",
      "                         'Original posts\\n'\n",
      "                         'Certifications\\n'\n",
      "                         'Courses\\n'\n",
      "                         'By Annaliese Bronz\\n'\n",
      "                         'Interesting question, I put the link of my project '\n",
      "                         'into my CV as showcase and make posts to show my '\n",
      "                         'progress.\\n'\n",
      "                         'By Ani Mkrtumyan'}]},\n",
      " {'course': 'mlops-zoomcamp',\n",
      "  'documents': [{'question': 'Format for questions: [Problem title]',\n",
      "                 'section': '+-General course questions',\n",
      "                 'text': 'MLOps Zoomcamp FAQ\\n'\n",
      "                         'The purpose of this document is to capture '\n",
      "                         'frequently asked technical questions.\\n'\n",
      "                         'We did this for our data engineering course, and it '\n",
      "                         'worked quite well. Check this document for '\n",
      "                         'inspiration on how to structure your questions and '\n",
      "                         'answers:\\n'\n",
      "                         'Data Engineering Zoomcamp FAQ\\n'\n",
      "                         '[Problem description]\\n'\n",
      "                         '[Solution description]\\n'\n",
      "                         '(optional) Added by Name'},\n",
      "                {'question': 'What is the expected duration of this course or '\n",
      "                             'that for each module?',\n",
      "                 'section': '+-General course questions',\n",
      "                 'text': 'Approximately 3 months. For each module, about 1 '\n",
      "                         'week with possible deadline extensions (in total 6~9 '\n",
      "                         'weeks), 2 weeks for working on the capstone project '\n",
      "                         'and 1 week for peer review.'},\n",
      "                {'question': 'What’s the difference between the 2023 and 2022 '\n",
      "                             'course?',\n",
      "                 'section': '+-General course questions',\n",
      "                 'text': 'The difference is the Orchestration and Monitoring '\n",
      "                         'modules. Those videos will be re-recorded. The rest '\n",
      "                         'should mostly be the same.\\n'\n",
      "                         'Also all of the homeworks will be changed for the '\n",
      "                         '2023 cohort.'},\n",
      "                {'question': 'Will there be a 2024 Cohort? When will the 2024 '\n",
      "                             'cohort start?',\n",
      "                 'section': '+-General course questions',\n",
      "                 'text': 'Yes, it will start in May 2024'},\n",
      "                {'question': 'What if my answer is not exactly the same as the '\n",
      "                             'choices presented?',\n",
      "                 'section': '+-General course questions',\n",
      "                 'text': 'Please choose the closest one to your answer. Also '\n",
      "                         'do not post your answer in the course slack '\n",
      "                         'channel.'},\n",
      "                {'question': 'Are we free to choose our own topics for the '\n",
      "                             'final project?',\n",
      "                 'section': '+-General course questions',\n",
      "                 'text': 'Please pick up a problem you want to solve yourself. '\n",
      "                         'Potential datasets can be found on either Kaggle, '\n",
      "                         'Hugging Face, Google, AWS, or the UCI Machine '\n",
      "                         'Learning Datasets Repository.'},\n",
      "                {'question': 'Can I still graduate when I didn’t complete '\n",
      "                             'homework for week x?',\n",
      "                 'section': '+-General course questions',\n",
      "                 'text': 'In order to obtain the certificate, completion of '\n",
      "                         'the final capstone project is mandatory. The '\n",
      "                         'completion of weekly homework assignments is '\n",
      "                         'optional, but they can contribute to your overall '\n",
      "                         'progress and ranking on the top 100 leaderboard.'},\n",
      "                {'question': 'For the final project, is it required to be put '\n",
      "                             'on the cloud?',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'You can get a few cloud points by using kubernetes '\n",
      "                         'even if you deploy it only locally. Or you can use '\n",
      "                         'local stack too to mimic AWS\\n'\n",
      "                         'Added by Ming Jun, Asked by Ben Pacheco, Answered by '\n",
      "                         'Alexey Grigorev'},\n",
      "                {'question': 'Port-forwarding without Visual Studio',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'For those who are not using VSCode (or other similar '\n",
      "                         'IDE), you can automate port-forwarding for Jupyter '\n",
      "                         'Notebook by adding the following line of code to '\n",
      "                         'your\\n'\n",
      "                         '~/.ssh/config file (under the mlops-zoomcamp host):\\n'\n",
      "                         'LocalForward 127.0.0.1:8899 127.0.0.1:8899\\n'\n",
      "                         'Then you can launch Jupyter Notebook using the '\n",
      "                         'following command: jupyter notebook --port=8899 '\n",
      "                         '--no-browser and copy paste the notebook URL into '\n",
      "                         'your browser.\\n'\n",
      "                         'Added by Vishal'},\n",
      "                {'question': 'Opening Jupyter in VSCode',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'You can install the Jupyter extension to open '\n",
      "                         'notebooks in VSCode.\\n'\n",
      "                         'Added by Khubaib'},\n",
      "                {'question': 'Configuring Github to work from the remote VM',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'In case one would like to set a github repository '\n",
      "                         '(e.g. for Homeworks), one can follow 2 great '\n",
      "                         'tutorials that helped a lot\\n'\n",
      "                         'Setting up github on AWS instance - this\\n'\n",
      "                         'Setting up keys on AWS instance - this\\n'\n",
      "                         'Then, one should be able to push to its repo\\n'\n",
      "                         'Added by Daniel Hen (daniel8hen@gmail.com)'},\n",
      "                {'question': 'Opening Jupyter in AWS',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Faced issue while setting up JUPYTER NOTEBOOK on '\n",
      "                         'AWS. I was unable to access it from my desktop. (I '\n",
      "                         'am not using visual studio and hence faced problem)\\n'\n",
      "                         'Run\\n'\n",
      "                         'jupyter notebook --generate-config\\n'\n",
      "                         'Edit file '\n",
      "                         '/home/ubuntu/.jupyter/jupyter_notebook_config.py to '\n",
      "                         'add following line:\\n'\n",
      "                         \"NotebookApp.ip = '*'\\n\"\n",
      "                         'Added by Atul Gupta (samatul@gmail.com)'},\n",
      "                {'question': 'WSL instructions',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'If you wish to use WSL on your windows machine, here '\n",
      "                         'are the setup instructions:\\n'\n",
      "                         'Command: Sudo apt install wget\\n'\n",
      "                         'Get Anaconda download address here. wget <download '\n",
      "                         'address>\\n'\n",
      "                         'Turn on Docker Desktop WFree Download | AnacondaSL2\\n'\n",
      "                         'Command: git clone <github repository address>\\n'\n",
      "                         'VSCODE on WSL\\n'\n",
      "                         'Jupyter: pip3 install jupyter\\n'\n",
      "                         'Added by Gregory Morris (gwm1980@gmail.com)\\n'\n",
      "                         'All in all softwares at one shop:\\n'\n",
      "                         'You can use anaconda which has all built in services '\n",
      "                         'like pycharm, jupyter\\n'\n",
      "                         'Added by Khaja Zaffer '\n",
      "                         '(khajazaffer@aln.iseg.ulisboa.pt)\\n'\n",
      "                         'For windows “wsl --install” in Powershell\\n'\n",
      "                         'Added by Vadim Surin (vdmsurin@gmai.com)'},\n",
      "                {'question': '.gitignore how-to',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'If you create a folder data and download datasets or '\n",
      "                         'raw files in your local repository. Then to push all '\n",
      "                         'your code to remote repository without this files or '\n",
      "                         'folder please use gitignore file. The simple way to '\n",
      "                         'create it do the following steps\\n'\n",
      "                         '1. Create empty .txt file (using text editor or '\n",
      "                         'command line)\\n'\n",
      "                         '2. Safe as .gitignore (. must use the dot symbol)\\n'\n",
      "                         '3. Add rules\\n'\n",
      "                         ' *.parquet - to ignore all parquet files\\n'\n",
      "                         'data/ - to ignore all files in folder data\\n'\n",
      "                         '\\n'\n",
      "                         'For more pattern read GIT documentation\\n'\n",
      "                         'https://git-scm.com/docs/gitignore\\n'\n",
      "                         'Added by Olga Rudakova (olgakurgan@gmail.com)'},\n",
      "                {'question': 'AWS suggestions',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Make sure when you stop an EC2 instance that it '\n",
      "                         \"actually stops (there's a meme about it somewhere). \"\n",
      "                         'There are green circles (running), orange '\n",
      "                         '(stopping), and red (stopped). Always refresh the '\n",
      "                         'page to make sure you see the red circle and status '\n",
      "                         'of stopped.\\n'\n",
      "                         'Even when an EC2 instance is stopped, there WILL be '\n",
      "                         'other charges that are incurred (e.g. if you '\n",
      "                         'uploaded data to the EC2 instance, this data has to '\n",
      "                         'be stored somewhere, usually an EBS volume and this '\n",
      "                         'storage incurs a cost).\\n'\n",
      "                         \"You can set up billing alerts. (I've never done \"\n",
      "                         'this, so no advice on how to do this).\\n'\n",
      "                         '(Question by: Akshit Miglani '\n",
      "                         '(akshit.miglani09@gmail.com) and Answer by Anna '\n",
      "                         'Vasylytsya)'},\n",
      "                {'question': 'IBM Cloud an alternative for AWS',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'You can get invitation code by coursera and use it '\n",
      "                         'in account to verify it it has different '\n",
      "                         'characteristics.\\n'\n",
      "                         'I really love it\\n'\n",
      "                         'https://www.youtube.com/watch?v=h_GdX6KtXjo'},\n",
      "                {'question': 'AWS costs',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'I am worried about the cost of keeping an AWS '\n",
      "                         'instance running during the course.\\n'\n",
      "                         'With the instance specified during working '\n",
      "                         'environment setup, if you remember to Stop Instance '\n",
      "                         'once you finished your work for the day.  Using that '\n",
      "                         'strategy, in a day with about 5 hours of work you '\n",
      "                         'will pay around $0.40 USD which will account for $12 '\n",
      "                         'USD per month, which seems to be an affordable '\n",
      "                         'amount.\\n'\n",
      "                         'You must remember that you would have a different IP '\n",
      "                         'public address every time you Restart your instance, '\n",
      "                         'and you would need to edit your ssh Config file.  '\n",
      "                         \"It's worth the time though.\\n\"\n",
      "                         'Additionally, AWS enables you to set up an automatic '\n",
      "                         'email alert if a predefined budget is exceeded.\\n'\n",
      "                         'Here is a tutorial to set this up.\\n'\n",
      "                         'Also, you can estimate the cost yourself, using AWS '\n",
      "                         'pricing calculator (to use it you don’t even need to '\n",
      "                         'be logged in).\\n'\n",
      "                         'At the time of writing (20.05.2023) t3a.xlarge '\n",
      "                         'instance with 2 hr/day usage (which translates to 10 '\n",
      "                         'hr/week that should be enough to complete the '\n",
      "                         'course) and 30GB EBS monthly cost is 10.14 USD\\n'\n",
      "                         'Here’s a link to the estimate\\n'\n",
      "                         'Added by Alex Litvinov (aaalex.lit@gmail.com)'},\n",
      "                {'question': 'Is the AWS free tier enough for doing this '\n",
      "                             'course?',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'For many parts - yes. Some things like kinesis are '\n",
      "                         'not in AWS free tier, but you can do it locally with '\n",
      "                         'localstack.'},\n",
      "                {'question': 'AWS EC2: this site can’t be reached',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'When I click an open IP-address in an AWS EC2 '\n",
      "                         'instance I get an error: “This site can’t be '\n",
      "                         'reached”. What should I do?\\n'\n",
      "                         'This ip-address is not required to be open in a '\n",
      "                         'browser. It is needed to connect to the running EC2 '\n",
      "                         'instance via terminal from your local machine or via '\n",
      "                         'terminal from a remote server with such command, for '\n",
      "                         'example if:\\n'\n",
      "                         'ip-address is 11.111.11.111\\n'\n",
      "                         'downloaded key name is razer.pem (the key should be '\n",
      "                         'moved to a hidden folder .ssh)\\n'\n",
      "                         'your user name is user_name\\n'\n",
      "                         'ssh -i /Users/user_name/.ssh/razer.pem '\n",
      "                         'ubuntu@11.111.11.111'},\n",
      "                {'question': 'Unprotected private key file!',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'After this command `ssh -i ~/.ssh/razer.pem '\n",
      "                         'ubuntu@XX.XX.XX.XX` I got this error: \"unprotected '\n",
      "                         'private key file\". This page '\n",
      "                         '(https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/) '\n",
      "                         'explains how to fix this error. Basically you need '\n",
      "                         'to change the file permissions of the key file with '\n",
      "                         'this command: chmod 400 ~/.ssh/razer.pem'},\n",
      "                {'question': 'AWS EC2 instance constantly drops SSH connection',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'My SSH connection to AWS cannot last more than a few '\n",
      "                         'minutes, whether via terminal or VS code.\\n'\n",
      "                         'My config:\\n'\n",
      "                         '# Copy Configuration in local nano editor, then Save '\n",
      "                         'it!\\n'\n",
      "                         'Host '\n",
      "                         'mlops-zoomcamp                                         '\n",
      "                         '# ssh connection calling name\\n'\n",
      "                         'User '\n",
      "                         'ubuntu                                             # '\n",
      "                         'username AWS EC2\\n'\n",
      "                         'HostName '\n",
      "                         '<instance-public-IPv4-addr>                    # '\n",
      "                         'Public IP, it changes when Source EC2 is turned '\n",
      "                         'off.\\n'\n",
      "                         'IdentityFile '\n",
      "                         '~/.ssh/name-of-your-private-key-file.pem   # Private '\n",
      "                         'SSH key file path\\n'\n",
      "                         'LocalForward 8888 '\n",
      "                         'localhost:8888                        # Connecting '\n",
      "                         'to a service on an internal network from the '\n",
      "                         'outside, static forward or set port user forward via '\n",
      "                         'on vscode\\n'\n",
      "                         'StrictHostKeyChecking no\\n'\n",
      "                         'Added by Muhammed Çelik\\n'\n",
      "                         'The disconnection will occur whether I SSH via WSL2 '\n",
      "                         'or via VS Code, and usually occurs after I run some '\n",
      "                         'code, i.e. “import mlflow”, so not particularly '\n",
      "                         'intense computation.\\n'\n",
      "                         'I cannot reconnect to the instance without stopping '\n",
      "                         'and restarting with a new IPv4 address.\\n'\n",
      "                         'I’ve gone through steps listed on this page: '\n",
      "                         'https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/\\n'\n",
      "                         'Inbound rule should allow all incoming IPs for SSH.\\n'\n",
      "                         'What I expect to happen:\\n'\n",
      "                         'SSH connection should remain while I’m actively '\n",
      "                         'using the instance, and if it does disconnect, I '\n",
      "                         'should be able to reconnect back.\\n'\n",
      "                         'Solution: sometimes the hang ups are caused by the '\n",
      "                         'instance running out of memory. In one instance, '\n",
      "                         'using EC2 feature to view screenshot of the instance '\n",
      "                         'as a means to troubleshoot, it was the OS '\n",
      "                         'out-of-memory feature which killed off some critical '\n",
      "                         'processes. In this case, if we can’t use a higher '\n",
      "                         'compute VM with more RAM, try adding a swap file, '\n",
      "                         'which uses the disk as RAM substitute and prevents '\n",
      "                         'the OOM error. Follow Ubuntu’s documentation here: '\n",
      "                         'https://help.ubuntu.com/community/SwapFaq.\\n'\n",
      "                         'Alternatively follow AWS’s own doc, which mirrors '\n",
      "                         'Ubuntu’s: '\n",
      "                         'https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/'},\n",
      "                {'question': 'AWS EC2 IP Update',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Everytime I restart my EC2 instance I keep getting '\n",
      "                         'different IP and need to update the config file '\n",
      "                         'manually.\\n'\n",
      "                         '\\n'\n",
      "                         'Solution: You can create a script like this to '\n",
      "                         'automatically update the IP address of your EC2 '\n",
      "                         'instance.https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_1/update_ssh_config.md'},\n",
      "                {'question': 'VS Code crashes when connecting to Jupyter',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Make sure to use an instance with enough compute '\n",
      "                         'capabilities such as a t2.xlarge. You can check the '\n",
      "                         'monitoring tab in the EC2 dashboard to monitor your '\n",
      "                         'instance.'},\n",
      "                {'question': 'X has 526 features, but expecting 525 features',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Error “ValueError: X has 526 features, but '\n",
      "                         'LinearRegression is expecting 525 features as '\n",
      "                         'input.” when running your Linear Regression Model on '\n",
      "                         'the validation data set:\\n'\n",
      "                         'Solution: The DictVectorizer creates an initial '\n",
      "                         'mapping for the features (columns). When calling the '\n",
      "                         'DictVecorizer again for the validation dataset '\n",
      "                         'transform should be used as it will ignore features '\n",
      "                         'that it did not see when fit_transform was last '\n",
      "                         'called. E.g.\\n'\n",
      "                         'X_train = dv.fit_transform(train_dict)\\n'\n",
      "                         'X_test = dv.transform(test_dict)'},\n",
      "                {'question': 'Missing dependencies',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'If some dependencies are missing\\n'\n",
      "                         'Install following packages\\n'\n",
      "                         'pandas\\n'\n",
      "                         'matplotlib\\n'\n",
      "                         'scikit-learn\\n'\n",
      "                         'fastparquet\\n'\n",
      "                         'pyarrow\\n'\n",
      "                         'seaborn\\n'\n",
      "                         'pip install -r requirements.txt\\n'\n",
      "                         'I have seen this error when using '\n",
      "                         'pandas.read_parquet(), the solution is to install '\n",
      "                         'pyarrow or fastparquet by doing !pip install pyarrow '\n",
      "                         'in the notebook\\n'\n",
      "                         'NOTE: if you’re using Conda instead of pip, install '\n",
      "                         'fastparquet rather than pyarrow, as it is much '\n",
      "                         'easier to install and it’s functionally identical to '\n",
      "                         'pyarrow for our needs.'},\n",
      "                {'question': 'No RMSE value in the options',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'The evaluation RMSE I get doesn’t figure within the '\n",
      "                         'options!\\n'\n",
      "                         'If you’re evaluating the model on the entire '\n",
      "                         'February data, try to filter outliers using the same '\n",
      "                         'technique you used on the train data (0≤duration≤60) '\n",
      "                         'and you’ll get a RMSE which is (approximately) in '\n",
      "                         'the options. Also don’t forget to convert the '\n",
      "                         'columns data types to str before using the '\n",
      "                         'DictVectorizer.\\n'\n",
      "                         'Another option: Along with filtering outliers, '\n",
      "                         'additionally filter on null values by replacing them '\n",
      "                         'with -1.  You will get a RMSE which is (almost same '\n",
      "                         'as) in the options. Use ‘.round(2)’ method to round '\n",
      "                         'it to 2 decimal points.\\n'\n",
      "                         'Warning deprecation\\n'\n",
      "                         'The python interpreter warning of modules that have '\n",
      "                         'been deprecated  and will be removed in future '\n",
      "                         'releases as well as making suggestion how to go '\n",
      "                         'about your code.\\n'\n",
      "                         'For example\\n'\n",
      "                         'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\seaborn\\\\distributions.py:2619:\\n'\n",
      "                         'FutureWarning: `distplot` is a deprecated function '\n",
      "                         'and will be removed in a future version. Please '\n",
      "                         'adapt your code to use either `displot` (a '\n",
      "                         'figure-level function with similar flexibility) or '\n",
      "                         '`histplot` (an axes-level function for histograms).\\n'\n",
      "                         'warnings.warn(msg, FutureWarning)\\n'\n",
      "                         'To suppress the warnings, you can include this code '\n",
      "                         'at the beginning of your notebook\\n'\n",
      "                         'import warnings\\n'\n",
      "                         'warnings.filterwarnings(\"ignore\")'},\n",
      "                {'question': 'How to replace distplot with histplot',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'sns.distplot(df_train[\"duration\"])\\n'\n",
      "                         'Can be replaced with\\n'\n",
      "                         'sns.histplot(\\n'\n",
      "                         'df_train[\"duration\"] , kde=True,\\n'\n",
      "                         'stat=\"density\", kde_kws=dict(cut=3), bins=50,\\n'\n",
      "                         'alpha=.4, edgecolor=(1, 1, 1, 0.4),\\n'\n",
      "                         ')\\n'\n",
      "                         'To get almost identical result'},\n",
      "                {'question': \"KeyError: 'PULocationID'  or  'DOLocationID'\",\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'You need to replace the capital letter “L” with a '\n",
      "                         'small one “l”'},\n",
      "                {'question': 'Reading large parquet files',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'I have faced a problem while reading the large '\n",
      "                         'parquet file. I tried some workarounds but they were '\n",
      "                         'NOT successful with Jupyter.\\n'\n",
      "                         'The error message is:\\n'\n",
      "                         'IndexError: index 311297 is out of bounds for axis 0 '\n",
      "                         'with size 131743\\n'\n",
      "                         'I solved it by performing the homework directly as a '\n",
      "                         'python script.\\n'\n",
      "                         'Added by Ibraheem Taha (ibraheemtaha91@gmail.com)\\n'\n",
      "                         'You can try using the Pyspark library\\n'\n",
      "                         'Answered by kamaldeen (kamaldeen32@gmail.com)'},\n",
      "                {'question': 'Distplot takes too long',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'First remove the outliers (trips with unusual '\n",
      "                         'duration) before plotting\\n'\n",
      "                         'Added by Ibraheem Taha (ibraheemtaha91@gmail.com)'},\n",
      "                {'question': 'RMSE on test set too high',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Problem: RMSE on test set was too high when hot '\n",
      "                         'encoding the validation set with a previously fitted '\n",
      "                         'OneHotEncoder(handle_unknown=’ignore’) on the '\n",
      "                         'training set, while DictVectorizer would yield the '\n",
      "                         'correct RMSE.\\n'\n",
      "                         'In principle both transformers should behave '\n",
      "                         'identically when treating categorical features (at '\n",
      "                         'least in this week’s homework where we don’t have '\n",
      "                         'sequences of strings in each row):\\n'\n",
      "                         'Features are put into binary columns encoding their '\n",
      "                         'presence (1) or absence (0)\\n'\n",
      "                         'Unknown categories are imputed as zeroes in the '\n",
      "                         'hot-encoded matrix'},\n",
      "                {'question': 'Q: Using of OneHotEncoder instead of '\n",
      "                             'DictVectorizer',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'A: Alexey’s answer '\n",
      "                         'https://www.youtube.com/watch?v=8uJ36ZZr_Is&t=13s\\n'\n",
      "                         'In summary,\\n'\n",
      "                         'pd.get_dummies or OHE can come up with result in '\n",
      "                         'different orders and handle missing data '\n",
      "                         'differently, so train and val set would have '\n",
      "                         'different columns during train and validation\\n'\n",
      "                         'DictVectorizer would ignore missing (in train) and '\n",
      "                         'new (in val) datasets\\n'\n",
      "                         'Other sources:\\n'\n",
      "                         'https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\\n'\n",
      "                         'https://scikit-learn.org/stable/modules/feature_extraction.html\\n'\n",
      "                         'https://innovation.alteryx.com/encode-smarter/\\n'\n",
      "                         '~ ellacharmed'},\n",
      "                {'question': 'Q: Why did we not use OneHotEncoder(sklearn) '\n",
      "                             'instead of DictVectorizer ?',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': \"Why didn't get_dummies in pandas library or \"\n",
      "                         'OneHotEncoder in scikit-learn library be used for '\n",
      "                         'one-hot encoding? I know OneHotEncoder is the most '\n",
      "                         'common and useful. One-hot coding can also be done '\n",
      "                         'using the eye or identity components of the NumPy '\n",
      "                         'library.\\n'\n",
      "                         'M.Sari\\n'\n",
      "                         'OneHotEncoder has the option to output a row column '\n",
      "                         'tuple matrix. DictVectorizer is a one step method to '\n",
      "                         'encode and support row column tuple matrix output.\\n'\n",
      "                         'Harinder(sudwalh@gmail.com)'},\n",
      "                {'question': 'Clipping outliers',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'How to check that we removed the outliers?\\n'\n",
      "                         'Use the pandas function describe() which can provide '\n",
      "                         'a report of the data distribution along with the '\n",
      "                         'statistics to describe the data. For example, after '\n",
      "                         'clipping the outliers using boolean expression, the '\n",
      "                         'min and max can be verified using\\n'\n",
      "                         'df[‘duration’].describe()'},\n",
      "                {'question': 'Replacing NaNs for pickup location and drop off '\n",
      "                             'location with -1 for One-Hot Encoding',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'pd.get_dummies and DictVectorizer both create a '\n",
      "                         'one-hot encoding on string values. Therefore you '\n",
      "                         'need to convert the values in PUlocationID and '\n",
      "                         'DOlocationID to string.\\n'\n",
      "                         'If you convert the values in PUlocationID and '\n",
      "                         'DOlocationID from numeric to string, the NaN values '\n",
      "                         'get converted to the string \"nan\".  With '\n",
      "                         'DictVectorizer the RMSE is the same whether you use '\n",
      "                         '\"nan\" or \"-1\" as string representation for the NaN '\n",
      "                         \"values. Therefore the representation doesn't have to \"\n",
      "                         'be \"-1\" specifically, it could also be some other '\n",
      "                         'string.'},\n",
      "                {'question': 'Slightly different RSME',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Problem: My LinearRegression RSME is very close to '\n",
      "                         'the answer but not exactly the same. Is this '\n",
      "                         'normal?\\n'\n",
      "                         'Answer: No, LinearRegression is an deterministic '\n",
      "                         'model, it should always output the same results when '\n",
      "                         'given the same inputs.\\n'\n",
      "                         'Answer:\\n'\n",
      "                         'Check if you have treated the outlier properly for '\n",
      "                         'both train and validation sets\\n'\n",
      "                         'Check if the one hot encoding has been done properly '\n",
      "                         'by looking at the shape of one hot encoded feature '\n",
      "                         'matrix. If it shows 2 features, there is wrong with '\n",
      "                         'one hot encoding. Hint: the drop off and pick up '\n",
      "                         'codes need to be converted to proper data format and '\n",
      "                         'then DictVectorizer is fitted.\\n'\n",
      "                         'Harshit Lamba (hlamba19@gmail.com)'},\n",
      "                {'question': 'Extremely low RSME',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Problem: I’m facing an extremely low RMSE score (eg: '\n",
      "                         '4.3451e-6) - what shall I do?\\n'\n",
      "                         'Answer: Recheck your code to see if your model is '\n",
      "                         'learning the target prior to making the prediction. '\n",
      "                         'If the target variable is passed in as a parameter '\n",
      "                         'while fitting the model, chances are the model would '\n",
      "                         'score extremely low. However, that’s not what you '\n",
      "                         'would want and would much like to have your model '\n",
      "                         'predict that. A good way to check that is to make '\n",
      "                         'sure your X_train doesn’t contain any part of your '\n",
      "                         'y_train. The same stands for validation too.\\n'\n",
      "                         'Snehangsu De (desnehangsu@gmail.com)'},\n",
      "                {'question': 'Enabling Auto-completion in jupyter notebook',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Problem: how to enable auto completion in jupyter '\n",
      "                         'notebook? Tab doesn’t work for me\\n'\n",
      "                         'Solution: !pip install --upgrade jedi==0.17.2\\n'\n",
      "                         'Christopher R.J.(romanjaimesc@gmail.com)'},\n",
      "                {'question': 'Downloading the data from the NY Taxis datasets '\n",
      "                             'gives error : 403 Forbidden',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Problem: While following the steps in the videos you '\n",
      "                         'may have problems trying to download with wget the '\n",
      "                         'files. Usually it is a 403 error type (Forbidden '\n",
      "                         'access).\\n'\n",
      "                         'Solution: The links point to files on '\n",
      "                         'cloudfront.net, something like this:\\n'\n",
      "                         'https://d37ci6vzurychx.cloudfront.net/tOSError: '\n",
      "                         \"Could not open parquet input source '<Buffer>': \"\n",
      "                         'Invalid: Parquet OSError: Could not open parquet '\n",
      "                         \"input source '<Buffer>': Invalid: Parquet \"\n",
      "                         'rip+data/green_tripdata_2021-01.parquet\\n'\n",
      "                         'I’m not download the dataset directly, i use dataset '\n",
      "                         'URL and run this in the file.\\n'\n",
      "                         'Update(27-May-2023): Vikram\\n'\n",
      "                         'I am able to download the data from the below link. '\n",
      "                         'This is from the official  NYC trip record page '\n",
      "                         '(https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). '\n",
      "                         'Copy link from page directly as the below url might '\n",
      "                         'get changed if the NYC decides to move away from '\n",
      "                         'this. Go to the page , right click and use copy '\n",
      "                         'link.\\n'\n",
      "                         'wget '\n",
      "                         'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\\n'\n",
      "                         '(Asif)\\n'\n",
      "                         'Copy the link address and replace the cloudfront.net '\n",
      "                         'part with s3.amazonaws.com/nyc-tlc/, so it looks '\n",
      "                         'like this:\\n'\n",
      "                         'https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.parquet\\n'\n",
      "                         'Mario Tormo (mario@tormo-romero.eu)\\n'\n",
      "                         'OSError: Could not open parquet input source '\n",
      "                         \"'<Buffer>': Invalid: Parquet\"},\n",
      "                {'question': 'Using PyCharm & Conda env in remote development',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Problem: PyCharm (remote) doesn’t see conda '\n",
      "                         'execution path. So, I cannot use conda env (which is '\n",
      "                         'located on a remote server).\\n'\n",
      "                         'Solution: In remote server in command line write '\n",
      "                         '“conda activate envname”, after write “which python” '\n",
      "                         '- it gives you python execution path. After you can '\n",
      "                         'use this path when you will add new interpreter in '\n",
      "                         'PyCharm: add local interpreter -> system interpreter '\n",
      "                         '-> and put the path with python.\\n'\n",
      "                         'Salimov Ilnaz (salimovilnaz777@gmail.com)'},\n",
      "                {'question': 'Running out of memory',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Problem: The output of DictVectorizer was taking up '\n",
      "                         'too much memory. So much so, that I couldn’t even '\n",
      "                         'fit the linear regression model before running out '\n",
      "                         'of memory on my 16 GB machine.\\n'\n",
      "                         'Solution: In the example for DictVectorizer in the '\n",
      "                         'scikit-learn website, they set the parameter '\n",
      "                         '“sparse” as False. Although this helps with viewing '\n",
      "                         'the results, this results in a lot of memory usage. '\n",
      "                         'The solution is to either use “sparse=True” instead, '\n",
      "                         'or leave it at the default which is also True.\\n'\n",
      "                         'Ahmed Fahim (afahim03@yahoo.com)'},\n",
      "                {'question': 'Activating Anaconda env in .bashrc',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Problem: For me, Installing anaconda didn’t modify '\n",
      "                         'the .bashrc profile. That means Anaconda env was not '\n",
      "                         'activated even after exiting and relaunching the '\n",
      "                         'unix shell.\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'For bash : Initiate conda again, which will add '\n",
      "                         'entries for anaconda in .bashrc file.\\n'\n",
      "                         '$ cd YOUR_PATH_ANACONDA/bin $ ./conda init bash\\n'\n",
      "                         'That will automatically edit your .bashrc.\\n'\n",
      "                         'Reload:\\n'\n",
      "                         '$ source ~/.bashrc\\n'\n",
      "                         'Ahamed Irshad (daisyfuentesahamed@gmail.com)'},\n",
      "                {'question': 'The feature size is different for training set '\n",
      "                             'and validation set',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'While working through the HW1, you will realize that '\n",
      "                         'the training and the validation data set feature '\n",
      "                         'sizes are different. I was trying to figure out why '\n",
      "                         'and went down the entire rabbit hole only to see '\n",
      "                         'that I wasn’t doing ```transform``` on the premade '\n",
      "                         'dictionary vectorizer instead of '\n",
      "                         '```fit_transform```. You already have the dictionary '\n",
      "                         'vectorizer made so no need to execute the fit '\n",
      "                         'pipeline on the model.\\n'\n",
      "                         'Sam Lim(changhyeonlim@gmail.com)'},\n",
      "                {'question': 'Permission denied (publickey) Error (when you '\n",
      "                             'remove your public key on the AWS machine)',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'I found a good guide how to get acces to your '\n",
      "                         'machine again when you removed your public key.\\n'\n",
      "                         'Using the following link you can go to Session '\n",
      "                         'Manager and log in to your instance and create '\n",
      "                         'public key again. '\n",
      "                         'https://repost.aws/knowledge-center/ec2-linux-fix-permission-denied-errors\\n'\n",
      "                         'The main problem for me here was to get my old '\n",
      "                         'public key, so for doing this you should run the '\n",
      "                         'following command: ssh-keygen -y -f '\n",
      "                         '/path_to_key_pair/my-key-pair.pem\\n'\n",
      "                         'For more information: '\n",
      "                         'https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/describe-keys.html#retrieving-the-public-key\\n'\n",
      "                         'Hanna Zhukavets (a.zhukovec1901@gmail.com)'},\n",
      "                {'question': 'Overfitting: Absurdly high RMSE on the '\n",
      "                             'validation dataset',\n",
      "                 'section': 'Module 1: Introduction',\n",
      "                 'text': 'Problem: The February dataset has been used as a '\n",
      "                         'validation/test dataset and been stripped of the '\n",
      "                         'outliers in a similar manner to the train dataset '\n",
      "                         '(taking only the rows for the duration between 1 and '\n",
      "                         '60, inclusive). The RMSE obtained afterward is in '\n",
      "                         'the thousands.\\n'\n",
      "                         'Answer: The sparsematrix result from DictVectorizer '\n",
      "                         'shouldn’t be turned into an ndarray. After removing '\n",
      "                         'that part of the code, I ended up receiving a '\n",
      "                         'correct result .\\n'\n",
      "                         'Tahina Mahatoky (tahinadanny@gmail.com)'},\n",
      "                {'question': 'Can’t import sklearn',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'more specific error line:\\n'\n",
      "                         'from sklearn.feature_extraction import '\n",
      "                         'DictVectorizer\\n'\n",
      "                         'I had this issue and to solve it I did\\n'\n",
      "                         '!pip install scikit-learn\\n'\n",
      "                         'Joel Auccapuclla (auccapuclla 2013@gmail.com)'},\n",
      "                {'question': 'Access Denied at Localhost:5000 - Authorization '\n",
      "                             'Issue',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: Localhost:5000 Unavailable // Access to '\n",
      "                         'Localhost Denied // You don’t have authorization to '\n",
      "                         'view this page (127.0.0.1:5000)\\n'\n",
      "                         '\\n'\n",
      "                         'Solution: If you are on an chrome browser you need '\n",
      "                         'to head to `chrome://net-internals/#sockets` and '\n",
      "                         'press “Flush Socket Pools”'},\n",
      "                {'question': \"Connection in use: ('127.0.0.1', 5000)\",\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'You have something running on the 5000 port. You '\n",
      "                         'need to stop it.\\n'\n",
      "                         'Answer: On terminal in mac .\\n'\n",
      "                         'Run ps -A | grep gunicorn\\n'\n",
      "                         'Look for the number process id which is the 1st '\n",
      "                         'number after running the command\\n'\n",
      "                         'kill 13580\\n'\n",
      "                         'where 13580  represents the process number.\\n'\n",
      "                         'Source\\n'\n",
      "                         'warrie.warrieus@gmail.com\\n'\n",
      "                         'Or by executing the following command it will kill '\n",
      "                         'all the processes using port 5000:\\n'\n",
      "                         '>> sudo fuser -k 5000/tcp\\n'\n",
      "                         'Answered by Vaibhav Khandelwal\\n'\n",
      "                         'Just execute in the command below in he command line '\n",
      "                         'to kill the running port\\n'\n",
      "                         \"->> kill -9 $(ps -A | grep python | awk '{print \"\n",
      "                         \"$1}')\\n\"\n",
      "                         'Answered by kamaldeen (kamaldeen32@gmail.com)\\n'\n",
      "                         'Change to different port (5001 in this case)\\n'\n",
      "                         '>> mlflow ui --backend-store-uri sqlite:///mlflow.db '\n",
      "                         '--port 5001\\n'\n",
      "                         'Answered by krishna (nellaikrishna@gmail.com)'},\n",
      "                {'question': 'Could not convert string to float - ValueError',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Running python register_model.py results in the '\n",
      "                         'following error:\\n'\n",
      "                         \"ValueError: could not convert string to float: '0 \"\n",
      "                         'int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       '\n",
      "                         'Literal{n_estimators}\\\\n4       quniform\\\\n5         '\n",
      "                         'Literal{10}\\\\n6         Literal{50}\\\\n7         '\n",
      "                         \"Literal{1}'\\n\"\n",
      "                         'Full Traceback:\\n'\n",
      "                         'Traceback (most recent call last):\\n'\n",
      "                         'File '\n",
      "                         '\"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. '\n",
      "                         'Experiment tracking and model '\n",
      "                         'management/homework/scripts/register_model.py\", line '\n",
      "                         '101, in <module>\\n'\n",
      "                         'run(args.data_path, args.top_n)\\n'\n",
      "                         'File '\n",
      "                         '\"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. '\n",
      "                         'Experiment tracking and model '\n",
      "                         'management/homework/scripts/register_model.py\", line '\n",
      "                         '67, in run\\n'\n",
      "                         'train_and_log_model(data_path=data_path, '\n",
      "                         'params=run.data.params)\\n'\n",
      "                         'File '\n",
      "                         '\"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. '\n",
      "                         'Experiment tracking and model '\n",
      "                         'management/homework/scripts/register_model.py\", line '\n",
      "                         '41, in train_and_log_model\\n'\n",
      "                         'params = space_eval(SPACE, params)\\n'\n",
      "                         'File '\n",
      "                         '\"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/fmin.py\", '\n",
      "                         'line 618, in space_eval\\n'\n",
      "                         'rval = pyll.rec_eval(space, memo=memo)\\n'\n",
      "                         'File '\n",
      "                         '\"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/pyll/base.py\", '\n",
      "                         'line 902, in rec_eval\\n'\n",
      "                         'rval = scope._impls[node.name](*args, **kwargs)\\n'\n",
      "                         \"ValueError: could not convert string to float: '0 \"\n",
      "                         'int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       '\n",
      "                         'Literal{n_estimators}\\\\n4       quniform\\\\n5         '\n",
      "                         'Literal{10}\\\\n6         Literal{50}\\\\n7         '\n",
      "                         \"Literal{1}'\\n\"\n",
      "                         'Solution: There are two plausible errors to this. '\n",
      "                         'Both are in the hpo.py file where the '\n",
      "                         'hyper-parameter tuning is run. The objective '\n",
      "                         'function should look like this.\\n'\n",
      "                         '\\n'\n",
      "                         '   def objective(params):\\n'\n",
      "                         '# It\\'s important to set the \"with\" statement and '\n",
      "                         'the \"log_params\" function here\\n'\n",
      "                         '# in order to properly log all the runs and '\n",
      "                         'parameters.\\n'\n",
      "                         'with mlflow.start_run():\\n'\n",
      "                         '# Log the parameters\\n'\n",
      "                         'mlflow.log_params(params)\\n'\n",
      "                         'rf = RandomForestRegressor(**params)\\n'\n",
      "                         'rf.fit(X_train, y_train)\\n'\n",
      "                         'y_pred = rf.predict(X_valid)\\n'\n",
      "                         '# Calculate and log rmse\\n'\n",
      "                         'rmse = mean_squared_error(y_valid, y_pred, '\n",
      "                         'squared=False)\\n'\n",
      "                         \"mlflow.log_metric('rmse', rmse)\\n\"\n",
      "                         'If you add the with statement before this function, '\n",
      "                         'and just after the following line\\n'\n",
      "                         'X_valid, y_valid = '\n",
      "                         'load_pickle(os.path.join(data_path, \"valid.pkl\"))\\n'\n",
      "                         'and you log the parameters just after the '\n",
      "                         'search_space dictionary is defined, like this\\n'\n",
      "                         'search_space = {....}\\n'\n",
      "                         '# Log the parameters\\n'\n",
      "                         'mlflow.log_params(search_space)\\n'\n",
      "                         'Then there is a risk that the parameters will be '\n",
      "                         'logged in group. As a result, the\\n'\n",
      "                         'params = space_eval(SPACE, params)\\n'\n",
      "                         'register_model.py file will receive the parameters '\n",
      "                         'in group, while in fact it expects to receive them '\n",
      "                         'one by one. Thus, make sure that the objective '\n",
      "                         'function looks as above.\\n'\n",
      "                         'Added by Jakob Salomonsson'},\n",
      "                {'question': 'Experiment not visible in MLflow UI',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Make sure you launch the mlflow UI from the same '\n",
      "                         'directory as thec that is running the experiments '\n",
      "                         '(same directory that has the mlflow directory and '\n",
      "                         'the database that stores the experiments).\\n'\n",
      "                         'Or navigate to the correct directory when specifying '\n",
      "                         'the tracking_uri.\\n'\n",
      "                         'For example:\\n'\n",
      "                         'If the mlflow.db is in a subdirectory called '\n",
      "                         'database, the tracking uri would be '\n",
      "                         '‘sqllite:///database/mlflow.db’\\n'\n",
      "                         'If the mlflow.db is a directory above your current '\n",
      "                         'directory: the tracking uri would be '\n",
      "                         '‘sqlite:///../mlflow.db’\\n'\n",
      "                         'Answered by Anna Vasylytsya\\n'\n",
      "                         'Another alternative is to use an absolute path to '\n",
      "                         'mlflow.db rather than relative path\\n'\n",
      "                         'And yet another alternative is to launch the UI from '\n",
      "                         'the same notebook by executing the following code '\n",
      "                         'cell\\n'\n",
      "                         'import subprocess\\n'\n",
      "                         'MLFLOW_TRACKING_URI = \"sqlite:///data/mlflow.db\"\\n'\n",
      "                         'subprocess.Popen([\"mlflow\", \"ui\", '\n",
      "                         '\"--backend-store-uri\", MLFLOW_TRACKING_URI])\\n'\n",
      "                         'And then using the same MLFLOW_TRACKING_URI when '\n",
      "                         'initializing mlflow or the client\\n'\n",
      "                         'client = '\n",
      "                         'MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\\n'\n",
      "                         'mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)'},\n",
      "                {'question': 'Hash Mismatch Error with Package Installation',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem:\\n'\n",
      "                         'Getting\\n'\n",
      "                         'ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM '\n",
      "                         'THE REQUIREMENTS FILE\\n'\n",
      "                         \"during MLFlow's installation process, particularly \"\n",
      "                         'while installing the Numpy package using pip\\n'\n",
      "                         'When I installed mlflow using ‘pip install mlflow’ '\n",
      "                         'on 27th May 2022, I got the following error while '\n",
      "                         'numpy was getting installed through mlflow:\\n'\n",
      "                         '\\n'\n",
      "                         'Collecting numpy\\n'\n",
      "                         'Downloading numpy-1.22.4-cp310-cp310-win_amd64.whl '\n",
      "                         '(14.7 MB)\\n'\n",
      "                         '|██████████████              \\t| 6.3 MB 107 kB/s eta '\n",
      "                         '0:01:19\\n'\n",
      "                         'ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM '\n",
      "                         'THE REQUIREMENTS FILE.\\n'\n",
      "                         'If you have updated the package versions, please '\n",
      "                         'update the hashes. Otherwise, examine the package '\n",
      "                         'contents carefully; someone may have tampered with '\n",
      "                         'them.\\n'\n",
      "                         'numpy from '\n",
      "                         'https://files.pythonhosted.org/packages/b5/50/d7978137464251c393df28fe0592fbb968110f752d66f60c7a53f7158076/numpy-1.22.4-cp310-cp310-win_amd64.whl#sha256=3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 '\n",
      "                         '(from mlflow):\\n'\n",
      "                         'Expected sha256 '\n",
      "                         '3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077\\n'\n",
      "                         'Got    \\t'\n",
      "                         '15e691797dba353af05cf51233aefc4c654ea7ff194b3e7435e6eec321807e90\\n'\n",
      "                         'Solution:\\n'\n",
      "                         'Then when I install numpy separately (and not as '\n",
      "                         'part of mlflow), numpy gets installed (same '\n",
      "                         \"version), and then when I do 'pip install mlflow', \"\n",
      "                         'it also goes through.\\n'\n",
      "                         'Please note that the above may not be consistently '\n",
      "                         'simulatable, but please be aware of this issue that '\n",
      "                         'could occur during pip install of mlflow.\\n'\n",
      "                         'Added by Venkat Ramakrishnan'},\n",
      "                {'question': 'How to Delete an Experiment Permanently from '\n",
      "                             'MLFlow UI',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'After deleting an experiment from UI, the deleted '\n",
      "                         'experiment still persists in the database.\\n'\n",
      "                         'Solution: To delete this experiment permanently, '\n",
      "                         'follow these steps.\\n'\n",
      "                         'Assuming you are using sqlite database;\\n'\n",
      "                         'Install ipython sql using the following command: pip '\n",
      "                         'install ipython-sql\\n'\n",
      "                         'In your jupyter notebook, load the SQL magic scripts '\n",
      "                         'with this: %load_ext sql\\n'\n",
      "                         'Load the database with this: %sql '\n",
      "                         'sqlite:///nameofdatabase.db\\n'\n",
      "                         'Run the following SQL script to delete the '\n",
      "                         'experiment permanently: check link'},\n",
      "                {'question': 'How to Update Git Public Repo Without '\n",
      "                             'Overwriting Changes',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: I cloned the public repo, made edits, '\n",
      "                         'committed and pushed them to my own repo. Now I want '\n",
      "                         'to get the recent commits from the public repo '\n",
      "                         'without overwriting my own changes to my own repo. '\n",
      "                         'Which command(s) should I use?\\n'\n",
      "                         'This is what my config looks like (in case this '\n",
      "                         'might be useful):\\n'\n",
      "                         '[core]\\n'\n",
      "                         'repositoryformatversion = 0\\n'\n",
      "                         'filemode = true\\n'\n",
      "                         'bare = false\\n'\n",
      "                         'logallrefupdates = true\\n'\n",
      "                         'ignorecase = true\\n'\n",
      "                         'precomposeunicode = true\\n'\n",
      "                         '[remote \"origin\"]\\n'\n",
      "                         'url = git@github.com:my_username/mlops-zoomcamp.git\\n'\n",
      "                         'fetch = +refs/heads/*:refs/remotes/origin/*\\n'\n",
      "                         '[branch \"main\"]\\n'\n",
      "                         'remote = origin\\n'\n",
      "                         'merge = refs/heads/main\\n'\n",
      "                         'Solution: You should fork DataClubsTak’s repo '\n",
      "                         'instead of cloning it. On GitHub, click “Fetch and '\n",
      "                         'Merge” under the menu “Fetch upstream” at the main '\n",
      "                         'page of your own'},\n",
      "                {'question': 'Image size of 460x93139 pixels is too large. It '\n",
      "                             'must be less than 2^16 in each direction.',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'This is caused by ```mlflow.xgboost.autolog()``` '\n",
      "                         'when version 1.6.1 of xgboost\\n'\n",
      "                         'Downgrade to 1.6.0\\n'\n",
      "                         '```pip install xgboost==1.6.0``` or update '\n",
      "                         'requirements file with xgboost==1.6.0 instead of '\n",
      "                         'xgboost\\n'\n",
      "                         'Added by Nakul Bajaj'},\n",
      "                {'question': 'MlflowClient object has no attribute '\n",
      "                             \"'list_experiments'\",\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Since the version 1.29 the list_experiments method '\n",
      "                         'was deprecated and then removed in the later '\n",
      "                         'version\\n'\n",
      "                         'You should use search_experiments instead\\n'\n",
      "                         'Added by Alex Litvinov'},\n",
      "                {'question': 'MLflow Autolog not working',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Make sure `mlflow.autolog()` ( or framework-specific '\n",
      "                         'autolog ) written BEFORE `with mlflow.start_run()` '\n",
      "                         'not after.\\n'\n",
      "                         'Also make sure that all dependencies for the '\n",
      "                         'autologger are installed, including matplotlib. A '\n",
      "                         'warning about uninstalled dependencies will be '\n",
      "                         'raised.\\n'\n",
      "                         'Mohammed Ayoub Chettouh'},\n",
      "                {'question': 'MLflow URL (http://127.0.0.1:5000), doesn’t '\n",
      "                             'open.',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'If you’re running MLflow on a remote VM, you need to '\n",
      "                         'forward the port too like we did in Module 1 for '\n",
      "                         'Jupyter notebook port 8888. Simply connect your '\n",
      "                         'server to VS Code, as we did, and add 5000 to the '\n",
      "                         'PORT like in the screenshot:\\n'\n",
      "                         'Added by Sharon Ibejih\\n'\n",
      "                         'If you are running MLflow locally and 127.0.0.1:5000 '\n",
      "                         'shows a blank page navigate to localhost:5000 '\n",
      "                         'instead.'},\n",
      "                {'question': 'MLflow.xgboost Autolog Model Signature Failure',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Got the same warning message as Warrie Warrie when '\n",
      "                         'using “mlflow.xgboost.autolog()”\\n'\n",
      "                         'It turned out that this was just a warning message '\n",
      "                         'and upon checking MLflow UI (making sure that no '\n",
      "                         '“tag” filters were included), the model was actually '\n",
      "                         'automatically tracked in the MLflow.\\n'\n",
      "                         'Added by Bengsoon Chuah, Asked by Warrie Warrie, '\n",
      "                         'Answered by Anna Vasylytsya & Ivan Starovit'},\n",
      "                {'question': 'MlflowException: Unable to Set a Deleted '\n",
      "                             'Experiment',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'mlflow.exceptions.MlflowException: Cannot set a '\n",
      "                         \"deleted experiment 'cross-sell' as the active \"\n",
      "                         'experiment. You can restore the experiment, or '\n",
      "                         'permanently delete the  experiment to create a new '\n",
      "                         'one.\\n'\n",
      "                         'There are many options to solve in this link: '\n",
      "                         'https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow'},\n",
      "                {'question': 'No Space Left on Device - OSError[Errno 28]',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'You do not have enough disk space to install the '\n",
      "                         'requirements. You can either increase the base EBS '\n",
      "                         'volume by following this link or add an external '\n",
      "                         'disk to your instance and configure conda '\n",
      "                         'installation to happen on the external disk.\\n'\n",
      "                         'Abinaya Mahendiran\\n'\n",
      "                         'On GCP: I added another disk to my vm and followed '\n",
      "                         'this guide to mount the disk. Confirm the mount by '\n",
      "                         'running df -H (disk free) command in bash shell. I '\n",
      "                         'also deleted Anaconda and instead used miniconda. I '\n",
      "                         'downloaded miniconda in the additional disk that I '\n",
      "                         'mounted and when installing miniconda, enter the '\n",
      "                         'path to the extra disk instead of the default disk, '\n",
      "                         'this way conda is installed on the extra disk.\\n'\n",
      "                         'Yang Cao'},\n",
      "                {'question': 'Parameters Mismatch in Homework Q3',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'I was using an old version of sklearn due to which I '\n",
      "                         'got the wrong number of parameters because in the '\n",
      "                         'latest version min_impurity_split for '\n",
      "                         'randomforrestRegressor was deprecated. Had to '\n",
      "                         'upgrade to the latest version to get the correct '\n",
      "                         'number of params.'},\n",
      "                {'question': 'Protobuf error when installing MLflow',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Error: I installed all the libraries from the '\n",
      "                         'requirements.txt document in a new environment as '\n",
      "                         'follows:\\n'\n",
      "                         'pip install -r requirementes.txt\\n'\n",
      "                         'Then when I run mlflow from my terminal like this:\\n'\n",
      "                         'mlflow\\n'\n",
      "                         'I get this error:\\n'\n",
      "                         'SOLUTION: You need to downgrade the version of '\n",
      "                         \"'protobuf' module to 3.20.x or lower. Initially, it \"\n",
      "                         'was version=4.21, I installed protobuf==3.20\\n'\n",
      "                         'pip install protobuf==3.20\\n'\n",
      "                         'After which I was able to run mlflow from my '\n",
      "                         'terminal.\\n'\n",
      "                         '-Submitted by Aashnna Soni'},\n",
      "                {'question': 'Setting up Artifacts folders',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Please check your current directory while running '\n",
      "                         'the mlflow ui command. You need to run mlflow ui or '\n",
      "                         'mlflow server command in the right directory.'},\n",
      "                {'question': 'Setting up MLflow experiment tracker on GCP',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'If you have problem with setting up MLflow for '\n",
      "                         'experiment tracking on GCP, you can check these two '\n",
      "                         'links:\\n'\n",
      "                         'https://kargarisaac.github.io/blog/mlops/data%20engineering/2022/06/15/MLFlow-on-GCP.html\\n'\n",
      "                         'https://kargarisaac.github.io/blog/mlops/2022/08/26/machine-learning-workflow-orchestration-zenml.html'},\n",
      "                {'question': 'Setuptools Replacing Distutils - MLflow Autolog '\n",
      "                             'Warning',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Solution: Downgrade setuptools (I downgraded 62.3.2 '\n",
      "                         '-> 49.1.0)'},\n",
      "                {'question': 'Sorting runs in MLflow UI',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'I can’t sort runs in MLFlow\\n'\n",
      "                         'Make sure you are in table view (not list view) in '\n",
      "                         'the MLflow UI.\\n'\n",
      "                         'Added and Answered by Anna Vasylytsya'},\n",
      "                {'question': 'TypeError: send_file() unexpected keyword '\n",
      "                             \"'max_age' during MLflow UI Launch\",\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: When I ran `$ mlflow ui` on a remote server '\n",
      "                         'and try to open it in my local browser I got an '\n",
      "                         'exception  and the page with mlflow ui wasn’t '\n",
      "                         'loaded.\\n'\n",
      "                         'Solution: You should `pip uninstall flask` on your '\n",
      "                         'remote server on conda env and after it install '\n",
      "                         'Flask `pip install Flask`. It is because the base '\n",
      "                         'conda env has ~flask<1.2, and when you clone it to '\n",
      "                         'your new work env, you are stuck with this old '\n",
      "                         'version.\\n'\n",
      "                         'Added by Salimov Ilnaz'},\n",
      "                {'question': 'mlflow ui on Windows FileNotFoundError: '\n",
      "                             '[WinError 2] The system cannot find the file '\n",
      "                             'specified',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: After successfully installing mlflow using '\n",
      "                         'pip install mlflow on my Windows system, I am trying '\n",
      "                         'to run the mlflow ui command but it throws the '\n",
      "                         'following error:\\n'\n",
      "                         'FileNotFoundError: [WinError 2] The system cannot '\n",
      "                         'find the file specified\\n'\n",
      "                         'Solution: Add '\n",
      "                         'C:\\\\Users\\\\{User_Name}\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts '\n",
      "                         'to the PATH\\n'\n",
      "                         'Added by Alex Litvinov'},\n",
      "                {'question': 'Unsupported Operand Type Error in hpo.py',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Running “python hpo.py --data_path=./your-path '\n",
      "                         '--max_evals=50” for the homework leads to the '\n",
      "                         'following error: TypeError: unsupported operand '\n",
      "                         \"type(s) for -: 'str' and 'int'\\n\"\n",
      "                         'Full Traceback:\\n'\n",
      "                         'File '\n",
      "                         '\"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", '\n",
      "                         'line 73, in <module>\\n'\n",
      "                         'run(args.data_path, args.max_evals)\\n'\n",
      "                         'File '\n",
      "                         '\"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", '\n",
      "                         'line 47, in run\\n'\n",
      "                         'fmin(\\n'\n",
      "                         'File '\n",
      "                         '\"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", '\n",
      "                         'line 540, in fmin\\n'\n",
      "                         'return trials.fmin(\\n'\n",
      "                         'File '\n",
      "                         '\"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/base.py\", '\n",
      "                         'line 671, in fmin\\n'\n",
      "                         'return fmin(\\n'\n",
      "                         'File '\n",
      "                         '\"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", '\n",
      "                         'line 586, in fmin\\n'\n",
      "                         'rval.exhaust()\\n'\n",
      "                         'File '\n",
      "                         '\"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", '\n",
      "                         'line 364, in exhaust\\n'\n",
      "                         'self.run(self.max_evals - n_done, '\n",
      "                         'block_until_done=self.asynchronous)\\n'\n",
      "                         \"TypeError: unsupported operand type(s) for -: 'str' \"\n",
      "                         \"and 'int'\\n\"\n",
      "                         'Solution:\\n'\n",
      "                         'The --max_evals argument in hpo.py has no defined '\n",
      "                         'datatype and will therefore implicitly be treated as '\n",
      "                         'string. It should be an integer, so that the script '\n",
      "                         'can work correctly. Add type=int to the argument '\n",
      "                         'definition:\\n'\n",
      "                         'parser.add_argument(\\n'\n",
      "                         '\"--max_evals\",\\n'\n",
      "                         'type=int,\\n'\n",
      "                         'default=50,\\n'\n",
      "                         'help=\"the number of parameter evaluations for the '\n",
      "                         'optimizer to explore.\"\\n'\n",
      "                         ')'},\n",
      "                {'question': 'Unsupported Scikit-Learn version',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Getting the following warning when running '\n",
      "                         'mlflow.sklearn:\\n'\n",
      "                         '\\n'\n",
      "                         '2022/05/28 04:36:36 WARNING '\n",
      "                         'mlflow.utils.autologging_utils: You are using an '\n",
      "                         'unsupported version of sklearn. If you encounter '\n",
      "                         'errors during autologging, try upgrading / '\n",
      "                         'downgrading sklearn to a supported version, or try '\n",
      "                         'upgrading MLflow. […]\\n'\n",
      "                         'Solution: use 0.22.1 <= scikit-learn <= 1.1.0\\n'\n",
      "                         'Reference: '\n",
      "                         'https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html'},\n",
      "                {'question': 'Mlflow CLI does not return experiments',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: CLI commands (mlflow experiments list) do '\n",
      "                         'not return experiments\\n'\n",
      "                         'Solution description: need to set environment '\n",
      "                         'variable for the Tracking URI:\\n'\n",
      "                         '$ export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\\n'\n",
      "                         'Added and Answered by Dino Vitale'},\n",
      "                {'question': 'Viewing MLflow Experiments using MLflow CLI',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: After starting the tracking server, when we '\n",
      "                         'try to use the mlflow cli commands as listed here, '\n",
      "                         'most of them can’t seem to find the experiments that '\n",
      "                         'have been run with the tracking server\\n'\n",
      "                         'Solution: We need to set the environment variable '\n",
      "                         'MLFLOW_TRACKING_URI to the URI of the sqlite '\n",
      "                         'database. This is something like “export '\n",
      "                         'MLFLOW_TRACKING_URI=sqlite:///{path to sqlite '\n",
      "                         'database}” . After this, we can view the experiments '\n",
      "                         'from the command line using commands like “mlflow '\n",
      "                         'experiments search”\\n'\n",
      "                         'Even after this commands like “mlflow gc” doesn’t '\n",
      "                         'seem to get the tracking uri, and they have to be '\n",
      "                         'passed explicitly as an argument every time the '\n",
      "                         'command is run.\\n'\n",
      "                         'Ahmed Fahim (afahim03@yahoo.com)'},\n",
      "                {'question': 'Viewing SQLlite Data Raw & Deleting Experiments '\n",
      "                             'Manually',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'All the experiment and other tracking information in '\n",
      "                         'mlflow are stored in sqllite database provided while '\n",
      "                         'initiating the mlflow ui command. This database can '\n",
      "                         'be inspected using Pycharm’s Database tab by using '\n",
      "                         'the SQLLite database type. Once the connection is '\n",
      "                         'created as below, the tables can be queried and '\n",
      "                         'inspected using regular SQL. The same applies for '\n",
      "                         'any SQL backed database such as postgres as well.\\n'\n",
      "                         'This is very useful to understand the entity '\n",
      "                         'structure of the data being stored within mlflow and '\n",
      "                         'useful for any kind of systematic archiving of model '\n",
      "                         'tracking for longer periods.\\n'\n",
      "                         'Added by Senthilkumar Gopal'},\n",
      "                {'question': 'What does launching the tracking server locally '\n",
      "                             'mean?',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Solution : It is another way to start it for remote '\n",
      "                         'hosting a mlflow server. For example, if you are '\n",
      "                         'multiple colleagues working together on something '\n",
      "                         'you most likely would not run mlflow on one laptop '\n",
      "                         'but rather everyone would connect to the same server '\n",
      "                         'running mlflow\\n'\n",
      "                         'Answer by Christoffer Added by Akshit Miglani '\n",
      "                         '(akshit.miglani09@gmail.com)'},\n",
      "                {'question': 'Parameter adding in case of max_depth not '\n",
      "                             'recognized',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: parameter was not recognized during the '\n",
      "                         'model registry\\n'\n",
      "                         'Solution: parameters should be added in previous to '\n",
      "                         'the model registry. The parameters can be added by '\n",
      "                         'mlflow.log_params(params) so that the dictionary can '\n",
      "                         'be directly appended to the data.run.params.\\n'\n",
      "                         'Added and Answered by Sam Lim'},\n",
      "                {'question': 'Max_depth is not recognize even when I add the '\n",
      "                             'mlflow.log_params',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: Max_depth is not recognize even when I add '\n",
      "                         'the mlflow.log_params\\n'\n",
      "                         'Solution: the mlflow.log_params(params) should be '\n",
      "                         'added to the hpo.py script, but if you run it it '\n",
      "                         'will append the new model to the previous run that '\n",
      "                         'doesn’t contain the parameters, you should either '\n",
      "                         'remove the previous experiment or change it\\n'\n",
      "                         'Pastor Soto'},\n",
      "                {'question': \"AttributeError: 'tuple' object has no attribute \"\n",
      "                             \"'tb_frame'\",\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: About week_2 homework: The '\n",
      "                         'register_model.py  script, when I copy it into a '\n",
      "                         'jupyter notebook fails and spits out the following '\n",
      "                         \"error. AttributeError: 'tuple' object has no \"\n",
      "                         \"attribute 'tb_frame'\\n\"\n",
      "                         'Solution: remove click decorators'},\n",
      "                {'question': 'WandB API error',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: when running the preprocess_data.py file '\n",
      "                         'you get the following error:\\n'\n",
      "                         '\\n'\n",
      "                         'wandb: ERROR api_key not configured (no-tty). call '\n",
      "                         'wandb.login(key=[your_api_key])\\n'\n",
      "                         'Solution: Go to your WandB profile (top RHS) → user '\n",
      "                         'settings → scroll down to “Danger Zone” and copy '\n",
      "                         'your API key. \\n'\n",
      "                         '\\n'\n",
      "                         'Then before running preprocess_data.py, add and run '\n",
      "                         'the following cell in your notebook:\\n'\n",
      "                         '\\n'\n",
      "                         '%%bash\\n'\n",
      "                         '\\n'\n",
      "                         'Wandb login <YOUR_API_KEY_HERE>.\\n'\n",
      "                         'Added and Answered by James Gammerman '\n",
      "                         '(jgammerman@gmail.com)'},\n",
      "                {'question': 'WARNING mlflow.xgboost: Failed to infer model '\n",
      "                             'signature: could not sample data to infer model '\n",
      "                             'signature: please ensure that autologging is '\n",
      "                             'enabled before constructing the dataset.',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Please make sure you following the order below nd '\n",
      "                         'enabling the autologging before constructing the '\n",
      "                         'dataset. If you still have this issue check that '\n",
      "                         'your data is in format compatible with XGBoost.\\n'\n",
      "                         '# Enable MLflow autologging for XGBoost\\n'\n",
      "                         'mlflow.xgboost.autolog()\\n'\n",
      "                         '# Construct your dataset\\n'\n",
      "                         'X_train, y_train = ...\\n'\n",
      "                         '# Train your XGBoost model\\n'\n",
      "                         'model = xgb.XGBRegressor(...)\\n'\n",
      "                         'model.fit(X_train, y_train)\\n'\n",
      "                         'Added by Olga Rudakova'},\n",
      "                {'question': 'wget not working',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem\\n'\n",
      "                         'Using wget command to download either data or python '\n",
      "                         'scripts on Windows, I am using the notebook provided '\n",
      "                         'by Visual Studio and despite having a python virtual '\n",
      "                         'env, it did not recognize the pip command.\\n'\n",
      "                         'Solution: Use python -m pip, this same for any other '\n",
      "                         'command. Ie. python -m wget\\n'\n",
      "                         'Added by Erick Calderin'},\n",
      "                {'question': 'Open/run github notebook(.ipynb) directly in '\n",
      "                             'Google Colab',\n",
      "                 'section': 'Module 2: Experiment tracking',\n",
      "                 'text': 'Problem: Open/run github notebook(.ipynb) directly '\n",
      "                         'in Google Colab\\n'\n",
      "                         \"Solution: Change the domain from 'github.com' to \"\n",
      "                         \"'githubtocolab.com'. The notebook will open in \"\n",
      "                         'Google Colab.\\n'\n",
      "                         'Only works with Public repo.\\n'\n",
      "                         'Added by Ming Jun\\n'\n",
      "                         'Navigating in Wandb UI became difficult to me, I had '\n",
      "                         'to intuit some options until I found the correct '\n",
      "                         'one.\\n'\n",
      "                         'Solution: Go to the official doc.\\n'\n",
      "                         'Added by Erick Calderin'},\n",
      "                {'question': 'Why do we use Jan/Feb/March for '\n",
      "                             'Train/Test/Validation Purposes?',\n",
      "                 'section': 'Module 3: Orchestration',\n",
      "                 'text': 'Problem: Someone asked why we are using this type of '\n",
      "                         'split approach instead of just a random split.\\n'\n",
      "                         'Solution: For example, I have some models at work '\n",
      "                         'that train on Jan 1 2020 — Aug 1 2021 time period, '\n",
      "                         'and then test on Aug 1 - Dec 31 2021, and finally '\n",
      "                         'validate on Jan - March or something\\n'\n",
      "                         'We do these “out of time”  validations to do a few '\n",
      "                         'things:\\n'\n",
      "                         'Check for seasonality of our data\\n'\n",
      "                         'We know if the RMSE for Test is 5 say, and then RMSE '\n",
      "                         'for validation is 20, then there’s serious '\n",
      "                         'seasonality to the data we are looking at, and now '\n",
      "                         'we might change to Time Series approaches\\n'\n",
      "                         'If I’m predicting on Mar 30 2023 the outcomes for '\n",
      "                         'the next 3 months, the “random sample” in our '\n",
      "                         'train/test would have caused data leakage, '\n",
      "                         'overfitting, and poor model performance in '\n",
      "                         'production. We mustn’t take information about the '\n",
      "                         'future and apply it to the present when we are '\n",
      "                         'predicting in a model context.\\n'\n",
      "                         'These are two of, I think, the biggest points for '\n",
      "                         'why we are doing jan/feb/march. I wouldn’t do it any '\n",
      "                         'other way.\\n'\n",
      "                         'Train: Jan\\n'\n",
      "                         'Test: Feb\\n'\n",
      "                         'Validate: March\\n'\n",
      "                         'The point of validation is to report out model '\n",
      "                         'metrics to leadership, regulators, auditors, and '\n",
      "                         'record the models performance to then later analyze '\n",
      "                         'target drift\\n'\n",
      "                         'Added by Sam LaFell\\n'\n",
      "                         'Problem: If you get an error while trying to run the '\n",
      "                         'mlflow server on AWS CLI with S3 bucket and POSTGRES '\n",
      "                         'database:\\n'\n",
      "                         'Reproducible Command:\\n'\n",
      "                         'mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri '\n",
      "                         'postgresql://<DB_USERNAME>:<DB_PASSWORD>@<DB_ENDPOINT>:<DB_PORT>/<DB_NAME> '\n",
      "                         '--default-artifact-root s3://<BUCKET_NAME>\\n'\n",
      "                         'Error:\\n'\n",
      "                         '\"urllib3 v2.0 only supports OpenSSL 1.1.1+, '\n",
      "                         'currently \"\\n'\n",
      "                         'ImportError: urllib3 v2.0 only supports OpenSSL '\n",
      "                         \"1.1.1+, currently the 'ssl' module is compiled with \"\n",
      "                         \"'OpenSSL 1.0.2k-fips  26 Jan 2017'. See: \"\n",
      "                         'https://github.com/urllib3/urllib3/issues/2168\\n'\n",
      "                         'Solution: Upgrade mlflow using\\n'\n",
      "                         'Code: pip3 install --upgrade mlflow\\n'\n",
      "                         'Resolution: It downgrades urllib3 2.0.3 to 1.26.16 '\n",
      "                         'which is compatible with mlflow and ssl 1.0.2\\n'\n",
      "                         'Installing collected packages: urllib3\\n'\n",
      "                         'Attempting uninstall: urllib3\\n'\n",
      "                         'Found existing installation: urllib3 2.0.3\\n'\n",
      "                         'Uninstalling urllib3-2.0.3:\\n'\n",
      "                         'Successfully uninstalled urllib3-2.0.3\\n'\n",
      "                         'Successfully installed urllib3-1.26.16\\n'\n",
      "                         'Added by Sarvesh Thakur'},\n",
      "                {'question': 'Problem title',\n",
      "                 'section': 'Module 3: Orchestration',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'Solution description\\n'\n",
      "                         '(optional) Added by Name'},\n",
      "                {'question': 'Where is the FAQ for Prefect questions?',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Here'},\n",
      "                {'question': 'aws.exe: error: argument operation: Invalid '\n",
      "                             'choice — Docker can not login to ECR.',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Windows with AWS CLI already installed\\n'\n",
      "                         'AWS CLI version:\\n'\n",
      "                         'aws-cli/2.4.24 Python/3.8.8 Windows/10 exe/AMD64 '\n",
      "                         'prompt/off\\n'\n",
      "                         'Executing\\n'\n",
      "                         '$(aws ecr get-login --no-include-email)\\n'\n",
      "                         'shows error\\n'\n",
      "                         'aws.exe: error: argument operation: Invalid choice, '\n",
      "                         'valid choices are…\\n'\n",
      "                         'Use this command instead. More info here:\\n'\n",
      "                         'https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html\\n'\n",
      "                         'aws ecr get-login-password \\\\\\n'\n",
      "                         '--region <region> \\\\\\n'\n",
      "                         '| docker login \\\\\\n'\n",
      "                         '--username AWS \\\\\\n'\n",
      "                         '--password-stdin '\n",
      "                         '<aws_account_id>.dkr.ecr.<region>.amazonaws.com\\n'\n",
      "                         'Added by MarcosMJD'},\n",
      "                {'question': 'Multiline commands in Windows Powershell',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Use ` at the end of each line except the last. Note '\n",
      "                         'that multiline string does not need `.\\n'\n",
      "                         'Escape “ to “\\\\ .\\n'\n",
      "                         'Use $env: to create env vars (non-persistent). '\n",
      "                         'E.g.:\\n'\n",
      "                         '$env:KINESIS_STREAM_INPUT=\"ride_events\"\\n'\n",
      "                         'aws kinesis put-record --cli-binary-format '\n",
      "                         'raw-in-base64-out `\\n'\n",
      "                         '--stream-name $env:KINESIS_STREAM_INPUT `\\n'\n",
      "                         '--partition-key 1 `\\n'\n",
      "                         \"--data '{\\n\"\n",
      "                         '\\\\\"ride\\\\\": {\\n'\n",
      "                         '\\\\\"PULocationID\\\\\": 130,\\n'\n",
      "                         '\\\\\"DOLocationID\\\\\": 205,\\n'\n",
      "                         '\\\\\"trip_distance\\\\\": 3.66\\n'\n",
      "                         '},\\n'\n",
      "                         '\\\\\"ride_id\\\\\": 156\\n'\n",
      "                         \"}'\\n\"\n",
      "                         'Added by MarcosMJD'},\n",
      "                {'question': 'Pipenv installation not working (AttributeError: '\n",
      "                             \"module 'collections' has no attribute \"\n",
      "                             \"'MutableMapping')\",\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'If one gets pipenv failures for pipenv install '\n",
      "                         'command -\\n'\n",
      "                         \"AttributeError: module 'collections' has no \"\n",
      "                         \"attribute 'MutableMapping'\\n\"\n",
      "                         'It happens because you use the system Python (3.10) '\n",
      "                         'for pipenv.\\n'\n",
      "                         'If you previously installed pipenv with apt-get, '\n",
      "                         'remove it - sudo-apt remove pipenv\\n'\n",
      "                         'Make sure you have a non-system Python installed in '\n",
      "                         'your environment. The easiest way to do it is to '\n",
      "                         'install anaconda or miniconda\\n'\n",
      "                         'Next, install pipenv to your non-system Python. If '\n",
      "                         'you use the setup from the lectures, it’s just this: '\n",
      "                         'pip install pipenv\\n'\n",
      "                         'Now re-run pipenv install XXXX (relevant '\n",
      "                         'dependencies) - should work\\n'\n",
      "                         'Tested and worked on AWS instance, similar to the '\n",
      "                         'config Alexey presented in class.\\n'\n",
      "                         'Added by Daniel HenSSL'},\n",
      "                {'question': \"module is not available (Can't connect to HTTPS \"\n",
      "                             'URL)',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'First check if SSL module configured with following '\n",
      "                         'command:\\n'\n",
      "                         'Python -m ssl\\n'\n",
      "                         '\\n'\n",
      "                         'If the output of this is empty there is no problem '\n",
      "                         'with SSL configuration.\\n'\n",
      "                         '\\n'\n",
      "                         'Then you should upgrade your pipenv package in your '\n",
      "                         'current environment to resolve the problem.\\n'\n",
      "                         'Added by Kenan Arslanbay'},\n",
      "                {'question': \"No module named 'pip._vendor.six'\",\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'During scikit-learn installation via the command:\\n'\n",
      "                         'pipenv install scikit-learn==1.0.2\\n'\n",
      "                         'The following error is raised:\\n'\n",
      "                         'ModuleNotFoundError: No module named '\n",
      "                         \"'pip._vendor.six'\\n\"\n",
      "                         'Then, one should:\\n'\n",
      "                         'sudo apt install python-six\\n'\n",
      "                         'pipenv --rm\\n'\n",
      "                         'pipenv install scikit-learn==1.0.2\\n'\n",
      "                         'Added by Giovanni Pecoraro'},\n",
      "                {'question': 'Pipenv with Jupyter',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Problem description. How can we use Jupyter '\n",
      "                         'notebooks with the Pipenv environment?\\n'\n",
      "                         'Solution: Refer to this stackoverflow question. '\n",
      "                         'Basically install jupyter and ipykernel using '\n",
      "                         'pipenv. And then register the kernel with `python -m '\n",
      "                         'ipykernel install --user --name=my-virtualenv-name` '\n",
      "                         'inside the Pipenv shell. If you are using Jupyter '\n",
      "                         'notebooks in VS Code, doing this will also add the '\n",
      "                         'virtual environment in the list of kernels.\\n'\n",
      "                         'Added by Ron Medina'},\n",
      "                {'question': 'Pipenv with Jupyter no output',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Problem: I tried to run starter notebook on pipenv '\n",
      "                         'environment but had issues with no output on '\n",
      "                         'prints. \\n'\n",
      "                         'I used scikit-learn==1.2.2 and python==3.10\\n'\n",
      "                         'Tornado version was 6.3.2\\n'\n",
      "                         '\\n'\n",
      "                         \"Solution: The error you're encountering seems to be \"\n",
      "                         'a bug related to Tornado, which is a Python web '\n",
      "                         \"server and networking library. It's used by Jupyter \"\n",
      "                         'under the hood to handle networking tasks.\\n'\n",
      "                         'Downgrading to tornado==6.1 fixed the issue\\n'\n",
      "                         'https://stackoverflow.com/questions/54971836/no-output-jupyter-notebook'},\n",
      "                {'question': '‘Invalid base64’ error after running `aws '\n",
      "                             'kinesis put-record`',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Problem description:  You might get an error '\n",
      "                         '‘Invalid base64’ after running the ‘aws kinesis '\n",
      "                         'put-record’ command on your local machine. This '\n",
      "                         'might be the case if you are using the AWS CLI '\n",
      "                         'version 2 (note that in the video 4.4, around 57:42, '\n",
      "                         'you can see a warning since the instructor is using '\n",
      "                         'v1 of the CLI.\\n'\n",
      "                         'Solution description: To get around this, pass the '\n",
      "                         'argument ‘--cli-binary-format raw-in-base64-out’. '\n",
      "                         'This will encode your data string into base64 before '\n",
      "                         'passing it to kinesis\\n'\n",
      "                         'Added by M'},\n",
      "                {'question': 'Error index 311297 is out of bounds for axis 0 '\n",
      "                             'with size 131483 when loading parquet file.',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Problem description:   Running starter.ipynb in '\n",
      "                         'homework’s Q1 will show up this error.\\n'\n",
      "                         'Solution description: Update pandas (actually pandas '\n",
      "                         'version was the latest, but several dependencies are '\n",
      "                         'updated).\\n'\n",
      "                         'Added by Marcos Jimenez'},\n",
      "                {'question': 'Pipfile.lock was not created along with Pipfile',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Use command $pipenv lock to force the creation of '\n",
      "                         'Pipfile.lock\\n'\n",
      "                         'Added by Bijay P.'},\n",
      "                {'question': 'Permission Denied using Pipenv',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'This issue is usually due to the pythonfinder module '\n",
      "                         'in pipenv.\\n'\n",
      "                         'The solution to this involves manually changing the '\n",
      "                         'scripts as describe here python_finder_fix\\n'\n",
      "                         'Added by Ridwan Amure'},\n",
      "                {'question': 'Error while parsing arguments via CLI  '\n",
      "                             \"[ValueError: Unknown format code 'd' for object \"\n",
      "                             \"of type 'str']\",\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'When passing arguments to a script via command line '\n",
      "                         'and converting it to a 4 digit number using '\n",
      "                         'f’{year:04d}’, this error showed up.\\n'\n",
      "                         'This happens because all inputs from the command '\n",
      "                         'line are read as string by the script. They need to '\n",
      "                         'be converted to numeric/integer before '\n",
      "                         'transformation in fstring.\\n'\n",
      "                         'year = int(sys.argv[1])\\n'\n",
      "                         'f’{year:04d}’\\n'\n",
      "                         'If you use click library just edit a decorator\\n'\n",
      "                         '@click.command()\\n'\n",
      "                         '@click.option( \"--year\",  help=\"Year for '\n",
      "                         'evaluation\",   type=int)\\n'\n",
      "                         'def  your_function(year):\\n'\n",
      "                         '<<Your code>>\\n'\n",
      "                         'Added by Taras Sh'},\n",
      "                {'question': 'Dockerizing tips',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Ensure the correct image is being used to derive '\n",
      "                         'from.\\n'\n",
      "                         'Copy the data from local to the docker image using '\n",
      "                         'the COPY command to a relative path. Using absolute '\n",
      "                         'paths within the image might be troublesome.\\n'\n",
      "                         'Use paths starting from /app and don’t forget to do '\n",
      "                         'WORKDIR /app before actually performing the code '\n",
      "                         'execution.\\n'\n",
      "                         'Most common commands\\n'\n",
      "                         'Build container using docker build -t mlops-learn .\\n'\n",
      "                         'Execute the script using docker run -it --rm '\n",
      "                         'mlops-learn\\n'\n",
      "                         '<mlops-learn> is just a name used for the image and '\n",
      "                         'does not have any significance.'},\n",
      "                {'question': 'Running multiple services in a Docker container',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'If you are trying to run Flask gunicorn & MLFlow '\n",
      "                         'server from the same container, defining both in '\n",
      "                         'Dockerfile with CMD will only run MLFlow & not '\n",
      "                         'Flask.\\n'\n",
      "                         'Solution: Create separate shell script with server '\n",
      "                         'run commands, for eg:\\n'\n",
      "                         '> \\tscript1.sh\\n'\n",
      "                         '#!/bin/bash\\n'\n",
      "                         'gunicorn --bind=0.0.0.0:9696 predict:app\\n'\n",
      "                         'Another script with e.g. MLFlow server:\\n'\n",
      "                         '>\\tscript2.sh\\n'\n",
      "                         '#!/bin/bash\\n'\n",
      "                         'mlflow server -h 0.0.0.0 -p 5000 '\n",
      "                         '--backend-store-uri=sqlite:///mlflow.db '\n",
      "                         '--default-artifact-root=g3://zc-bucket/mlruns/\\n'\n",
      "                         'Create a wrapper script to run above 2 scripts:\\n'\n",
      "                         '>\\twrapper_script.sh\\n'\n",
      "                         '#!/bin/bash\\n'\n",
      "                         '# Start the first process\\n'\n",
      "                         './script1.sh &\\n'\n",
      "                         '# Start the second process\\n'\n",
      "                         './script2.sh &\\n'\n",
      "                         '# Wait for any process to exit\\n'\n",
      "                         'wait -n\\n'\n",
      "                         '# Exit with status of process that exited first\\n'\n",
      "                         'exit $?\\n'\n",
      "                         'Give executable permissions to all scripts:\\n'\n",
      "                         'chmod +x *.sh\\n'\n",
      "                         'Now we can define last line of Dockerfile as:\\n'\n",
      "                         '> \\tDockerfile\\n'\n",
      "                         'CMD ./wrapper_script.sh\\n'\n",
      "                         'Dont forget to expose all ports defined by '\n",
      "                         'services!'},\n",
      "                {'question': 'Cannot generate pipfile.lock raise '\n",
      "                             'InstallationError( '\n",
      "                             'pip9.exceptions.InstallationError)',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Problem description cannot generate pipfile.lock '\n",
      "                         'raise InstallationError( '\n",
      "                         'pip9.exceptions.InstallationError: Command \"python '\n",
      "                         'setup.py egg_info\" failed with error code 1\\n'\n",
      "                         'Solution: you need to force and upgrade wheel and '\n",
      "                         'pipenv\\n'\n",
      "                         'Just run the command line :\\n'\n",
      "                         'pip install --user --upgrade --upgrade-strategy '\n",
      "                         'eager pipenv wheel'},\n",
      "                {'question': 'Connecting s3 bucket to MLFLOW',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Problem description. How can we connect s3 bucket to '\n",
      "                         'MLFLOW?\\n'\n",
      "                         'Solution: Use boto3 and AWS CLI to store access '\n",
      "                         'keys. The access keys are what will be used by boto3 '\n",
      "                         \"(AWS' Python API tool) to connect with the AWS \"\n",
      "                         'servers. If there are no Access Keys how can they '\n",
      "                         'make sure that they have the right to access this '\n",
      "                         \"Bucket? Maybe you're a malicious actor (Hacker for \"\n",
      "                         'ex). The keys must be present for boto3 to talk to '\n",
      "                         'the AWS servers and they will provide access to the '\n",
      "                         'Bucket if you possess the right permissions. You can '\n",
      "                         'always set the Bucket as public so anyone can access '\n",
      "                         \"it, now you don't need access keys because AWS won't \"\n",
      "                         'care.\\n'\n",
      "                         'Read more here: '\n",
      "                         'https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\n'\n",
      "                         'Added by Akshit Miglani'},\n",
      "                {'question': 'Uploading to s3 fails with An error occurred '\n",
      "                             '(InvalidAccessKeyId) when calling the PutObject '\n",
      "                             'operation: The AWS Access Key Id you provided '\n",
      "                             'does not exist in our records.\"',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Even though the upload works using aws cli and boto3 '\n",
      "                         'in Jupyter notebook.\\n'\n",
      "                         'Solution set the AWS_PROFILE environment variable '\n",
      "                         '(the default profile is called default)'},\n",
      "                {'question': 'Dockerizing lightgbm',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Problem description: lib_lightgbm.so Reason: image '\n",
      "                         'not found\\n'\n",
      "                         'Solution description: Add “RUN apt-get install '\n",
      "                         'libgomp1” to your docker. (change installer command '\n",
      "                         'based on OS)\\n'\n",
      "                         'Added by Kazeem Hakeem'},\n",
      "                {'question': 'Error raised when executing mlflow’s '\n",
      "                             'pyfunc.load_model in lambda function.',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'When the request is processed in lambda function, '\n",
      "                         'mlflow library raises:\\n'\n",
      "                         '2022/09/19 21:18:47 WARNING mlflow.pyfunc: '\n",
      "                         'Encountered an unexpected error '\n",
      "                         '(AttributeError(\"module \\'dataclasses\\' has no '\n",
      "                         'attribute \\'__version__\\'\")) while detecting model '\n",
      "                         'dependency mismatches. Set logging level to DEBUG to '\n",
      "                         'see the full traceback.\\n'\n",
      "                         'Solution: Increase the memory of the lambda '\n",
      "                         'function.\\n'\n",
      "                         'Added by MarcosMJD'},\n",
      "                {'question': '4.3 FYI Notebook is end state of Video -',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Just a note if you are following the video but also '\n",
      "                         'using the repo’s notebook The notebook is the end '\n",
      "                         'state of the video which eventually uses mlflow '\n",
      "                         'pipelines.\\n'\n",
      "                         'Just watch the video and be patient. Everything will '\n",
      "                         'work :)\\n'\n",
      "                         'Added by Quinn Avila'},\n",
      "                {'question': 'Passing envs to my docker image',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Problem description: I was having issues because my '\n",
      "                         'python script was not reading AWS credentials from '\n",
      "                         'env vars, after building the image I was running it '\n",
      "                         'like this:\\n'\n",
      "                         'docker run -it homework-04 -e '\n",
      "                         'AWS_ACCESS_KEY_ID=xxxxxxxx -e '\n",
      "                         'AWS_SECRET_ACCESS_KEY=xxxxxx\\n'\n",
      "                         'Solution 1:\\n'\n",
      "                         '\\n'\n",
      "                         'Environment Variables: \\n'\n",
      "                         'You can set the AWS_ACCESS_KEY_ID, '\n",
      "                         'AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN (if you '\n",
      "                         'are using AWS STS) environment variables. You can '\n",
      "                         'set these in your shell, or you can include them in '\n",
      "                         'your Docker run command like this:\\n'\n",
      "                         'I found out by myself that those variables must be '\n",
      "                         'passed before specifying the name of the image, as '\n",
      "                         'follow:\\n'\n",
      "                         'docker run -e AWS_ACCESS_KEY_ID=xxxxxxxx -e '\n",
      "                         'AWS_SECRET_ACCESS_KEY=xxxxxx -it homework-04\\n'\n",
      "                         'Added by Erick Cal\\n'\n",
      "                         'Solution 2 (if AWS credentials were not found):\\n'\n",
      "                         'AWS Configuration Files: \\n'\n",
      "                         'The AWS SDKs and CLI will check the '\n",
      "                         '~/.aws/credentials and ~/.aws/config files for '\n",
      "                         'credentials if they exist. You can map these files '\n",
      "                         'into your Docker container using volumes:\\n'\n",
      "                         '\\n'\n",
      "                         'docker run -it --rm -v ~/.aws:/root/.aws '\n",
      "                         'homework:v1'},\n",
      "                {'question': 'How to see the model in the docker container in '\n",
      "                             'app/?',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'If anyone is troubleshooting or just interested in '\n",
      "                         'seeing the model listed on the image '\n",
      "                         'svizor/zoomcamp-model:mlops-3.10.0-slim.\\n'\n",
      "                         'Create a dockerfile. (yep thats all) and build '\n",
      "                         '“docker build -t zoomcamp_test .”\\n'\n",
      "                         'FROM svizor/zoomcamp-model:mlops-3.10.0-slim\\n'\n",
      "                         'Run “docker run -it zoomcamp_test ls /app” output -> '\n",
      "                         'model.bin\\n'\n",
      "                         'This will list the contents of the app directory and '\n",
      "                         '“model.bin” should output. With this you could just '\n",
      "                         'copy your files, for example “copy myfile .” maybe a '\n",
      "                         'requirements file and this can be run for example '\n",
      "                         '“docker run -it myimage myscript arg1 arg2 ”. Of '\n",
      "                         'course keep in mind a build is needed everytime you '\n",
      "                         'change the Dockerfile.\\n'\n",
      "                         'Another variation is to have it run when you run the '\n",
      "                         'docker file.\\n'\n",
      "                         '“””\\n'\n",
      "                         'FROM svizor/zoomcamp-model:mlops-3.10.0-slim\\n'\n",
      "                         'WORKDIR /app\\n'\n",
      "                         'CMD ls\\n'\n",
      "                         '“””\\n'\n",
      "                         'Just keep in mind CMD is needed because the RUN '\n",
      "                         'commands are used for building the image and the CMD '\n",
      "                         'is used at container runtime. And in your example '\n",
      "                         'you probably want to run a script or should we say '\n",
      "                         'CMD a script.\\n'\n",
      "                         'Quinn Avila'},\n",
      "                {'question': \"WARNING: The requested image's platform \"\n",
      "                             '(linux/amd64) does not match the detected host '\n",
      "                             'platform (linux/arm64/v8) and no specific '\n",
      "                             'platform was requested',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'To resolve this make sure to build the docker image '\n",
      "                         'with the platform tag, like this:\\n'\n",
      "                         '“docker build -t homework:v1 --platform=linux/arm64 '\n",
      "                         '.”'},\n",
      "                {'question': 'HTTPError: HTTP Error 403: Forbidden when call '\n",
      "                             'apply_model() in score.ipynb',\n",
      "                 'section': 'Module 4: Deployment',\n",
      "                 'text': 'Solution: instead of input_file = '\n",
      "                         \"f'https://s3.amazonaws.com/nyc-tlc/trip+data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'  \"\n",
      "                         'use input_file = '\n",
      "                         \"f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\\n\"\n",
      "                         'Ilnaz Salimov\\n'\n",
      "                         'salimovilnaz777@gmail.com'},\n",
      "                {'question': 'ModuleNotFoundError: No module named '\n",
      "                             \"'pipenv.patched.pip._vendor.urllib3.response'\",\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': \"i'm getting this error ModuleNotFoundError: No \"\n",
      "                         'module named '\n",
      "                         \"'pipenv.patched.pip._vendor.urllib3.response'\\n\"\n",
      "                         'and Resolved from this command pip install pipenv '\n",
      "                         '--force-reinstall\\n'\n",
      "                         'getting this errror '\n",
      "                         'site-packages\\\\pipenv\\\\patched\\\\pip\\\\_vendor\\\\urllib3\\\\connectionpool.py\"\\n'\n",
      "                         'Resolved from this command pip install -U pip and '\n",
      "                         'pip install requests\\n'\n",
      "                         'Asif'},\n",
      "                {'question': 'Login window in Grafana',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Problem description: When running docker-compose up '\n",
      "                         'as shown in the video 5.2 if you go to '\n",
      "                         'http://localhost:3000/ you get asked for a username '\n",
      "                         'and a password.\\n'\n",
      "                         'Solution: for both of them the default is “admin”. '\n",
      "                         'Then you can enter your new password. \\n'\n",
      "                         'See also here\\n'\n",
      "                         'Added by JaimeRV'},\n",
      "                {'question': 'Error in starting monitoring services in Linux',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Problem Description : In Linux, when starting '\n",
      "                         'services using docker compose up --build  as shown '\n",
      "                         'in video 5.2, the services won’t start and instead '\n",
      "                         'we get message unknown flag: --build in command '\n",
      "                         'prompt.\\n'\n",
      "                         'Solution : Since we install docker-compose '\n",
      "                         'separately in Linux, we have to run docker-compose '\n",
      "                         'up --build instead of docker compose up --build\\n'\n",
      "                         'Added by Ashish Lalchandani'},\n",
      "                {'question': 'KeyError ‘content-length’ when running '\n",
      "                             'prepare.py',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Problem: When running prepare.py getting KeyError: '\n",
      "                         '‘content-length’\\n'\n",
      "                         'Solution: From Emeli Dral:\\n'\n",
      "                         'It seems to me that the link we used in prepare.py '\n",
      "                         'to download taxi data does not work anymore. I '\n",
      "                         'substituted the instruction:\\n'\n",
      "                         'url = '\n",
      "                         'f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file}\\n'\n",
      "                         'by the\\n'\n",
      "                         'url = '\n",
      "                         'f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file}\"\\n'\n",
      "                         'in the prepare.py and it worked for me. Hopefully, '\n",
      "                         'if you do the same you will be able to get those '\n",
      "                         'data.'},\n",
      "                {'question': 'Evidently service exit with code 2',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'When I run the command “docker-compose up –build” '\n",
      "                         'and send the data to the real-time prediction '\n",
      "                         'service. The service will return “Max retries '\n",
      "                         'exceeded with url: /api”.\\n'\n",
      "                         'In my case it because of my evidently service exit '\n",
      "                         'with code 2 due to the “app.py” in evidently service '\n",
      "                         'cannot import “from pyarrow import parquet as pq”.\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'The first solution is just install the pyarrow '\n",
      "                         'module “pip install pyarrow”\\n'\n",
      "                         'The second solution is restart your machine.\\n'\n",
      "                         'The third solution is if the first and second one '\n",
      "                         'didn’t work with your machine. I found that “app.py” '\n",
      "                         'of evidently service didn’t use that module. So '\n",
      "                         'comment the pyarrow module out and the problem was '\n",
      "                         'solved for me.\\n'\n",
      "                         'Added by Surawut Jirasaktavee'},\n",
      "                {'question': 'ValueError: Incorrect item instead of a metric '\n",
      "                             'or metric preset was passed to Report',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'When using evidently if you get this error.\\n'\n",
      "                         'You probably forgot to and parentheses () just and '\n",
      "                         'opening and closing and you are good to go.\\n'\n",
      "                         'Quinn Avila'},\n",
      "                {'question': 'For the report RegressionQualityMetric()',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'You will get an error if you didn’t add a '\n",
      "                         'target=’duration_min’\\n'\n",
      "                         'If you want to use RegressionQualityMetric() you '\n",
      "                         'need a target=’duration_min and you need this added '\n",
      "                         'to you current_data[‘duration_min’]\\n'\n",
      "                         'Quinn Avila'},\n",
      "                {'question': 'Found array with 0 sample(s)',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'ValueError: Found array with 0 sample(s) (shape=(0, '\n",
      "                         '6)) while a minimum of 1 is required by '\n",
      "                         'LinearRegression.\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'This happens because the generated data is based on '\n",
      "                         'an early date therefore the training dataset would '\n",
      "                         'be empty.\\n'\n",
      "                         'Adjust the following\\n'\n",
      "                         'begin = datetime.datetime(202X, X, X, 0, 0)\\n'\n",
      "                         'Added by Luke'},\n",
      "                {'question': 'Adding additional metric',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'Getting “target columns” “prediction columns” not '\n",
      "                         'present errors after adding a metric\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Make sure to read through the documentation on what '\n",
      "                         'is required or optional when adding the metric. I '\n",
      "                         'added DatasetCorrelationsMetric which doesn’t '\n",
      "                         'require any parameters because the metric evaluates '\n",
      "                         'for correlations among the features.\\n'\n",
      "                         'Sam Lim'},\n",
      "                {'question': 'Standard login in Grafana does not work',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'When you try to login in Grafana with standard '\n",
      "                         'requisites (admin/admin) it throw up an error.\\n'\n",
      "                         'After run grafana-cli admin reset-admin-password '\n",
      "                         'admin in Grafana container the problem will be '\n",
      "                         'fixed\\n'\n",
      "                         'Added by Artem Glazkov'},\n",
      "                {'question': 'The chart in Grafana doesn’t get updates',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Problem description. While my metric generation '\n",
      "                         'script was still running, I noticed that the charts '\n",
      "                         'in Grafana don’t get updated.\\n'\n",
      "                         'Solution description. There are two things to pay '\n",
      "                         'attention to:\\n'\n",
      "                         'Refresh interval: set it to a small value: 5-10-30 '\n",
      "                         'seconds\\n'\n",
      "                         'Use your local timezone in a call to `pytz.timezone` '\n",
      "                         '– I couldn’t get updates before changing this from '\n",
      "                         'the original value “Europe/London” to my own zone'},\n",
      "                {'question': 'Prefect server was not running locally',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Problem description. Prefect server was not running '\n",
      "                         'locally, I ran `prefect server start` command but it '\n",
      "                         'stopped immediately..\\n'\n",
      "                         'Solution description. I used Prefect cloud to run '\n",
      "                         'the script, however I created an issue on the '\n",
      "                         'Prefect github.\\n'\n",
      "                         'By Erick Calderin'},\n",
      "                {'question': 'no disk space left error when doing docker '\n",
      "                             'compose up',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Solution. Using docker CLI run docker system prune '\n",
      "                         'to remove unused things (build cache, containers, '\n",
      "                         'images etc)\\n'\n",
      "                         'Also, to see what’s taking space before pruning you '\n",
      "                         'can run docker system df\\n'\n",
      "                         'By Alex Litvinov'},\n",
      "                {'question': 'Failed to listen on :::8080 (reason: '\n",
      "                             'php_network_getaddresses: getaddrinfo failed: '\n",
      "                             'Address family for hostname not supported)',\n",
      "                 'section': 'Module 5: Monitoring',\n",
      "                 'text': 'Problem: when run docker-compose up –build, you may '\n",
      "                         'see this error. To solve, add `command: php -S '\n",
      "                         '0.0.0.0:8080 -t /var/www/html` in adminer block in '\n",
      "                         'yml file like:\\n'\n",
      "                         'adminer:\\n'\n",
      "                         'command: php -S 0.0.0.0:8080 -t /var/www/html\\n'\n",
      "                         'image: adminer\\n'\n",
      "                         '…\\n'\n",
      "                         'Ilnaz Salimov\\n'\n",
      "                         'salimovilnaz777@gmail.com'},\n",
      "                {'question': 'Generate Evidently Chart in Grafana',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem: Can we generate charts like Evidently '\n",
      "                         'inside Grafana?\\n'\n",
      "                         'Solution: In Grafana that would be a stat panel '\n",
      "                         '(just a number) and scatter plot panel (I believe it '\n",
      "                         'requires a plug-in). However, there is no native way '\n",
      "                         'to quickly recreate this exact Evidently dashboard. '\n",
      "                         \"You'd need to make sure you have all the relevant \"\n",
      "                         'information logged to your Grafana data source, and '\n",
      "                         'then design your own plots in Grafana.\\n'\n",
      "                         'If you want to recreate the Evidently visualizations '\n",
      "                         'externally, you can export the Evidently output in '\n",
      "                         'JSON with include_render=True\\n'\n",
      "                         '(more details here '\n",
      "                         'https://docs.evidentlyai.com/user-guide/customization/json-dict-output) '\n",
      "                         'and then parse information from it for your external '\n",
      "                         'visualization layer. To include everything you need '\n",
      "                         'for non-aggregated visuals, you should also add '\n",
      "                         '\"raw_data\": True  option (more details here '\n",
      "                         'https://docs.evidentlyai.com/user-guide/customization/report-data-aggregation).\\n'\n",
      "                         'Overall, this specific plot with under- and '\n",
      "                         'over-performance segments is more useful during '\n",
      "                         'debugging, so might be easier to access it ad hoc '\n",
      "                         'using Evidently.\\n'\n",
      "                         'Added by Ming Jun, Asked by Luke, Answered by Elena '\n",
      "                         'Samuylova'},\n",
      "                {'question': 'Get an error ‘Unable to locate credentials’ '\n",
      "                             'after running localstack with kinesis',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': \"You may get an error ‘{'errorMessage': 'Unable to \"\n",
      "                         \"locate credentials', …’ from the print statement in \"\n",
      "                         'test_docker.py after running localstack with '\n",
      "                         'kinesis.\\n'\n",
      "                         'To fix this, in the docker-compose.yaml file, in '\n",
      "                         'addition to the environment variables like '\n",
      "                         'AWS_DEFAULT_REGION, add two other variables '\n",
      "                         'AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Their '\n",
      "                         'value is not important; anything like abc will '\n",
      "                         'suffice\\n'\n",
      "                         'Added by M\\n'\n",
      "                         'Other possibility is just to run\\n'\n",
      "                         'aws --endpoint-url http://localhost:4566 configure\\n'\n",
      "                         'And providing random values for AWS Access Key ID , '\n",
      "                         'AWS Secret Access Key, Default region name, and '\n",
      "                         'Default output format.\\n'\n",
      "                         'Added by M.A. Monjas'},\n",
      "                {'question': 'Get an error ‘ unspecified location constraint '\n",
      "                             'is incompatible ’',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'You may get an error while creating a bucket with '\n",
      "                         'localstack and the boto3 client:\\n'\n",
      "                         'botocore.exceptions.ClientError: An error occurred '\n",
      "                         '(IllegalLocationConstraintException) when calling '\n",
      "                         'the CreateBucket operation: The unspecified location '\n",
      "                         'constraint is incompatible for the region specific '\n",
      "                         'endpoint this request was sent to.\\n'\n",
      "                         'To fix this, instead of creating a bucket via\\n'\n",
      "                         \"s3_client.create_bucket(Bucket='nyc-duration')\\n\"\n",
      "                         'Create it with\\n'\n",
      "                         \"s3_client.create_bucket(Bucket='nyc-duration', \"\n",
      "                         'CreateBucketConfiguration={\\n'\n",
      "                         \"'LocationConstraint': AWS_DEFAULT_REGION})\\n\"\n",
      "                         'yam\\n'\n",
      "                         'Added by M'},\n",
      "                {'question': 'Get an error “<botocore.awsrequest.AWSRequest '\n",
      "                             'object at 0x7fbaf2666280>” after running an AWS '\n",
      "                             'CLI command',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'When executing an AWS CLI command (e.g., aws s3 ls), '\n",
      "                         'you can get the error '\n",
      "                         '<botocore.awsrequest.AWSRequest object at '\n",
      "                         '0x7fbaf2666280>.\\n'\n",
      "                         'To fix it, simply set the AWS CLI environment '\n",
      "                         'variables:\\n'\n",
      "                         'export AWS_DEFAULT_REGION=eu-west-1\\n'\n",
      "                         'export AWS_ACCESS_KEY_ID=foobar\\n'\n",
      "                         'export AWS_SECRET_ACCESS_KEY=foobar\\n'\n",
      "                         'Their value is not important; anything would be ok.\\n'\n",
      "                         'Added by Giovanni Pecoraro'},\n",
      "                {'question': 'Pre-commit triggers an error at every commit: '\n",
      "                             '“mapping values are not allowed in this context”',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'At every commit the above error is thrown and no '\n",
      "                         'pre-commit hooks are ran.\\n'\n",
      "                         'Make sure the indentation in .pre-commit-config.yaml '\n",
      "                         'is correct. Especially the 4 spaces ahead of every '\n",
      "                         '`repo` statement\\n'\n",
      "                         'Added by M. Ayoub C.'},\n",
      "                {'question': 'Could not reconfigure pytest from zero after '\n",
      "                             'getting done with previous folder',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'No option to remove pytest test\\n'\n",
      "                         'Remove .vscode folder located on the folder you '\n",
      "                         'previously used for testing, e.g. folder code (from '\n",
      "                         'week6-best-practices) was chosen to test, so you may '\n",
      "                         'remove .vscode inside the folder.\\n'\n",
      "                         'Added by Rizdi Aprilian'},\n",
      "                {'question': 'Empty Records in Kinesis Get Records with '\n",
      "                             'LocalStack',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'Following video 6.3, at minute 11:23, get records '\n",
      "                         'command returns empty Records.\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Add --no-sign-request to Kinesis get records call:\\n'\n",
      "                         ' aws --endpoint-url=http://localhost:4566 kinesis '\n",
      "                         'get-records --shard-iterator […] --no-sign-request'},\n",
      "                {'question': 'In Powershell, Git commit raises utf-8 encoding '\n",
      "                             'error after creating pre-commit yaml file',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         \"git commit -m 'Updated xxxxxx'\\n\"\n",
      "                         'An error has occurred: InvalidConfigError:\\n'\n",
      "                         '==> File .pre-commit-config.yaml\\n'\n",
      "                         \"=====> 'utf-8' codec can't decode byte 0xff in \"\n",
      "                         'position 0: invalid start byte\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Set uft-8 encoding when creating the pre-commit yaml '\n",
      "                         'file:\\n'\n",
      "                         'pre-commit sample-config | out-file '\n",
      "                         '.pre-commit-config.yaml -encoding utf8\\n'\n",
      "                         'Added by MarcosMJD'},\n",
      "                {'question': 'Git commit with pre-commit hook raises error '\n",
      "                             \"‘'PythonInfo' object has no attribute \"\n",
      "                             \"'version_nodot'\",\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         \"git commit -m 'Updated xxxxxx'\\n\"\n",
      "                         '[INFO] Initializing environment for '\n",
      "                         'https://github.com/pre-commit/pre-commit-hooks.\\n'\n",
      "                         '[INFO] Installing environment for '\n",
      "                         'https://github.com/pre-commit/pre-commit-hooks.\\n'\n",
      "                         '[INFO] Once installed this environment will be '\n",
      "                         'reused.\\n'\n",
      "                         'An unexpected error has occurred: '\n",
      "                         'CalledProcessError: command:\\n'\n",
      "                         '…\\n'\n",
      "                         'return code: 1\\n'\n",
      "                         'expected return code: 0\\n'\n",
      "                         'stdout:\\n'\n",
      "                         \"AttributeError: 'PythonInfo' object has no attribute \"\n",
      "                         \"'version_nodot'\\n\"\n",
      "                         'Solution description\\n'\n",
      "                         'Clear app-data of the virtualenv\\n'\n",
      "                         'python -m virtualenv api -vvv --reset-app-data\\n'\n",
      "                         'Added by MarcosMJD'},\n",
      "                {'question': 'Pytest error ‘module not found’ when if using '\n",
      "                             'custom packages in the source code',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'Project structure:\\n'\n",
      "                         '/sources/production/model_service.py\\n'\n",
      "                         '/sources/tests/unit_tests/test_model_service.py '\n",
      "                         '(“from production.model_service import '\n",
      "                         'ModelService)\\n'\n",
      "                         'When running python test_model_service.py from the '\n",
      "                         'sources directory, it works.\\n'\n",
      "                         'When running pytest ./test/unit_tests fails. ‘No '\n",
      "                         'module named ‘production’’\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Use python -m pytest ./test/unit_tests\\n'\n",
      "                         'Explanation: pytest does not add to the sys.path the '\n",
      "                         'path where pytest is run.\\n'\n",
      "                         'You can run python -m pytest, or alternatively '\n",
      "                         'export PYTHONPATH=. Before executing pytest\\n'\n",
      "                         'Added by MarcosMJD'},\n",
      "                {'question': 'Pytest error ‘module not found’ when using '\n",
      "                             'pre-commit hooks if using custom packages in the '\n",
      "                             'source code',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'Project structure:\\n'\n",
      "                         '/sources/production/model_service.py\\n'\n",
      "                         '/sources/tests/unit_tests/test_model_service.py '\n",
      "                         '(“from production.model_service import '\n",
      "                         'ModelService)\\n'\n",
      "                         'git commit -t ‘test’ raises ‘No module named '\n",
      "                         '‘production’’ when calling pytest hook\\n'\n",
      "                         '- repo: local\\n'\n",
      "                         'hooks:\\n'\n",
      "                         '- id: pytest-check\\n'\n",
      "                         'name: pytest-check\\n'\n",
      "                         'entry: pytest\\n'\n",
      "                         'language: system\\n'\n",
      "                         'pass_filenames: false\\n'\n",
      "                         'always_run: true\\n'\n",
      "                         'args: [\\n'\n",
      "                         '\"tests/\"\\n'\n",
      "                         ']\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Use this hook instead:\\n'\n",
      "                         '- repo: local\\n'\n",
      "                         'hooks:\\n'\n",
      "                         '- id: pytest-check\\n'\n",
      "                         'name: pytest-check\\n'\n",
      "                         'entry: \"./sources/tests/unit_tests/run.sh\"\\n'\n",
      "                         'language: system\\n'\n",
      "                         'types: [python]\\n'\n",
      "                         'pass_filenames: false\\n'\n",
      "                         'always_run: true\\n'\n",
      "                         'And make sure that run.sh sets the right directory '\n",
      "                         'and run pytest:\\n'\n",
      "                         'cd \"$(dirname \"$0\")\"\\n'\n",
      "                         'cd ../..\\n'\n",
      "                         'export PYTHONPATH=.\\n'\n",
      "                         'pipenv run pytest ./tests/unit_tests\\n'\n",
      "                         'Added by MarcosMJD'},\n",
      "                {'question': 'Github actions: Permission denied error when '\n",
      "                             'executing script file',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'This is the step in the ci yml file definition:\\n'\n",
      "                         '- name: Run Unit Tests\\n'\n",
      "                         'working-directory: \"sources\"\\n'\n",
      "                         'run: ./tests/unit_tests/run.sh\\n'\n",
      "                         'When executing github ci action, error raises:\\n'\n",
      "                         '…/tests/unit_test/run.sh Permission error\\n'\n",
      "                         'Error: Process completed with error code 126\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Add execution  permission to the script and '\n",
      "                         'commit+push:\\n'\n",
      "                         'git update-index --chmod=+x '\n",
      "                         '.\\\\sources\\\\tests\\\\unit_tests\\\\run.sh\\n'\n",
      "                         'Added by MarcosMJD'},\n",
      "                {'question': 'Managing Multiple Docker Containers with '\n",
      "                             'docker-compose profile',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'When a docker-compose file contains a lot of '\n",
      "                         'containers, running the containers may take too much '\n",
      "                         'resource. There is a need to easily select only a '\n",
      "                         'group of containers while ignoring irrelevant '\n",
      "                         'containers during testing.\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Add profiles: [“profile_name”] in the service '\n",
      "                         'definition.\\n'\n",
      "                         'When starting up the service, add `--profile '\n",
      "                         'profile_name` in the command.\\n'\n",
      "                         'Added by Ammar Chalifah'},\n",
      "                {'question': 'AWS regions need to match docker-compose',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'If you are having problems with the integration '\n",
      "                         'tests and kinesis double check that your aws regions '\n",
      "                         'match on the docker-compose and local config. '\n",
      "                         'Otherwise you will be creating a stream in the wrong '\n",
      "                         'region\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'For example set ~/.aws/config region = us-east-1 and '\n",
      "                         'the docker-compose.yaml - '\n",
      "                         'AWS_DEFAULT_REGION=us-east-1\\n'\n",
      "                         'Added by Quinn Avila'},\n",
      "                {'question': 'Isort Pre-commit',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'Pre-commit command was failing with isort repo.\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'Set version to 5.12.0\\n'\n",
      "                         'Added by Erick Calderin'},\n",
      "                {'question': 'How to destroy infrastructure created via GitHub '\n",
      "                             'Actions',\n",
      "                 'section': 'Module 6: Best practices',\n",
      "                 'text': 'Problem description\\n'\n",
      "                         'Infrastructure created in AWS with CD-Deploy Action '\n",
      "                         'needs to be destroyed\\n'\n",
      "                         'Solution description\\n'\n",
      "                         'From local:\\n'\n",
      "                         'terraform init '\n",
      "                         '-backend-config=\"key=mlops-zoomcamp-prod.tfstate\" '\n",
      "                         '--reconfigure\\n'\n",
      "                         'terraform destroy --var-file vars/prod.tfvars\\n'\n",
      "                         'Added by Erick Calderin'}]}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "pprint(documents_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1420a",
   "metadata": {},
   "source": [
    "Data already seems cleaned and chunked (i.e., divided into small pieces that embedding models can easily digest), so what's left is to define:\n",
    "\n",
    "- which fields could be used for **semantic search** ;\n",
    "- which fields should be stored as **metadata**, e.g. useable for filtering conditions; \n",
    "\n",
    "We have a dataset with three course types:  \n",
    "`data-engineering-zoomcamp`, `machine-learning-zoomcamp`, and `mlops-zoomcamp`.  \n",
    "Each course includes a collection of `question` and `text` (answer) pairs, along with the `section` the question refers to.\n",
    "\n",
    "---\n",
    "\n",
    "#### Which Fields Could Be Used for Semantic Search\n",
    "\n",
    "Here we can observe semantic similarity in practice: some of the questions and answers don’t share many overlapping words,  \n",
    "yet they clearly address the same topic. One asks about a topic, the other provides an answer.\n",
    "\n",
    "For example:\n",
    "\n",
    "**Question**:  \n",
    "- *“I have **registered** for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?”*\n",
    "\n",
    "**Answer**:  \n",
    "- *“You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any **registered** list. Registration is just to gauge interest before the start date.”*\n",
    "\n",
    "These two could be matched via the keyword **registered**,  \n",
    "but a sentence like *“Not **registered** participants are not getting certification”* would also match that keyword, while having a different semantic meaning.\n",
    "\n",
    "So, if we’re building a Q&A retrieval-augmented generation (RAG) system,  \n",
    "it makes sense to store the `text` field (answers) as **embeddings**, and use vector search to find the most relevant answer to a given `question` query.\n",
    "\n",
    "#### Which Fields Should Be Stored as Metadata\n",
    "\n",
    "For example, we could store the `course` and `section` fields as metadata.  \n",
    "This way, we can filter search results when asking questions related to a specific course or a specific section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb849bc",
   "metadata": {},
   "source": [
    "## Step 3: Choosing the Embedding Model with FastEmbed\n",
    "\n",
    "Now that we know we're embedding small chunks of English text (course-related question and answer pairs), we can choose a suitable embedding model to convert this data into vectors.\n",
    "\n",
    "The choice of an embedding model depends on many factors:\n",
    "- The task, data modality, and data specifics;\n",
    "- The trade-off between search precision and resource usage (larger embeddings require more storage and memory);\n",
    "- The cost of inference (especially if you're using a third-party provider);\n",
    "- etc\n",
    "\n",
    "> The best way to select an embedding model is to **test and benchmark different options on your own data**.\n",
    "\n",
    "In this notebook, we’re going to use [FastEmbed](https://github.com/qdrant/fastembed) as our embedding provider.\n",
    "\n",
    "---\n",
    "\n",
    "**FastEmbed** is an optimized embedding solution designed specifically for Qdrant. It delivers low-latency, CPU-friendly embedding generation, eliminating the need for heavy frameworks like PyTorch or TensorFlow. It uses quantized model weights and ONNX Runtime, making it significantly faster than traditional Sentence Transformers on CPU while maintaining competitive accuracy.\n",
    "\n",
    "FastEmbed supports:\n",
    "- **Dense embeddings** for text and images (*the most common type in vector search, ones we're going to use today*)\n",
    "- **Sparse embeddings** (e.g., BM25 and sparse neural embeddings)  \n",
    "- **Multivector embeddings** (e.g., ColPali and ColBERT, late interaction models)  \n",
    "- **Rerankers**\n",
    "\n",
    "All of these can be directly used in Qdrant (as **Qdrant supports dense, sparse & multivectors along with hybrid search**).  \n",
    "FastEmbed’s integration with Qdrant allows you to directly pass text or images to the Qdrant client for embedding.\n",
    "\n",
    "In this notebook, we’ll use FastEmbed for local inference with Qdrant.  \n",
    "> Keep in mind your machine's resources when choosing an embedding model for local inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e459b7a",
   "metadata": {},
   "source": [
    "### FastEmbed for Textual Data\n",
    "\n",
    "Let’s select an embedding model to use for our course question answers, stored in `text` fields, from the options supported by FastEmbed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ed8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: necessary, 2023 '\n",
      "                 'year.',\n",
      "  'dim': 768,\n",
      "  'license': 'mit',\n",
      "  'model': 'BAAI/bge-base-en',\n",
      "  'model_file': 'model_optimized.onnx',\n",
      "  'size_in_GB': 0.42,\n",
      "  'sources': {'_deprecated_tar_struct': True,\n",
      "              'hf': 'Qdrant/fast-bge-base-en',\n",
      "              'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz'},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: not so '\n",
      "                 'necessary, 2023 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'mit',\n",
      "  'model': 'BAAI/bge-base-en-v1.5',\n",
      "  'model_file': 'model_optimized.onnx',\n",
      "  'size_in_GB': 0.21,\n",
      "  'sources': {'_deprecated_tar_struct': True,\n",
      "              'hf': 'qdrant/bge-base-en-v1.5-onnx-q',\n",
      "              'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz'},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: not so '\n",
      "                 'necessary, 2023 year.',\n",
      "  'dim': 1024,\n",
      "  'license': 'mit',\n",
      "  'model': 'BAAI/bge-large-en-v1.5',\n",
      "  'model_file': 'model.onnx',\n",
      "  'size_in_GB': 1.2,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'qdrant/bge-large-en-v1.5-onnx',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: necessary, 2023 '\n",
      "                 'year.',\n",
      "  'dim': 384,\n",
      "  'license': 'mit',\n",
      "  'model': 'BAAI/bge-small-en',\n",
      "  'model_file': 'model_optimized.onnx',\n",
      "  'size_in_GB': 0.13,\n",
      "  'sources': {'_deprecated_tar_struct': True,\n",
      "              'hf': 'Qdrant/bge-small-en',\n",
      "              'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz'},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: not so '\n",
      "                 'necessary, 2023 year.',\n",
      "  'dim': 384,\n",
      "  'license': 'mit',\n",
      "  'model': 'BAAI/bge-small-en-v1.5',\n",
      "  'model_file': 'model_optimized.onnx',\n",
      "  'size_in_GB': 0.067,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'qdrant/bge-small-en-v1.5-onnx-q',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), Chinese, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: not so '\n",
      "                 'necessary, 2023 year.',\n",
      "  'dim': 512,\n",
      "  'license': 'mit',\n",
      "  'model': 'BAAI/bge-small-zh-v1.5',\n",
      "  'model_file': 'model_optimized.onnx',\n",
      "  'size_in_GB': 0.09,\n",
      "  'sources': {'_deprecated_tar_struct': True,\n",
      "              'hf': 'Qdrant/bge-small-zh-v1.5',\n",
      "              'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz'},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: necessary, 2024 '\n",
      "                 'year.',\n",
      "  'dim': 1024,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'mixedbread-ai/mxbai-embed-large-v1',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.64,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'mixedbread-ai/mxbai-embed-large-v1',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: necessary, 2024 '\n",
      "                 'year.',\n",
      "  'dim': 384,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'snowflake/snowflake-arctic-embed-xs',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.09,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'snowflake/snowflake-arctic-embed-xs',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: necessary, 2024 '\n",
      "                 'year.',\n",
      "  'dim': 384,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'snowflake/snowflake-arctic-embed-s',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.13,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'snowflake/snowflake-arctic-embed-s',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: necessary, 2024 '\n",
      "                 'year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'snowflake/snowflake-arctic-embed-m',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.43,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'Snowflake/snowflake-arctic-embed-m',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 2048 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: necessary, 2024 '\n",
      "                 'year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'snowflake/snowflake-arctic-embed-m-long',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.54,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'snowflake/snowflake-arctic-embed-m-long',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: necessary, 2024 '\n",
      "                 'year.',\n",
      "  'dim': 1024,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'snowflake/snowflake-arctic-embed-l',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 1.02,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'snowflake/snowflake-arctic-embed-l',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Multimodal (text&image), English, Prefixes '\n",
      "                 'for queries/documents: not necessary, 2024 year',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'jinaai/jina-clip-v1',\n",
      "  'model_file': 'onnx/text_model.onnx',\n",
      "  'size_in_GB': 0.55,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'jinaai/jina-clip-v1',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Multimodal (text&image), English, 77 input '\n",
      "                 'tokens truncation, Prefixes for queries/documents: not '\n",
      "                 'necessary, 2021 year',\n",
      "  'dim': 512,\n",
      "  'license': 'mit',\n",
      "  'model': 'Qdrant/clip-ViT-B-32-text',\n",
      "  'model_file': 'model.onnx',\n",
      "  'size_in_GB': 0.25,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'Qdrant/clip-ViT-B-32-text',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 256 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: not necessary, '\n",
      "                 '2021 year.',\n",
      "  'dim': 384,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
      "  'model_file': 'model.onnx',\n",
      "  'size_in_GB': 0.09,\n",
      "  'sources': {'_deprecated_tar_struct': True,\n",
      "              'hf': 'qdrant/all-MiniLM-L6-v2-onnx',\n",
      "              'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz'},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: not necessary, '\n",
      "                 '2023 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'jinaai/jina-embeddings-v2-base-en',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.52,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'xenova/jina-embeddings-v2-base-en',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 8192 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: not necessary, '\n",
      "                 '2023 year.',\n",
      "  'dim': 512,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'jinaai/jina-embeddings-v2-small-en',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.12,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'xenova/jina-embeddings-v2-small-en',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), Multilingual (German, '\n",
      "                 'English), 8192 input tokens truncation, Prefixes for '\n",
      "                 'queries/documents: not necessary, 2024 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'jinaai/jina-embeddings-v2-base-de',\n",
      "  'model_file': 'onnx/model_fp16.onnx',\n",
      "  'size_in_GB': 0.32,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'jinaai/jina-embeddings-v2-base-de',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), Multilingual (English, 30 '\n",
      "                 'programming languages), 8192 input tokens truncation, '\n",
      "                 'Prefixes for queries/documents: not necessary, 2024 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'jinaai/jina-embeddings-v2-base-code',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.64,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'jinaai/jina-embeddings-v2-base-code',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), supports mixed '\n",
      "                 'Chinese-English input text, 8192 input tokens truncation, '\n",
      "                 'Prefixes for queries/documents: not necessary, 2024 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'jinaai/jina-embeddings-v2-base-zh',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.64,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'jinaai/jina-embeddings-v2-base-zh',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), supports mixed '\n",
      "                 'Spanish-English input text, 8192 input tokens truncation, '\n",
      "                 'Prefixes for queries/documents: not necessary, 2024 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'jinaai/jina-embeddings-v2-base-es',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.64,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'jinaai/jina-embeddings-v2-base-es',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'General text embeddings, Unimodal (text), supports English '\n",
      "                 'only input text, 512 input tokens truncation, Prefixes for '\n",
      "                 'queries/documents: not necessary, 2024 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'mit',\n",
      "  'model': 'thenlper/gte-base',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.44,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'thenlper/gte-base',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), English, 512 input tokens '\n",
      "                 'truncation, Prefixes for queries/documents: not necessary, '\n",
      "                 '2023 year.',\n",
      "  'dim': 1024,\n",
      "  'license': 'mit',\n",
      "  'model': 'thenlper/gte-large',\n",
      "  'model_file': 'model.onnx',\n",
      "  'size_in_GB': 1.2,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'qdrant/gte-large-onnx',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 '\n",
      "                 'input tokens truncation, Prefixes for queries/documents: '\n",
      "                 'necessary, 2024 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'nomic-ai/nomic-embed-text-v1.5',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.52,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 '\n",
      "                 'input tokens truncation, Prefixes for queries/documents: '\n",
      "                 'necessary, 2024 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'nomic-ai/nomic-embed-text-v1.5-Q',\n",
      "  'model_file': 'onnx/model_quantized.onnx',\n",
      "  'size_in_GB': 0.13,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'nomic-ai/nomic-embed-text-v1.5',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Multimodal (text, image), English, 8192 '\n",
      "                 'input tokens truncation, Prefixes for queries/documents: '\n",
      "                 'necessary, 2024 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'nomic-ai/nomic-embed-text-v1',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 0.52,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'nomic-ai/nomic-embed-text-v1',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 '\n",
      "                 'languages), 512 input tokens truncation, Prefixes for '\n",
      "                 'queries/documents: not necessary, 2019 year.',\n",
      "  'dim': 384,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
      "  'model_file': 'model_optimized.onnx',\n",
      "  'size_in_GB': 0.22,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': [],\n",
      "  'description': 'Text embeddings, Unimodal (text), Multilingual (~50 '\n",
      "                 'languages), 384 input tokens truncation, Prefixes for '\n",
      "                 'queries/documents: not necessary, 2021 year.',\n",
      "  'dim': 768,\n",
      "  'license': 'apache-2.0',\n",
      "  'model': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 1.0,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2',\n",
      "              'url': None},\n",
      "  'tasks': {}},\n",
      " {'additional_files': ['model.onnx_data'],\n",
      "  'description': 'Text embeddings, Unimodal (text), Multilingual (~100 '\n",
      "                 'languages), 512 input tokens truncation, Prefixes for '\n",
      "                 'queries/documents: necessary, 2024 year.',\n",
      "  'dim': 1024,\n",
      "  'license': 'mit',\n",
      "  'model': 'intfloat/multilingual-e5-large',\n",
      "  'model_file': 'model.onnx',\n",
      "  'size_in_GB': 2.24,\n",
      "  'sources': {'_deprecated_tar_struct': True,\n",
      "              'hf': 'qdrant/multilingual-e5-large-onnx',\n",
      "              'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz'},\n",
      "  'tasks': {}},\n",
      " {'additional_files': ['onnx/model.onnx_data'],\n",
      "  'description': 'Multi-task unimodal (text) embedding model, multi-lingual '\n",
      "                 '(~100), 1024 tokens truncation, and 8192 sequence length. '\n",
      "                 'Prefixes for queries/documents: not necessary, 2024 year.',\n",
      "  'dim': 1024,\n",
      "  'license': 'cc-by-nc-4.0',\n",
      "  'model': 'jinaai/jina-embeddings-v3',\n",
      "  'model_file': 'onnx/model.onnx',\n",
      "  'size_in_GB': 2.29,\n",
      "  'sources': {'_deprecated_tar_struct': False,\n",
      "              'hf': 'jinaai/jina-embeddings-v3',\n",
      "              'url': None},\n",
      "  'tasks': {'classification': 3,\n",
      "            'retrieval.passage': 1,\n",
      "            'retrieval.query': 0,\n",
      "            'separation': 2,\n",
      "            'text-matching': 4}}]\n"
     ]
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "\n",
    "pprint(TextEmbedding.list_supported_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2893f2",
   "metadata": {},
   "source": [
    "It makes sense to choose a model that produces small-to-moderate-sized embeddings (e.g., 512 dimensions), so we don’t overuse resources in our simple setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18dd0905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n'\n",
      " '  \"model\": \"BAAI/bge-small-zh-v1.5\",\\n'\n",
      " '  \"sources\": {\\n'\n",
      " '    \"hf\": \"Qdrant/bge-small-zh-v1.5\",\\n'\n",
      " '    \"url\": '\n",
      " '\"https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz\",\\n'\n",
      " '    \"_deprecated_tar_struct\": true\\n'\n",
      " '  },\\n'\n",
      " '  \"model_file\": \"model_optimized.onnx\",\\n'\n",
      " '  \"description\": \"Text embeddings, Unimodal (text), Chinese, 512 input '\n",
      " 'tokens truncation, Prefixes for queries/documents: not so necessary, 2023 '\n",
      " 'year.\",\\n'\n",
      " '  \"license\": \"mit\",\\n'\n",
      " '  \"size_in_GB\": 0.09,\\n'\n",
      " '  \"additional_files\": [],\\n'\n",
      " '  \"dim\": 512,\\n'\n",
      " '  \"tasks\": {}\\n'\n",
      " '}')\n",
      "('{\\n'\n",
      " '  \"model\": \"Qdrant/clip-ViT-B-32-text\",\\n'\n",
      " '  \"sources\": {\\n'\n",
      " '    \"hf\": \"Qdrant/clip-ViT-B-32-text\",\\n'\n",
      " '    \"url\": null,\\n'\n",
      " '    \"_deprecated_tar_struct\": false\\n'\n",
      " '  },\\n'\n",
      " '  \"model_file\": \"model.onnx\",\\n'\n",
      " '  \"description\": \"Text embeddings, Multimodal (text&image), English, 77 '\n",
      " 'input tokens truncation, Prefixes for queries/documents: not necessary, 2021 '\n",
      " 'year\",\\n'\n",
      " '  \"license\": \"mit\",\\n'\n",
      " '  \"size_in_GB\": 0.25,\\n'\n",
      " '  \"additional_files\": [],\\n'\n",
      " '  \"dim\": 512,\\n'\n",
      " '  \"tasks\": {}\\n'\n",
      " '}')\n",
      "('{\\n'\n",
      " '  \"model\": \"jinaai/jina-embeddings-v2-small-en\",\\n'\n",
      " '  \"sources\": {\\n'\n",
      " '    \"hf\": \"xenova/jina-embeddings-v2-small-en\",\\n'\n",
      " '    \"url\": null,\\n'\n",
      " '    \"_deprecated_tar_struct\": false\\n'\n",
      " '  },\\n'\n",
      " '  \"model_file\": \"onnx/model.onnx\",\\n'\n",
      " '  \"description\": \"Text embeddings, Unimodal (text), English, 8192 input '\n",
      " 'tokens truncation, Prefixes for queries/documents: not necessary, 2023 '\n",
      " 'year.\",\\n'\n",
      " '  \"license\": \"apache-2.0\",\\n'\n",
      " '  \"size_in_GB\": 0.12,\\n'\n",
      " '  \"additional_files\": [],\\n'\n",
      " '  \"dim\": 512,\\n'\n",
      " '  \"tasks\": {}\\n'\n",
      " '}')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "\n",
    "for model in TextEmbedding.list_supported_models():\n",
    "    if model[\"dim\"] == EMBEDDING_DIMENSIONALITY:\n",
    "        pprint(json.dumps(model, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e08302",
   "metadata": {},
   "source": [
    "We need an embedding model suitable for **English text**. \n",
    "\n",
    "It also makes sense to select a **unimodal** model, since we’re not including images in our search, and specifically tailored solutions are usually better than universal ones.\n",
    "\n",
    "It seems like `jina-embedding-small-en` is a good choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c2fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ca508",
   "metadata": {},
   "source": [
    "Like most dense embedding models, `jina-embedding-small-en` was trained to measure semantic closeness using **cosine similarity**.  \n",
    "You can find this information, for example, on the model’s [Hugging Face card](https://huggingface.co/jinaai/jina-embeddings-v2-small-en).\n",
    "\n",
    "> The parameters of the chosen embedding model, including the output embedding dimensions and the semantic similarity (distance) metric, are required to configure semantic search in Qdrant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b360404",
   "metadata": {},
   "source": [
    "Now we’re ready to configure and use Qdrant for semantic search.\n",
    "To fully understand what’s happening, here’s a quick overview of Qdrant’s core terminology:\n",
    "\n",
    "- **Points** are the central entity Qdrant works with.  \n",
    "  A point is a record consisting of an **ID**, a **vector**, and an optional **payload**.\n",
    "- A **collection** is a named set of points (i.e., vectors with optional payloads) that you can search within.  \n",
    "  *Think of it as the container for your vector search solution, a single business problem solved.*\n",
    "\n",
    "> Qdrant supports different types of vectors to enable different modes of data exploration and search (dense, sparse, multivectors, and named vectors).\n",
    "\n",
    "In this example, we’ll use the most common type, **dense vectors**.\n",
    "\n",
    "Embeddings capture the semantic essence of the data, while the **payload** holds structured metadata.  \n",
    "This metadata becomes especially useful when applying filters or sorting during search. **Qdrant's payloads** can hold structured data like booleans, keywords, geo-locations, arrays, and nested objects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f485934",
   "metadata": {},
   "source": [
    "## Step 4: Create a Collection\n",
    "\n",
    "When creating a [collection](https://qdrant.tech/documentation/concepts/collections/), we need to specify:\n",
    "\n",
    "*   Name: A unique identifier for the collection.\n",
    "*   Vector Configuration:\n",
    "    *   Size: The dimensionality of the vectors.\n",
    "    *   Distance Metric: The method used to measure similarity between vectors.\n",
    "\n",
    "\n",
    "There are additional parameters you can explore in our [documentation](https://qdrant.tech/documentation/concepts/collections/#create-a-collection). Moreover, you can configure other vector types in Qdrant beyond typical dense embeddings (f.e., for hybrid search). However, for this example, the simplest default configuration is sufficient.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d24ad190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the collection name\n",
    "collection_name = \"zoomcamp-rag\"\n",
    "\n",
    "# Create the collection with specified vector parameters\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY,  # Dimensionality of the vectors\n",
    "        distance=models.Distance.COSINE  # Distance metric for similarity search\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b24e4",
   "metadata": {},
   "source": [
    "## Step 5: Create, Embed & Insert Points into the Collection\n",
    "\n",
    "[Points](https://qdrant.tech/documentation/concepts/points/#points) are the core data entities in Qdrant. Each point consists of:\n",
    "\n",
    "1. **ID**. A unique identifier. Qdrant supports both 64-bit unsigned integers and UUIDs.  \n",
    "2. **Vector**. The embedding that represents the data point in vector space.  \n",
    "3. **Payload** *(optional)*. Additional metadata as key-value pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69014231",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "id = 0\n",
    "\n",
    "for course in documents_raw:\n",
    "    for doc in course[\"documents\"]:\n",
    "\n",
    "        point = models.PointStruct(\n",
    "            id=id,\n",
    "            vector=models.Document(text=doc[\"text\"], model=model_handle), # embed text locally with \"jinaai/jina-embeddings-v2-small-en\" from FastEmbed\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": course[\"course\"]\n",
    "            } # save all needed metadata fields\n",
    "        )\n",
    "        points.append(point)\n",
    "\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f108b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qdrant_client.http.models.models.PointStruct"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(points[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4b874",
   "metadata": {},
   "source": [
    "Now we’re going to embed and upload points to our collection.\n",
    "\n",
    "First, FastEmbed will fetch&download the selected model (path defaults to `os.path.join(tempfile.gettempdir(), \"fastembed_cache\")`), and perform inference directly on your machine.  \n",
    "Then, the generated points will be upserted into the collection, and the vector index will be built.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "523a8c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:05<00:00,  1.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665f5f1",
   "metadata": {},
   "source": [
    "The speed of upsert mainly depends on the time spent on local inference.  \n",
    "To speed this up, you could run FastEmbed on GPUs or use a machine with more resources.\n",
    "\n",
    "In addition to basic `upsert`, Qdrant supports **batch upsert** in both column- and record-oriented formats.\n",
    "\n",
    "The Python client offers:\n",
    "- Parallelization  \n",
    "- Retries  \n",
    "- Lazy batching  \n",
    "\n",
    "These can be configured via parameters in the `upload_collection` and `upload_points` functions.  \n",
    "For details, check the [documentation](https://qdrant.tech/documentation/concepts/points/#upload-points).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4de735",
   "metadata": {},
   "source": [
    "### Study Data Visually\n",
    "\n",
    "Let’s explore the uploaded data in the Qdrant Web UI at [http://localhost:6333/dashboard](http://localhost:6333/dashboard) to study semantic similarity visually.\n",
    "\n",
    "For example, using the `Visualize` tab in the `zoomcamp-rag` collection, we can view all answers to the course questions (948 points) and see how they group together by meaning, additionally coloured by the course type.  \n",
    "\n",
    "To do that, run the following command:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"limit\": 948,\n",
    "  \"color_by\": {\n",
    "    \"payload\": \"course\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This 2D representation is the result of dimensionality reduction applied to `jina-embeddings`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7bc33",
   "metadata": {},
   "source": [
    "## Step 6: Running a Similarity Search\n",
    "\n",
    "Now, let’s find the most similar `text` vector in Qdrant to a given query embedding - the most relevant answer to a given question.\n",
    "\n",
    "### How Similarity Search Works\n",
    "\n",
    "1. Qdrant compares the query vector to stored vectors (based on a vector index) using the distance metric defined when creating the collection.\n",
    "\n",
    "2. The closest matches are returned, ranked by similarity.\n",
    "\n",
    "> Vector index is built for **approximate** nearest neighbor (ANN) search, making large-scale vector search feasible.\n",
    "\n",
    "If you'd like to dive into our choice of vector index for vector search, check our article [\"What is a vector database\"](https://qdrant.tech/articles/what-is-a-vector-database/), or, for a more technical deep dive, our article on [Filterable Hierarchical Navigable Small World](https://qdrant.tech/articles/filtrable-hnsw/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52a2d2",
   "metadata": {},
   "source": [
    "Let's define a search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "145dbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, limit=1):\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document( # embed the query text locally with \"jinaai/jina-embeddings-v2-small-en\"\n",
    "            text=query,\n",
    "            model=model_handle \n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True # to get metadata in the results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38c1e1",
   "metadata": {},
   "source": [
    "Now let’s pick a random question from the course data.  \n",
    "As you remember, we didn’t upload the questions to Qdrant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e6048ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n'\n",
      " '  \"text\": \"Problem description\\\\nPre-commit command was failing with isort '\n",
      " 'repo.\\\\nSolution description\\\\nSet version to 5.12.0\\\\nAdded by Erick '\n",
      " 'Calderin\",\\n'\n",
      " '  \"section\": \"Module 6: Best practices\",\\n'\n",
      " '  \"question\": \"Isort Pre-commit\"\\n'\n",
      " '}')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "course = random.choice(documents_raw)\n",
    "course_piece = random.choice(course['documents'])\n",
    "pprint(json.dumps(course_piece, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6770f4",
   "metadata": {},
   "source": [
    "Let's see which answer we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "612f0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search(course_piece['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "371b5b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(points=[ScoredPoint(id=946, version=0, score=0.8516542, payload={'text': 'Problem description\\nPre-commit command was failing with isort repo.\\nSolution description\\nSet version to 5.12.0\\nAdded by Erick Calderin', 'section': 'Module 6: Best practices', 'course': 'mlops-zoomcamp'}, vector=None, shard_key=None, order_value=None)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561077bb",
   "metadata": {},
   "source": [
    "`score` – the cosine similarity between the `question` and `text` embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992645e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let’s compare the original and retrieved answers for our randomly selected question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40ca2226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Isort Pre-commit\n",
      "\n",
      "Top Retrieved Answer:\n",
      "Problem description\n",
      "Pre-commit command was failing with isort repo.\n",
      "Solution description\n",
      "Set version to 5.12.0\n",
      "Added by Erick Calderin\n",
      "\n",
      "Original Answer:\n",
      "Problem description\n",
      "Pre-commit command was failing with isort repo.\n",
      "Solution description\n",
      "Set version to 5.12.0\n",
      "Added by Erick Calderin\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question:\\n{course_piece['question']}\\n\")\n",
    "print(\"Top Retrieved Answer:\\n{}\\n\".format(result.points[0].payload['text']))\n",
    "print(\"Original Answer:\\n{}\".format(course_piece['text']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbe8fc",
   "metadata": {},
   "source": [
    "Now let’s search the answer to a question that wasn’t in the initial dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78a51171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, late submissions are not allowed. But if the form is still not closed and it’s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\n",
      "Older news:[source1] [source2]\n"
     ]
    }
   ],
   "source": [
    "print(search(\"What if I submit homeworks late?\").points[0].payload['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b45a9",
   "metadata": {},
   "source": [
    "## Step 7: Running a Similarity Search with Filters\n",
    "\n",
    "We can refine our search using metadata filters. \n",
    "\n",
    "> Qdrant’s custom vector index implementation, Filterable HNSW, allows for precise and scalable vector search with filtering conditions.\n",
    "\n",
    "For example, we can search for an answer to a question related to a specific course from the three available in the dataset.  \n",
    "Using a `must` filter ensures that all specified conditions are met for a data point to be included in the search results.\n",
    "\n",
    "> Qdrant also supports other filter types such as `should`, `must_not`, `range`, and more. For a full overview, check our [Filtering Guide](https://qdrant.tech/articles/vector-search-filtering/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89878180",
   "metadata": {},
   "source": [
    "To enable efficient filtering, we need to turn on [indexing of payload fields](https://qdrant.tech/documentation/concepts/indexing/#payload-index).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d75d9f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=2, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"course\",\n",
    "    field_schema=\"keyword\" # exact matching on string metadata fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ff71e",
   "metadata": {},
   "source": [
    "Now let's update our search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64c3284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_course(query, course=\"mlops-zoomcamp\", limit=1):\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document( # embed the query text locally with \"jinaai/jina-embeddings-v2-small-en\"\n",
    "            text=query,\n",
    "            model=model_handle\n",
    "        ),\n",
    "        query_filter=models.Filter( # filter by course name\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"course\",\n",
    "                    match=models.MatchValue(value=course)\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True # to get metadata in the results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad60d7b",
   "metadata": {},
   "source": [
    "Let’s see how the same question is answered across different courses:  \n",
    "`data-engineering-zoomcamp`, `machine-learning-zoomcamp`, and `mlops-zoomcamp`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80a988ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose the closest one to your answer. Also do not post your answer in the course slack channel.\n"
     ]
    }
   ],
   "source": [
    "print(search_in_course(\"What if I submit homeworks late?\", \"mlops-zoomcamp\").points[0].payload['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e5dabd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "🎉 Congratulations! You now have everything you need to run a simple semantic search with Qdrant! 👏\n",
    "\n",
    "In general, data preparation, organization, and storage in a production-ready vector search solution is a topic worth a course of its own.  \n",
    "If you’re curious to dive deeper into efficient vector search setup, check out our [Vector Search Manuals](https://qdrant.tech/articles/vector-search-manuals/).\n",
    "\n",
    "In the next videos, we will show you how to use **[hybrid search](https://qdrant.tech/articles/hybrid-search/)**, combining the strengths of both keywords-based search and vector search. In many real-world applications, they work hand-in-hand, balancing the precision of keywords with the flexibility of embeddings to deliver the best results.\n",
    "\n",
    "P.S. We encourage you to check out Qdrant’s capabilities, which go beyond similarity search powering RAG & agentic pipelines (but still, here's our [MCP server](https://github.com/qdrant/mcp-server-qdrant) ;) ).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
